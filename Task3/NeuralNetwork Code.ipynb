{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6006de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39d7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0936fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def softmax(X):\n",
    "    e = np.exp(X - np.max(X))\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(target, output):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "def cross_entropy_matrix(output, target):\n",
    "    target = np.array(target)\n",
    "    output = np.array(output)\n",
    "    product = target*np.log(output)\n",
    "    errors = -np.sum(product, axis=1)\n",
    "    m = len(errors)\n",
    "    errors = np.sum(errors) / m\n",
    "    return errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ds(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "\n",
    "def dr(x):\n",
    "    dr = (np.sign(x) + 1) / 2\n",
    "    return dr\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-x)\n",
    "    return (a-b)/(a+b)\n",
    "\n",
    "def dt(x):\n",
    "    return 1-tanh(x)**2\n",
    "    \n",
    "def leaky(x,a):\n",
    "    leaky = np.maximum(x,0)*x + a*np.minimum(x,0)\n",
    "    return leaky\n",
    "\n",
    "def dl(x,a):\n",
    "    dl = (np.sign(x)+1)/2 - a*(np.sign(x)-1)/2\n",
    "    return dl\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid :\n",
    "        return ds\n",
    "    if f == tanh :\n",
    "        return dt\n",
    "    if f == relu :\n",
    "        return dr\n",
    "    if f == leaky :\n",
    "        return dl\n",
    "    return None\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i][y[i]]=1\n",
    "    return ind\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7220f",
   "metadata": {},
   "source": [
    "# One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5c79",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden x N_features\n",
    "- **b1**    : Hidden\n",
    "- **W2**    : Output x Hidden\n",
    "- **b2**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26933734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenOne:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes,\n",
    "                 learning_rate,\n",
    "                 activation_hidden,\n",
    "                 optimizer = None,\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes = hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.hidden_derivative = derivative(self.activation_hidden)\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "        # W1 of size hidden x features\n",
    "        n = self.input_nodes * self.hidden_nodes\n",
    "        self.W1 = tn.rvs(n).reshape((self.hidden_nodes, self.input_nodes )) # hidden x features\n",
    "        # W2 of size output x hidden\n",
    "        m = self.hidden_nodes  * self.output_nodes\n",
    "        self.W2 = tn.rvs(m).reshape((self.output_nodes, self.hidden_nodes )) # output x hidden\n",
    "    \n",
    "    def create_biases(self):    \n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.b1 = tn.rvs(self.hidden_nodes).reshape(-1,1) \n",
    "        self.b2 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1 # Hidden x N_samples\n",
    "        A1 = self.activation_hidden(Z1)      # Hidden x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2  # Output x N_samples\n",
    "        A2 = softmax(Z2)      #Output x N_samples\n",
    "        return A2, Z2, A1, Z1\n",
    "    \n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backpropSGD(self, X, target):\n",
    "        m = X.shape[0]                  #N_samples\n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(m))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        for i in range(m) :\n",
    "            # Forward prop\n",
    "            x = X_SGD[i,:].reshape(1,-1)                   # 1 x N_features\n",
    "            a2, z2, a1, z1 = self.forward(x)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,i].reshape(-1,1), a2)/m\n",
    "            # deltas\n",
    "            dz2 = a2 - target[:,i].reshape(-1,1)                    #Output x 1\n",
    "            dW2 = dz2.dot(a1.T)                                     #Output x hidden\n",
    "            db2 = dz2                                               #Output x 1\n",
    "            dz1 = self.W2.T.dot(dz2)*self.hidden_derivative(z1)     # Hidden x 1\n",
    "            dW1 = dz1.dot(x)                                        # Hidden x N_Features\n",
    "            db1 = dz1                                               # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)                   #  batch_size x N_features\n",
    "            A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A2)/pass_length\n",
    "            # deltas\n",
    "            dZ2 = A2 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   #Output x hidden\n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)              # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              #Hidden x1                                            # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "      \n",
    "    def predict(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            for i in range(epochs):\n",
    "                if self.optimizer == 'SGD' :\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                elif self.optimizer == 'minibatch' :\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                else :\n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                if i%100 == 0 and i>0 :\n",
    "                    print(f'Loss after epoch {i} : {cost}')\n",
    "            print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "            costs.pop(0)\n",
    "            return costs  \n",
    "        else :\n",
    "            counter = 0\n",
    "            for i in range(epochs):\n",
    "                \n",
    "                if self.optimizer == 'SGD' :\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                        \n",
    "                elif self.optimizer == 'minibatch' :\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    \n",
    "                else :\n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    \n",
    "                if i%100 == 0 and i>0 :\n",
    "                    print(f'Loss after epoch {i} : {cost}')\n",
    "            costs.pop(0)\n",
    "            return costs  \n",
    "            \n",
    "\n",
    "          \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "    def minibatch_size(self, n_samples):\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff538fbf",
   "metadata": {},
   "source": [
    "# Testing with Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc09cb",
   "metadata": {},
   "source": [
    "## Loading and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c5c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b23d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "t = to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3037d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5\n",
    "D = data.shape[1]\n",
    "K = len(set(target))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data ,target ,test_size=0.25)\n",
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995238c",
   "metadata": {},
   "source": [
    "## One Hidden Layer \n",
    "### Activation Function Tests :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17885aa",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa65c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sigmoid = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba53db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.3640715382224344\n",
      "Loss after epoch 200 : 0.3586844396037788\n",
      "Loss after epoch 300 : 0.35394463797726583\n",
      "Loss after epoch 400 : 0.34780761348635025\n",
      "Loss after epoch 500 : 0.34058388674540363\n",
      "Loss after epoch 600 : 0.33237661312354905\n",
      "Loss after epoch 700 : 0.32329589855085544\n",
      "Loss after epoch 800 : 0.3136145051785436\n",
      "Loss after epoch 900 : 0.30367710951708604\n",
      "Loss after epoch 1001 : 0.29390703026170084\n"
     ]
    }
   ],
   "source": [
    "c=nn_sigmoid.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d83813ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb150051",
   "metadata": {},
   "source": [
    "#### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e2e70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2637ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.26826755836104194\n",
      "Loss after epoch 200 : 0.22191966605465757\n",
      "Loss after epoch 300 : 0.1969476385679529\n",
      "Loss after epoch 400 : 0.1821942504105333\n",
      "Loss after epoch 500 : 0.17245754687658393\n",
      "Loss after epoch 600 : 0.16516604553311875\n",
      "Loss after epoch 700 : 0.1588449634046712\n",
      "Loss after epoch 800 : 0.15270039076857908\n",
      "Loss after epoch 900 : 0.14636666701290846\n",
      "Loss after epoch 1001 : 0.1398017332851927\n"
     ]
    }
   ],
   "source": [
    "c=nn_tanh.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f52318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6c244",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54129052",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db3c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.2772421482651626\n",
      "Loss after epoch 200 : 0.20820459012469716\n",
      "Loss after epoch 300 : 0.16880478701464785\n",
      "Loss after epoch 400 : 0.14683879567831162\n",
      "Loss after epoch 500 : 0.13142969535341795\n",
      "Loss after epoch 600 : 0.11859444569504309\n",
      "Loss after epoch 700 : 0.10690303675042401\n",
      "Loss after epoch 800 : 0.09620301855450754\n",
      "Loss after epoch 900 : 0.08663758568277846\n",
      "Loss after epoch 1001 : 0.07831268387740098\n"
     ]
    }
   ],
   "source": [
    "c=nn_relu.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43d8e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31205dee",
   "metadata": {},
   "source": [
    "### Conclusion :\n",
    "\n",
    "ReLU works better, need to confirm that later with Fashion-MNIST + multiple tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25401ba5",
   "metadata": {},
   "source": [
    "### Optimizer  Tests :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e95da",
   "metadata": {},
   "source": [
    "### SGD :\n",
    "\n",
    "Doing 1 sample each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb03f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_SGD = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='SGD',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "107a7a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.03644590836376605\n",
      "Loss after epoch 200 : 0.03126310387418847\n",
      "Loss after epoch 300 : 0.028409191117879873\n",
      "Loss after epoch 400 : 0.02642768319853237\n",
      "Loss after epoch 500 : 0.02498934933185719\n",
      "Loss after epoch 600 : 0.023898591466685614\n",
      "Loss after epoch 700 : 0.023058686486137\n",
      "Loss after epoch 800 : 0.022406135269015155\n",
      "Loss after epoch 900 : 0.02188033526305925\n",
      "Loss after epoch 1001 : 0.021458981480286438\n"
     ]
    }
   ],
   "source": [
    "c=nn_SGD.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "636f41f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_SGD.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bc022",
   "metadata": {},
   "source": [
    "### SGD :\n",
    "\n",
    "Testing different minibatch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8737e3",
   "metadata": {},
   "source": [
    "#### Minibatch size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3693e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mini2 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 2,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a6f5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.03498501193662961\n",
      "Loss after epoch 200 : 0.030997956386203628\n",
      "Loss after epoch 300 : 0.028381456388318512\n",
      "Loss after epoch 400 : 0.026301124526082156\n",
      "Loss after epoch 500 : 0.024757312578555766\n",
      "Loss after epoch 600 : 0.023589566256232566\n",
      "Loss after epoch 700 : 0.022671345773454105\n",
      "Loss after epoch 800 : 0.02193766161225882\n",
      "Loss after epoch 900 : 0.021332304528252292\n",
      "Loss after epoch 1001 : 0.020828909797608174\n"
     ]
    }
   ],
   "source": [
    "c=nn_mini2.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9f84c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711f802",
   "metadata": {},
   "source": [
    "#### Minibatch size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86abf565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.05614378971393492\n",
      "Loss after epoch 200 : 0.03485524970426049\n",
      "Loss after epoch 300 : 0.028809353818824934\n",
      "Loss after epoch 400 : 0.0258319285608501\n",
      "Loss after epoch 500 : 0.0239676932092404\n",
      "Loss after epoch 600 : 0.02263837715982777\n",
      "Loss after epoch 700 : 0.021616757968266485\n",
      "Loss after epoch 800 : 0.02079679189109771\n",
      "Loss after epoch 900 : 0.020122050526230895\n",
      "Loss after epoch 1001 : 0.01956301073770052\n"
     ]
    }
   ],
   "source": [
    "nn_mini8 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 8,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini8.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "891fd386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini8.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf3c8e",
   "metadata": {},
   "source": [
    "#### minibatch size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "707d2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.08849296390155213\n",
      "Loss after epoch 200 : 0.04854915979496169\n",
      "Loss after epoch 300 : 0.035526781067759354\n",
      "Loss after epoch 400 : 0.02981107106832045\n",
      "Loss after epoch 500 : 0.02665975771049685\n",
      "Loss after epoch 600 : 0.024644336808873728\n",
      "Loss after epoch 700 : 0.0232193402866422\n",
      "Loss after epoch 800 : 0.022141612704066536\n",
      "Loss after epoch 900 : 0.021285697199933416\n",
      "Loss after epoch 1001 : 0.02058946117043143\n"
     ]
    }
   ],
   "source": [
    "nn_mini16 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 16,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini16.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6196aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini16.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d42315",
   "metadata": {},
   "source": [
    "#### Minibatch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3d45d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.14030598914379372\n",
      "Loss after epoch 200 : 0.09523366751736848\n",
      "Loss after epoch 300 : 0.06793508025521475\n",
      "Loss after epoch 400 : 0.05221431524291784\n",
      "Loss after epoch 500 : 0.04297771724585693\n",
      "Loss after epoch 600 : 0.03708850744334557\n",
      "Loss after epoch 700 : 0.033145614979128254\n",
      "Loss after epoch 800 : 0.030319966872596325\n",
      "Loss after epoch 900 : 0.02817798460143004\n",
      "Loss after epoch 1001 : 0.026510557840496624\n"
     ]
    }
   ],
   "source": [
    "nn_mini32 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 32,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini32.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf4b7c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini32.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d23f63",
   "metadata": {},
   "source": [
    "#### Minibatch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54786328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.2225649212479308\n",
      "Loss after epoch 200 : 0.1746783730727635\n",
      "Loss after epoch 300 : 0.1506769786770443\n",
      "Loss after epoch 400 : 0.13405347434497825\n",
      "Loss after epoch 500 : 0.12030784548970064\n",
      "Loss after epoch 600 : 0.10820255208821787\n",
      "Loss after epoch 700 : 0.09734836846972406\n",
      "Loss after epoch 800 : 0.08774881439710196\n",
      "Loss after epoch 900 : 0.07947163340046999\n",
      "Loss after epoch 1001 : 0.07252464554249426\n"
     ]
    }
   ],
   "source": [
    "nn_mini64 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 64,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini64.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "685e1984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini64.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1c891",
   "metadata": {},
   "source": [
    "#### Minibactch size not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77cb70ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.2686493950882637\n",
      "Loss after epoch 200 : 0.23478762078042306\n",
      "Loss after epoch 300 : 0.1961432855747637\n",
      "Loss after epoch 400 : 0.1580255189336089\n",
      "Loss after epoch 500 : 0.13503595447114933\n",
      "Loss after epoch 600 : 0.11954909916138183\n",
      "Loss after epoch 700 : 0.10722251722886818\n",
      "Loss after epoch 800 : 0.09667479075062574\n",
      "Loss after epoch 900 : 0.0874821954778473\n",
      "Loss after epoch 1001 : 0.07955369602104269\n"
     ]
    }
   ],
   "source": [
    "nn_mini = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               #batch_size = 64,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d9851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf1ee59",
   "metadata": {},
   "source": [
    "# Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1383e",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden1 x N_features\n",
    "- **b1**    : Hidden1\n",
    "- **W2**    : Hidden2 x Hidden1\n",
    "- **b2**    : Hidden2\n",
    "- **W3**    : Output x Hidden\n",
    "- **b3**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de86d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenTwo:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes_1,\n",
    "                 hidden_nodes_2,\n",
    "                 learning_rate,\n",
    "                 activation_hidden_1,\n",
    "                 activation_hidden_2,\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes_1 = hidden_nodes_1    \n",
    "        self.hidden_nodes_2 = hidden_nodes_2    \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden_1 = activation_hidden_1\n",
    "        self.activation_hidden_2 = activation_hidden_2\n",
    "        self.hidden_derivative_1 = derivative(self.activation_hidden_1)\n",
    "        self.hidden_derivative_2 = derivative(self.activation_hidden_2)\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "        # W1 of size hidden x features\n",
    "        n1 = self.input_nodes * self.hidden_nodes_1\n",
    "        self.W1 = tn.rvs(n1).reshape((self.hidden_nodes_1, self.input_nodes )) # hidden1 x features\n",
    "        # W2 of size hidden2 x hidden1\n",
    "        n2 = self.hidden_nodes_2 * self.hidden_nodes_1\n",
    "        self.W2 = tn.rvs(n2).reshape((self.hidden_nodes_2, self.hidden_nodes_1 )) # hidden1 x features\n",
    "        # W3 of size output x hidden2\n",
    "        n3 = self.hidden_nodes_2  * self.output_nodes\n",
    "        self.W3 = tn.rvs(n3).reshape((self.output_nodes, self.hidden_nodes_2 )) # output x hidden\n",
    "    \n",
    "    def create_biases(self):    \n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.b1 = tn.rvs(self.hidden_nodes_1).reshape(-1,1) \n",
    "        self.b2 = tn.rvs(self.hidden_nodes_2).reshape(-1,1) \n",
    "        self.b3 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1      # Hidden1 x N_samples\n",
    "        A1 = self.activation_hidden_1(Z1)      # Hidden1 x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2      # Hidden2 x N_samples\n",
    "        A2 = self.activation_hidden_2(Z2)      # Hidden2 x N_samples\n",
    "        Z3 = self.W3.dot(A2) + self.b3       # Output x N_samples\n",
    "        A3 = softmax(Z3)                     #Output x N_samples\n",
    "        return A3, Z3, A2, Z2, A1, Z1\n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # N_samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "     \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr*dW3\n",
    "        self.b3 -= lr*db3\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "      \n",
    "    def predict(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A3\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X_train)\n",
    "            cost = cross_entropy(target, A3)\n",
    "            costs.append(cost)\n",
    "            if i%100 == 0:\n",
    "                print(f'Loss after epoch {i} : {cost}')\n",
    "            self.backprop(X_train, target)\n",
    "        return costs  \n",
    "          \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "896a8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "               hidden_nodes_2 = M-1,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = tanh,\n",
    "               activation_hidden_2 = tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0f73c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 0.43052450075533344\n",
      "Loss after epoch 100 : 0.37957020204320085\n",
      "Loss after epoch 200 : 0.35210309707940923\n",
      "Loss after epoch 300 : 0.3188208696201698\n",
      "Loss after epoch 400 : 0.27298965046000545\n",
      "Loss after epoch 500 : 0.2314611247316099\n",
      "Loss after epoch 600 : 0.20380904135917347\n",
      "Loss after epoch 700 : 0.18670910956380002\n",
      "Loss after epoch 800 : 0.17531041770056072\n",
      "Loss after epoch 900 : 0.16647347546663915\n",
      "Loss after epoch 1000 : 0.15848237654815536\n",
      "Loss after epoch 1100 : 0.15051364355502048\n",
      "Loss after epoch 1200 : 0.1422375164659015\n",
      "Loss after epoch 1300 : 0.13358871541430414\n",
      "Loss after epoch 1400 : 0.12466753232923737\n",
      "Loss after epoch 1500 : 0.11568834497515186\n",
      "Loss after epoch 1600 : 0.10691950381148614\n",
      "Loss after epoch 1700 : 0.09861300736259554\n",
      "Loss after epoch 1800 : 0.09095137989637579\n",
      "Loss after epoch 1900 : 0.08402941976684392\n",
      "Loss after epoch 2000 : 0.07786546693216173\n",
      "Loss after epoch 2100 : 0.07242566397931635\n",
      "Loss after epoch 2200 : 0.06764772945342407\n",
      "Loss after epoch 2300 : 0.06345836724767609\n",
      "Loss after epoch 2400 : 0.05978385328755871\n",
      "Loss after epoch 2500 : 0.05655550136775975\n",
      "Loss after epoch 2600 : 0.05371192291616013\n",
      "Loss after epoch 2700 : 0.051199517684305515\n",
      "Loss after epoch 2800 : 0.0489720968405635\n",
      "Loss after epoch 2900 : 0.04699014468050877\n",
      "Loss after epoch 3000 : 0.04521997985152376\n",
      "Loss after epoch 3100 : 0.04363293941492002\n",
      "Loss after epoch 3200 : 0.04220463722825204\n",
      "Loss after epoch 3300 : 0.04091431277713008\n",
      "Loss after epoch 3400 : 0.03974427023478076\n",
      "Loss after epoch 3500 : 0.03867940061644125\n",
      "Loss after epoch 3600 : 0.03770677749807437\n",
      "Loss after epoch 3700 : 0.03681531645043247\n",
      "Loss after epoch 3800 : 0.0359954889299318\n",
      "Loss after epoch 3900 : 0.03523908231325747\n",
      "Loss after epoch 4000 : 0.034538998802832094\n",
      "Loss after epoch 4100 : 0.0338890869440032\n",
      "Loss after epoch 4200 : 0.03328400042744129\n",
      "Loss after epoch 4300 : 0.03271907968003122\n",
      "Loss after epoch 4400 : 0.03219025246941015\n",
      "Loss after epoch 4500 : 0.031693950364778366\n",
      "Loss after epoch 4600 : 0.031227038417555412\n",
      "Loss after epoch 4700 : 0.030786755859769605\n",
      "Loss after epoch 4800 : 0.030370665976488166\n",
      "Loss after epoch 4900 : 0.029976613602175146\n",
      "Loss after epoch 5000 : 0.02960268893091267\n",
      "Loss after epoch 5100 : 0.02924719652858564\n",
      "Loss after epoch 5200 : 0.028908628603373825\n",
      "Loss after epoch 5300 : 0.028585641740996585\n",
      "Loss after epoch 5400 : 0.028277036453484845\n",
      "Loss after epoch 5500 : 0.027981739031767833\n",
      "Loss after epoch 5600 : 0.02769878533433739\n",
      "Loss after epoch 5700 : 0.027427306280750186\n",
      "Loss after epoch 5800 : 0.027166514937183934\n",
      "Loss after epoch 5900 : 0.026915695165995287\n",
      "Loss after epoch 6000 : 0.026674191849448352\n",
      "Loss after epoch 6100 : 0.02644140268586102\n",
      "Loss after epoch 6200 : 0.026216771503314393\n",
      "Loss after epoch 6300 : 0.02599978296171484\n",
      "Loss after epoch 6400 : 0.025789958442895253\n",
      "Loss after epoch 6500 : 0.025586852881398905\n",
      "Loss after epoch 6600 : 0.02539005227647201\n",
      "Loss after epoch 6700 : 0.02519917164776812\n",
      "Loss after epoch 6800 : 0.025013853243574857\n",
      "Loss after epoch 6900 : 0.02483376486744316\n",
      "Loss after epoch 7000 : 0.02465859824445748\n",
      "Loss after epoch 7100 : 0.02448806739364014\n",
      "Loss after epoch 7200 : 0.024321907004526196\n",
      "Loss after epoch 7300 : 0.02415987083405642\n",
      "Loss after epoch 7400 : 0.024001730147202395\n",
      "Loss after epoch 7500 : 0.023847272224572773\n",
      "Loss after epoch 7600 : 0.023696298955869374\n",
      "Loss after epoch 7700 : 0.023548625531955795\n",
      "Loss after epoch 7800 : 0.023404079242134432\n",
      "Loss after epoch 7900 : 0.023262498377915386\n",
      "Loss after epoch 8000 : 0.02312373124048229\n",
      "Loss after epoch 8100 : 0.02298763524624968\n",
      "Loss after epoch 8200 : 0.02285407612320477\n",
      "Loss after epoch 8300 : 0.0227229271899101\n",
      "Loss after epoch 8400 : 0.022594068708867328\n",
      "Loss after epoch 8500 : 0.02246738730620244\n",
      "Loss after epoch 8600 : 0.022342775450158302\n",
      "Loss after epoch 8700 : 0.02222013098154397\n",
      "Loss after epoch 8800 : 0.022099356690008485\n",
      "Loss after epoch 8900 : 0.02198035993071742\n",
      "Loss after epoch 9000 : 0.021863052276682084\n",
      "Loss after epoch 9100 : 0.021747349202603548\n",
      "Loss after epoch 9200 : 0.021633169796641316\n",
      "Loss after epoch 9300 : 0.02152043649699493\n",
      "Loss after epoch 9400 : 0.021409074850604816\n",
      "Loss after epoch 9500 : 0.02129901329163818\n",
      "Loss after epoch 9600 : 0.02119018293773433\n",
      "Loss after epoch 9700 : 0.021082517402248455\n",
      "Loss after epoch 9800 : 0.02097595262096021\n",
      "Loss after epoch 9900 : 0.020870426691908184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43052450075533344,\n",
       " 0.42980217747232324,\n",
       " 0.4290849853891115,\n",
       " 0.428372899211334,\n",
       " 0.42766589358092827,\n",
       " 0.42696394308231145,\n",
       " 0.42626702224861635,\n",
       " 0.4255751055679585,\n",
       " 0.4248881674897048,\n",
       " 0.4242061824307125,\n",
       " 0.4235291247815108,\n",
       " 0.4228569689123941,\n",
       " 0.42218968917939914,\n",
       " 0.42152725993013734,\n",
       " 0.42086965550945254,\n",
       " 0.4202168502648793,\n",
       " 0.4195688185518741,\n",
       " 0.4189255347387933,\n",
       " 0.4182869732115942,\n",
       " 0.4176531083782351,\n",
       " 0.41702391467275357,\n",
       " 0.41639936655900006,\n",
       " 0.41577943853401,\n",
       " 0.41516410513099494,\n",
       " 0.4145533409219375,\n",
       " 0.41394712051977656,\n",
       " 0.41334541858016877,\n",
       " 0.4127482098028172,\n",
       " 0.4121554689323582,\n",
       " 0.41156717075879934,\n",
       " 0.41098329011750456,\n",
       " 0.41040380188872383,\n",
       " 0.409828680996666,\n",
       " 0.40925790240811794,\n",
       " 0.4086914411306114,\n",
       " 0.4081292722101447,\n",
       " 0.40757137072846705,\n",
       " 0.40701771179993174,\n",
       " 0.4064682705679339,\n",
       " 0.4059230222009423,\n",
       " 0.40538194188814164,\n",
       " 0.40484500483470026,\n",
       " 0.4043121862566824,\n",
       " 0.40378346137562376,\n",
       " 0.40325880541278974,\n",
       " 0.4027381935831403,\n",
       " 0.40222160108902105,\n",
       " 0.4017090031136081,\n",
       " 0.40120037481412685,\n",
       " 0.40069569131487354,\n",
       " 0.4001949277000631,\n",
       " 0.3996980590065294,\n",
       " 0.3992050602163057,\n",
       " 0.3987159062491103,\n",
       " 0.39823057195476497,\n",
       " 0.3977490321055709,\n",
       " 0.3972712613886705,\n",
       " 0.39679723439841974,\n",
       " 0.3963269256287958,\n",
       " 0.3958603094658657,\n",
       " 0.3953973601803403,\n",
       " 0.3949380519202355,\n",
       " 0.3944823587036654,\n",
       " 0.39403025441178596,\n",
       " 0.39358171278191345,\n",
       " 0.39313670740083445,\n",
       " 0.39269521169832655,\n",
       " 0.39225719894090777,\n",
       " 0.39182264222583013,\n",
       " 0.39139151447533277,\n",
       " 0.3909637884311674,\n",
       " 0.3905394366494105,\n",
       " 0.3901184314955706,\n",
       " 0.38970074514000363,\n",
       " 0.3892863495536425,\n",
       " 0.3888752165040506,\n",
       " 0.388467317551804,\n",
       " 0.3880626240472079,\n",
       " 0.3876611071273519,\n",
       " 0.3872627377135064,\n",
       " 0.3868674865088617,\n",
       " 0.38647532399661233,\n",
       " 0.3860862204383836,\n",
       " 0.385700145873002,\n",
       " 0.3853170701156059,\n",
       " 0.3849369627570954,\n",
       " 0.38455979316391425,\n",
       " 0.3841855304781651,\n",
       " 0.38381414361804794,\n",
       " 0.3834456012786192,\n",
       " 0.383079871932865,\n",
       " 0.382716923833081,\n",
       " 0.38235672501255374,\n",
       " 0.38199924328753354,\n",
       " 0.381644446259494,\n",
       " 0.3812923013176685,\n",
       " 0.3809427756418557,\n",
       " 0.3805958362054857,\n",
       " 0.3802514497789393,\n",
       " 0.3799095829331099,\n",
       " 0.37957020204320085,\n",
       " 0.379233273292749,\n",
       " 0.3788987626778655,\n",
       " 0.3785666360116853,\n",
       " 0.3782368589290173,\n",
       " 0.37790939689118574,\n",
       " 0.3775842151910543,\n",
       " 0.37726127895822625,\n",
       " 0.3769405531644106,\n",
       " 0.3766220026289477,\n",
       " 0.3763055920244851,\n",
       " 0.37599128588279845,\n",
       " 0.3756790486007476,\n",
       " 0.37536884444636226,\n",
       " 0.37506063756505104,\n",
       " 0.3747543919859246,\n",
       " 0.3744500716282298,\n",
       " 0.3741476403078871,\n",
       " 0.37384706174412446,\n",
       " 0.37354829956620433,\n",
       " 0.3732513173202365,\n",
       " 0.37295607847607265,\n",
       " 0.3726625464342773,\n",
       " 0.37237068453316996,\n",
       " 0.37208045605593565,\n",
       " 0.3717918242377967,\n",
       " 0.37150475227324503,\n",
       " 0.37121920332332714,\n",
       " 0.3709351405229819,\n",
       " 0.3706525269884238,\n",
       " 0.370371325824571,\n",
       " 0.3700915001325119,\n",
       " 0.3698130130170097,\n",
       " 0.36953582759403847,\n",
       " 0.3692599069983498,\n",
       " 0.36898521439106463,\n",
       " 0.36871171296728894,\n",
       " 0.36843936596374693,\n",
       " 0.36816813666643144,\n",
       " 0.3678979884182645,\n",
       " 0.3676288846267669,\n",
       " 0.3673607887717313,\n",
       " 0.36709366441289487,\n",
       " 0.36682747519760767,\n",
       " 0.3665621848684912,\n",
       " 0.3662977572710833,\n",
       " 0.36603415636146336,\n",
       " 0.3657713462138529,\n",
       " 0.3655092910281855,\n",
       " 0.3652479551376407,\n",
       " 0.36498730301613386,\n",
       " 0.3647272992857578,\n",
       " 0.364467908724166,\n",
       " 0.3642090962718931,\n",
       " 0.3639508270396022,\n",
       " 0.36369306631525267,\n",
       " 0.363435779571179,\n",
       " 0.36317893247107264,\n",
       " 0.3629224908768564,\n",
       " 0.362666420855444,\n",
       " 0.3624106886853728,\n",
       " 0.36215526086330235,\n",
       " 0.3619001041103659,\n",
       " 0.36164518537836754,\n",
       " 0.3613904718558116,\n",
       " 0.361135930973757,\n",
       " 0.3608815304114836,\n",
       " 0.36062723810196157,\n",
       " 0.36037302223711326,\n",
       " 0.36011885127285825,\n",
       " 0.3598646939339296,\n",
       " 0.3596105192184552,\n",
       " 0.35935629640229244,\n",
       " 0.359101995043109,\n",
       " 0.3588475849842023,\n",
       " 0.3585930363580487,\n",
       " 0.35833831958957757,\n",
       " 0.3580834053991644,\n",
       " 0.3578282648053362,\n",
       " 0.35757286912718833,\n",
       " 0.3573171899865079,\n",
       " 0.35706119930960195,\n",
       " 0.3568048693288313,\n",
       " 0.35654817258384935,\n",
       " 0.3562910819225483,\n",
       " 0.3560335705017157,\n",
       " 0.35577561178740624,\n",
       " 0.35551717955503376,\n",
       " 0.3552582478891917,\n",
       " 0.3549987911832088,\n",
       " 0.35473878413845233,\n",
       " 0.35447820176338646,\n",
       " 0.35421701937240146,\n",
       " 0.3539552125844247,\n",
       " 0.3536927573213292,\n",
       " 0.3534296298061551,\n",
       " 0.3531658065611603,\n",
       " 0.35290126440571856,\n",
       " 0.35263598045408184,\n",
       " 0.3523699321130277,\n",
       " 0.35210309707940923,\n",
       " 0.35183545333762867,\n",
       " 0.3515669791570545,\n",
       " 0.35129765308940186,\n",
       " 0.3510274539660972,\n",
       " 0.3507563608956466,\n",
       " 0.35048435326102856,\n",
       " 0.35021141071713024,\n",
       " 0.34993751318824584,\n",
       " 0.3496626408656567,\n",
       " 0.3493867742053086,\n",
       " 0.34910989392560626,\n",
       " 0.3488319810053366,\n",
       " 0.3485530166817397,\n",
       " 0.34827298244873767,\n",
       " 0.34799186005533606,\n",
       " 0.3477096315042072,\n",
       " 0.34742627905046636,\n",
       " 0.34714178520064876,\n",
       " 0.3468561327118941,\n",
       " 0.3465693045913444,\n",
       " 0.3462812840957598,\n",
       " 0.34599205473135464,\n",
       " 0.34570160025385527,\n",
       " 0.34540990466878013,\n",
       " 0.3451169522319411,\n",
       " 0.34482272745016357,\n",
       " 0.3445272150822212,\n",
       " 0.3442304001399827,\n",
       " 0.3439322678897625,\n",
       " 0.34363280385387174,\n",
       " 0.34333199381235996,\n",
       " 0.3430298238049406,\n",
       " 0.3427262801330918,\n",
       " 0.34242134936232216,\n",
       " 0.3421150183245935,\n",
       " 0.34180727412088874,\n",
       " 0.3414981041239152,\n",
       " 0.34118749598093323,\n",
       " 0.3408754376166983,\n",
       " 0.34056191723650636,\n",
       " 0.3402469233293314,\n",
       " 0.339930444671044,\n",
       " 0.33961247032770114,\n",
       " 0.3392929896588954,\n",
       " 0.3389719923211545,\n",
       " 0.3386494682713791,\n",
       " 0.338325407770312,\n",
       " 0.33799980138602653,\n",
       " 0.3376726399974262,\n",
       " 0.3373439147977465,\n",
       " 0.33701361729805085,\n",
       " 0.3366817393307118,\n",
       " 0.3363482730528706,\n",
       " 0.3360132109498671,\n",
       " 0.3356765458386338,\n",
       " 0.33533827087104723,\n",
       " 0.33499837953722916,\n",
       " 0.3346568656687949,\n",
       " 0.33431372344203913,\n",
       " 0.3339689473810579,\n",
       " 0.333622532360799,\n",
       " 0.3332744736100371,\n",
       " 0.3329247667142711,\n",
       " 0.33257340761853565,\n",
       " 0.33222039263012676,\n",
       " 0.33186571842123586,\n",
       " 0.3315093820314897,\n",
       " 0.33115138087039186,\n",
       " 0.3307917127196638,\n",
       " 0.33043037573548223,\n",
       " 0.3300673684506097,\n",
       " 0.3297026897764156,\n",
       " 0.32933633900478554,\n",
       " 0.32896831580991676,\n",
       " 0.32859862024999614,\n",
       " 0.32822725276875986,\n",
       " 0.32785421419693156,\n",
       " 0.32747950575353696,\n",
       " 0.32710312904709404,\n",
       " 0.3267250860766751,\n",
       " 0.3263453792328398,\n",
       " 0.3259640112984373,\n",
       " 0.3255809854492757,\n",
       " 0.3251963052546564,\n",
       " 0.3248099746777731,\n",
       " 0.3244219980759719,\n",
       " 0.32403238020087327,\n",
       " 0.3236411261983527,\n",
       " 0.32324824160837934,\n",
       " 0.32285373236471104,\n",
       " 0.32245760479444535,\n",
       " 0.3220598656174243,\n",
       " 0.32166052194549277,\n",
       " 0.3212595812816087,\n",
       " 0.32085705151880506,\n",
       " 0.32045294093900256,\n",
       " 0.3200472582116723,\n",
       " 0.3196400123923471,\n",
       " 0.3192312129209833,\n",
       " 0.3188208696201698,\n",
       " 0.3184089926931863,\n",
       " 0.31799559272190936,\n",
       " 0.31758068066456735,\n",
       " 0.3171642678533428,\n",
       " 0.31674636599182465,\n",
       " 0.31632698715230817,\n",
       " 0.3159061437729457,\n",
       " 0.31548384865474594,\n",
       " 0.3150601149584259,\n",
       " 0.31463495620111254,\n",
       " 0.31420838625289893,\n",
       " 0.31378041933325257,\n",
       " 0.31335107000728024,\n",
       " 0.31292035318184797,\n",
       " 0.31248828410155954,\n",
       " 0.3120548783445947,\n",
       " 0.3116201518184081,\n",
       " 0.31118412075529145,\n",
       " 0.31074680170780133,\n",
       " 0.3103082115440535,\n",
       " 0.3098683674428872,\n",
       " 0.3094272868889011,\n",
       " 0.3089849876673631,\n",
       " 0.3085414878589979,\n",
       " 0.308096805834653,\n",
       " 0.30765096024984784,\n",
       " 0.3072039700392077,\n",
       " 0.3067558544107858,\n",
       " 0.30630663284027676,\n",
       " 0.3058563250651247,\n",
       " 0.30540495107852844,\n",
       " 0.3049525311233487,\n",
       " 0.30449908568591877,\n",
       " 0.3040446354897643,\n",
       " 0.3035892014892336,\n",
       " 0.3031328048630445,\n",
       " 0.3026754670077492,\n",
       " 0.3022172095311226,\n",
       " 0.3017580542454771,\n",
       " 0.3012980231609078,\n",
       " 0.3008371384784735,\n",
       " 0.30037542258331434,\n",
       " 0.299912898037714,\n",
       " 0.29944958757410756,\n",
       " 0.2989855140880409,\n",
       " 0.29852070063108455,\n",
       " 0.29805517040370755,\n",
       " 0.29758894674811415,\n",
       " 0.29712205314104845,\n",
       " 0.29665451318657104,\n",
       " 0.2961863506088118,\n",
       " 0.29571758924470287,\n",
       " 0.29524825303669633,\n",
       " 0.29477836602547103,\n",
       " 0.2943079523426324,\n",
       " 0.2938370362034091,\n",
       " 0.2933656418993521,\n",
       " 0.29289379379103825,\n",
       " 0.2924215163007848,\n",
       " 0.29194883390537696,\n",
       " 0.2914757711288138,\n",
       " 0.2910023525350763,\n",
       " 0.2905286027209207,\n",
       " 0.29005454630870264,\n",
       " 0.2895802079392348,\n",
       " 0.2891056122646813,\n",
       " 0.2886307839414951,\n",
       " 0.28815574762339946,\n",
       " 0.2876805279544182,\n",
       " 0.2872051495619585,\n",
       " 0.2867296370499501,\n",
       " 0.2862540149920426,\n",
       " 0.28577830792486675,\n",
       " 0.28530254034136027,\n",
       " 0.2848267366841644,\n",
       " 0.2843509213390914,\n",
       " 0.283875118628668,\n",
       " 0.2833993528057568,\n",
       " 0.2829236480472594,\n",
       " 0.282448028447902,\n",
       " 0.28197251801410944,\n",
       " 0.2814971406579666,\n",
       " 0.28102192019127253,\n",
       " 0.28054688031968794,\n",
       " 0.28007204463697943,\n",
       " 0.2795974366193616,\n",
       " 0.27912307961994015,\n",
       " 0.27864899686325756,\n",
       " 0.27817521143994245,\n",
       " 0.27770174630146616,\n",
       " 0.27722862425500655,\n",
       " 0.27675586795842116,\n",
       " 0.27628349991533196,\n",
       " 0.2758115424703215,\n",
       " 0.2753400178042436,\n",
       " 0.27486894792964783,\n",
       " 0.2743983546863209,\n",
       " 0.2739282597369438,\n",
       " 0.2734586845628672,\n",
       " 0.27298965046000545,\n",
       " 0.2725211785348491,\n",
       " 0.27205328970059756,\n",
       " 0.27158600467341226,\n",
       " 0.27111934396878956,\n",
       " 0.27065332789805513,\n",
       " 0.2701879765649794,\n",
       " 0.2697233098625142,\n",
       " 0.26925934746965036,\n",
       " 0.26879610884839655,\n",
       " 0.26833361324087984,\n",
       " 0.2678718796665659,\n",
       " 0.26741092691960044,\n",
       " 0.2669507735662708,\n",
       " 0.26649143794258656,\n",
       " 0.26603293815198015,\n",
       " 0.26557529206312463,\n",
       " 0.26511851730787067,\n",
       " 0.2646626312792993,\n",
       " 0.2642076511298917,\n",
       " 0.26375359376981367,\n",
       " 0.26330047586531463,\n",
       " 0.2628483138372401,\n",
       " 0.2623971238596558,\n",
       " 0.2619469218585836,\n",
       " 0.2614977235108465,\n",
       " 0.26104954424302274,\n",
       " 0.26060239923050715,\n",
       " 0.2601563033966783,\n",
       " 0.2597112714121702,\n",
       " 0.2592673176942476,\n",
       " 0.2588244564062818,\n",
       " 0.25838270145732833,\n",
       " 0.2579420665018013,\n",
       " 0.25750256493924595,\n",
       " 0.2570642099142066,\n",
       " 0.2566270143161872,\n",
       " 0.2561909907797053,\n",
       " 0.25575615168443494,\n",
       " 0.2553225091554395,\n",
       " 0.25489007506349004,\n",
       " 0.25445886102546966,\n",
       " 0.2540288784048603,\n",
       " 0.25360013831231193,\n",
       " 0.25317265160629027,\n",
       " 0.25274642889380283,\n",
       " 0.25232148053120096,\n",
       " 0.2518978166250555,\n",
       " 0.25147544703310526,\n",
       " 0.2510543813652748,\n",
       " 0.25063462898476174,\n",
       " 0.25021619900918984,\n",
       " 0.24979910031182706,\n",
       " 0.24938334152286654,\n",
       " 0.24896893103076798,\n",
       " 0.2485558769836589,\n",
       " 0.24814418729079207,\n",
       " 0.24773386962405927,\n",
       " 0.2473249314195576,\n",
       " 0.24691737987920825,\n",
       " 0.24651122197242445,\n",
       " 0.24610646443782777,\n",
       " 0.24570311378501045,\n",
       " 0.24530117629634207,\n",
       " 0.24490065802881902,\n",
       " 0.2445015648159546,\n",
       " 0.24410390226970852,\n",
       " 0.24370767578245342,\n",
       " 0.24331289052897734,\n",
       " 0.24291955146852015,\n",
       " 0.24252766334684234,\n",
       " 0.24213723069832413,\n",
       " 0.24174825784809437,\n",
       " 0.24136074891418627,\n",
       " 0.24097470780971908,\n",
       " 0.24059013824510456,\n",
       " 0.2402070437302759,\n",
       " 0.23982542757693798,\n",
       " 0.23944529290083746,\n",
       " 0.23906664262405125,\n",
       " 0.2386894794772923,\n",
       " 0.23831380600223015,\n",
       " 0.23793962455382658,\n",
       " 0.23756693730268333,\n",
       " 0.23719574623740172,\n",
       " 0.23682605316695268,\n",
       " 0.2364578597230551,\n",
       " 0.23609116736256264,\n",
       " 0.23572597736985657,\n",
       " 0.23536229085924396,\n",
       " 0.23500010877736063,\n",
       " 0.23463943190557618,\n",
       " 0.2342802608624019,\n",
       " 0.2339225961058991,\n",
       " 0.23356643793608742,\n",
       " 0.23321178649735191,\n",
       " 0.232858641780848,\n",
       " 0.23250700362690363,\n",
       " 0.23215687172741667,\n",
       " 0.23180824562824828,\n",
       " 0.2314611247316099,\n",
       " 0.23111550829844363,\n",
       " 0.2307713954507952,\n",
       " 0.23042878517417886,\n",
       " 0.23008767631993282,\n",
       " 0.2297480676075654,\n",
       " 0.22940995762709018,\n",
       " 0.2290733448413501,\n",
       " 0.22873822758833035,\n",
       " 0.2284046040834573,\n",
       " 0.22807247242188597,\n",
       " 0.22774183058077246,\n",
       " 0.227412676421533,\n",
       " 0.22708500769208761,\n",
       " 0.22675882202908845,\n",
       " 0.2264341169601328,\n",
       " 0.2261108899059589,\n",
       " 0.22578913818262594,\n",
       " 0.22546885900367586,\n",
       " 0.22515004948227826,\n",
       " 0.2248327066333568,\n",
       " 0.22451682737569761,\n",
       " 0.2242024085340383,\n",
       " 0.22388944684113876,\n",
       " 0.22357793893983166,\n",
       " 0.22326788138505388,\n",
       " 0.2229592706458576,\n",
       " 0.22265210310740147,\n",
       " 0.2223463750729208,\n",
       " 0.22204208276567786,\n",
       " 0.2217392223308901,\n",
       " 0.2214377898376386,\n",
       " 0.22113778128075412,\n",
       " 0.2208391925826825,\n",
       " 0.22054201959532788,\n",
       " 0.2202462581018747,\n",
       " 0.21995190381858756,\n",
       " 0.21965895239658928,\n",
       " 0.219367399423617,\n",
       " 0.21907724042575616,\n",
       " 0.21878847086915204,\n",
       " 0.21850108616169966,\n",
       " 0.21821508165471104,\n",
       " 0.2179304526445604,\n",
       " 0.21764719437430682,\n",
       " 0.2173653020352949,\n",
       " 0.21708477076873312,\n",
       " 0.21680559566724988,\n",
       " 0.21652777177642732,\n",
       " 0.21625129409631294,\n",
       " 0.21597615758290903,\n",
       " 0.2157023571496404,\n",
       " 0.21542988766879947,\n",
       " 0.21515874397296986,\n",
       " 0.21488892085642816,\n",
       " 0.21462041307652355,\n",
       " 0.2143532153550359,\n",
       " 0.2140873223795127,\n",
       " 0.2138227288045837,\n",
       " 0.21355942925325505,\n",
       " 0.21329741831818164,\n",
       " 0.21303669056291857,\n",
       " 0.21277724052315156,\n",
       " 0.21251906270790652,\n",
       " 0.21226215160073864,\n",
       " 0.21200650166090065,\n",
       " 0.21175210732449085,\n",
       " 0.21149896300558102,\n",
       " 0.21124706309732377,\n",
       " 0.2109964019730408,\n",
       " 0.21074697398729073,\n",
       " 0.21049877347691756,\n",
       " 0.21025179476208009,\n",
       " 0.21000603214726152,\n",
       " 0.20976147992226077,\n",
       " 0.20951813236316402,\n",
       " 0.20927598373329873,\n",
       " 0.20903502828416828,\n",
       " 0.2087952602563686,\n",
       " 0.20855667388048732,\n",
       " 0.2083192633779838,\n",
       " 0.20808302296205278,\n",
       " 0.20784794683846988,\n",
       " 0.20761402920641975,\n",
       " 0.20738126425930742,\n",
       " 0.20714964618555268,\n",
       " 0.20691916916936778,\n",
       " 0.20668982739151862,\n",
       " 0.20646161503006968,\n",
       " 0.20623452626111294,\n",
       " 0.20600855525948103,\n",
       " 0.2057836961994444,\n",
       " 0.20555994325539345,\n",
       " 0.20533729060250513,\n",
       " 0.2051157324173946,\n",
       " 0.20489526287875207,\n",
       " 0.20467587616796534,\n",
       " 0.20445756646972701,\n",
       " 0.2042403279726284,\n",
       " 0.20402415486973935,\n",
       " 0.20380904135917347,\n",
       " 0.2035949816446407,\n",
       " 0.203381969935986,\n",
       " 0.20317000044971484,\n",
       " 0.20295906740950573,\n",
       " 0.20274916504671006,\n",
       " 0.20254028760083925,\n",
       " 0.20233242932003956,\n",
       " 0.20212558446155438,\n",
       " 0.20191974729217513,\n",
       " 0.20171491208867953,\n",
       " 0.20151107313825922,\n",
       " 0.20130822473893456,\n",
       " 0.20110636119996,\n",
       " 0.20090547684221646,\n",
       " 0.20070556599859463,\n",
       " 0.20050662301436584,\n",
       " 0.200308642247544,\n",
       " 0.20011161806923608,\n",
       " 0.1999155448639832,\n",
       " 0.19972041703009105,\n",
       " 0.19952622897995104,\n",
       " 0.19933297514035203,\n",
       " 0.19914064995278158,\n",
       " 0.19894924787371918,\n",
       " 0.19875876337491957,\n",
       " 0.19856919094368747,\n",
       " 0.1983805250831434,\n",
       " 0.19819276031248081,\n",
       " 0.19800589116721515,\n",
       " 0.19781991219942424,\n",
       " 0.1976348179779808,\n",
       " 0.19745060308877682,\n",
       " 0.19726726213494025,\n",
       " 0.19708478973704402,\n",
       " 0.19690318053330755,\n",
       " 0.1967224291797908,\n",
       " 0.19654253035058136,\n",
       " 0.1963634787379743,\n",
       " 0.19618526905264505,\n",
       " 0.19600789602381571,\n",
       " 0.19583135439941465,\n",
       " 0.19565563894622953,\n",
       " 0.1954807444500538,\n",
       " 0.195306665715828,\n",
       " 0.19513339756777304,\n",
       " 0.19496093484951973,\n",
       " 0.19478927242423075,\n",
       " 0.1946184051747179,\n",
       " 0.19444832800355347,\n",
       " 0.194279035833176,\n",
       " 0.19411052360599118,\n",
       " 0.19394278628446687,\n",
       " 0.19377581885122375,\n",
       " 0.19360961630912063,\n",
       " 0.19344417368133468,\n",
       " 0.1932794860114375,\n",
       " 0.19311554836346603,\n",
       " 0.19295235582198933,\n",
       " 0.1927899034921706,\n",
       " 0.19262818649982524,\n",
       " 0.1924671999914744,\n",
       " 0.1923069391343947,\n",
       " 0.1921473991166637,\n",
       " 0.19198857514720166,\n",
       " 0.19183046245580956,\n",
       " 0.19167305629320305,\n",
       " 0.1915163519310433,\n",
       " 0.19136034466196397,\n",
       " 0.19120502979959492,\n",
       " 0.19105040267858234,\n",
       " 0.19089645865460633,\n",
       " 0.19074319310439414,\n",
       " 0.19059060142573184,\n",
       " 0.1904386790374713,\n",
       " 0.19028742137953603,\n",
       " 0.19013682391292266,\n",
       " 0.1899868821197008,\n",
       " 0.18983759150300947,\n",
       " 0.18968894758705143,\n",
       " 0.1895409459170848,\n",
       " 0.1893935820594121,\n",
       " 0.1892468516013673,\n",
       " 0.18910075015130023,\n",
       " 0.18895527333855883,\n",
       " 0.18881041681346944,\n",
       " 0.18866617624731472,\n",
       " 0.18852254733230978,\n",
       " 0.188379525781576,\n",
       " 0.18823710732911358,\n",
       " 0.18809528772977158,\n",
       " 0.18795406275921647,\n",
       " 0.18781342821389904,\n",
       " 0.1876733799110196,\n",
       " 0.1875339136884914,\n",
       " 0.18739502540490274,\n",
       " 0.18725671093947738,\n",
       " 0.18711896619203391,\n",
       " 0.18698178708294294,\n",
       " 0.18684516955308397,\n",
       " 0.18670910956380002,\n",
       " 0.18657360309685173,\n",
       " 0.18643864615436975,\n",
       " 0.18630423475880656,\n",
       " 0.18617036495288644,\n",
       " 0.18603703279955516,\n",
       " 0.18590423438192794,\n",
       " 0.18577196580323704,\n",
       " 0.18564022318677809,\n",
       " 0.18550900267585563,\n",
       " 0.18537830043372763,\n",
       " 0.18524811264354954,\n",
       " 0.1851184355083173,\n",
       " 0.1849892652508096,\n",
       " 0.18486059811352962,\n",
       " 0.1847324303586459,\n",
       " 0.1846047582679327,\n",
       " 0.18447757814270985,\n",
       " 0.18435088630378163,\n",
       " 0.18422467909137566,\n",
       " 0.18409895286508088,\n",
       " 0.18397370400378513,\n",
       " 0.18384892890561236,\n",
       " 0.18372462398785944,\n",
       " 0.18360078568693244,\n",
       " 0.18347741045828253,\n",
       " 0.18335449477634175,\n",
       " 0.18323203513445813,\n",
       " 0.1831100280448309,\n",
       " 0.18298847003844507,\n",
       " 0.1828673576650059,\n",
       " 0.18274668749287318,\n",
       " 0.18262645610899525,\n",
       " 0.18250666011884278,\n",
       " 0.18238729614634247,\n",
       " 0.18226836083381054,\n",
       " 0.18214985084188604,\n",
       " 0.18203176284946423,\n",
       " 0.1819140935536295,\n",
       " 0.18179683966958873,\n",
       " 0.1816799979306039,\n",
       " 0.18156356508792532,\n",
       " 0.18144753791072438,\n",
       " 0.18133191318602634,\n",
       " 0.18121668771864344,\n",
       " 0.1811018583311074,\n",
       " 0.18098742186360242,\n",
       " 0.18087337517389818,\n",
       " 0.18075971513728245,\n",
       " 0.18064643864649432,\n",
       " 0.18053354261165694,\n",
       " 0.18042102396021073,\n",
       " 0.18030887963684658,\n",
       " 0.18019710660343888,\n",
       " 0.18008570183897885,\n",
       " 0.17997466233950804,\n",
       " 0.1798639851180518,\n",
       " 0.1797536672045527,\n",
       " 0.1796437056458045,\n",
       " 0.17953409750538596,\n",
       " 0.17942483986359453,\n",
       " 0.17931592981738087,\n",
       " 0.17920736448028274,\n",
       " 0.1790991409823596,\n",
       " 0.17899125647012715,\n",
       " 0.17888370810649182,\n",
       " 0.1787764930706858,\n",
       " 0.178669608558202,\n",
       " 0.178563051780729,\n",
       " 0.17845681996608692,\n",
       " 0.17835091035816222,\n",
       " 0.17824532021684378,\n",
       " 0.1781400468179587,\n",
       " 0.1780350874532082,\n",
       " 0.17793043943010373,\n",
       " 0.17782610007190366,\n",
       " 0.1777220667175494,\n",
       " 0.17761833672160224,\n",
       " 0.17751490745418033,\n",
       " 0.17741177630089572,\n",
       " 0.17730894066279143,\n",
       " 0.17720639795627904,\n",
       " 0.17710414561307622,\n",
       " 0.1770021810801444,\n",
       " 0.176900501819627,\n",
       " 0.17679910530878717,\n",
       " 0.17669798903994624,\n",
       " 0.17659715052042244,\n",
       " 0.17649658727246909,\n",
       " 0.17639629683321376,\n",
       " 0.17629627675459703,\n",
       " 0.17619652460331192,\n",
       " 0.17609703796074289,\n",
       " 0.17599781442290552,\n",
       " 0.17589885160038612,\n",
       " 0.1758001471182816,\n",
       " 0.17570169861613932,\n",
       " 0.1756035037478973,\n",
       " 0.1755055601818246,\n",
       " 0.17540786560046162,\n",
       " 0.17531041770056072,\n",
       " 0.17521321419302716,\n",
       " 0.17511625280285972,\n",
       " 0.17501953126909203,\n",
       " 0.1749230473447336,\n",
       " 0.17482679879671123,\n",
       " 0.1747307834058103,\n",
       " 0.17463499896661686,\n",
       " 0.1745394432874588,\n",
       " 0.17444411419034825,\n",
       " 0.17434900951092325,\n",
       " 0.17425412709839008,\n",
       " 0.17415946481546557,\n",
       " 0.1740650205383195,\n",
       " 0.17397079215651687,\n",
       " 0.1738767775729611,\n",
       " 0.17378297470383633,\n",
       " 0.17368938147855037,\n",
       " 0.17359599583967814,\n",
       " 0.17350281574290416,\n",
       " 0.17340983915696645,\n",
       " 0.17331706406359923,\n",
       " 0.173224488457477,\n",
       " 0.1731321103461576,\n",
       " 0.1730399277500263,\n",
       " 0.1729479387022392,\n",
       " 0.17285614124866747,\n",
       " 0.17276453344784098,\n",
       " 0.17267311337089272,\n",
       " 0.17258187910150272,\n",
       " 0.17249082873584246,\n",
       " 0.17239996038251915,\n",
       " 0.17230927216252034,\n",
       " 0.1722187622091583,\n",
       " 0.17212842866801475,\n",
       " 0.17203826969688565,\n",
       " 0.17194828346572594,\n",
       " 0.1718584681565945,\n",
       " 0.17176882196359905,\n",
       " 0.17167934309284144,\n",
       " 0.17159002976236273,\n",
       " 0.17150088020208826,\n",
       " 0.17141189265377324,\n",
       " 0.17132306537094819,\n",
       " 0.17123439661886414,\n",
       " 0.17114588467443856,\n",
       " 0.17105752782620093,\n",
       " 0.17096932437423837,\n",
       " 0.17088127263014158,\n",
       " 0.17079337091695074,\n",
       " 0.17070561756910158,\n",
       " 0.17061801093237117,\n",
       " 0.17053054936382453,\n",
       " 0.1704432312317604,\n",
       " 0.17035605491565792,\n",
       " 0.1702690188061228,\n",
       " 0.17018212130483393,\n",
       " 0.17009536082448978,\n",
       " 0.1700087357887552,\n",
       " 0.16992224463220804,\n",
       " 0.16983588580028594,\n",
       " 0.16974965774923345,\n",
       " 0.16966355894604854,\n",
       " 0.16957758786843002,\n",
       " 0.16949174300472447,\n",
       " 0.16940602285387363,\n",
       " 0.1693204259253615,\n",
       " 0.1692349507391619,\n",
       " 0.16914959582568567,\n",
       " 0.16906435972572864,\n",
       " 0.1689792409904188,\n",
       " 0.1688942381811644,\n",
       " 0.1688093498696015,\n",
       " 0.168724574637542,\n",
       " 0.16863991107692172,\n",
       " 0.16855535778974828,\n",
       " 0.16847091338804948,\n",
       " 0.16838657649382127,\n",
       " 0.16830234573897654,\n",
       " 0.1682182197652932,\n",
       " 0.1681341972243629,\n",
       " 0.16805027677753973,\n",
       " 0.1679664570958888,\n",
       " 0.16788273686013527,\n",
       " 0.1677991147606132,\n",
       " 0.16771558949721457,\n",
       " 0.16763215977933854,\n",
       " 0.1675488243258406,\n",
       " 0.1674655818649821,\n",
       " 0.16738243113437937,\n",
       " 0.1672993708809537,\n",
       " 0.16721639986088088,\n",
       " 0.16713351683954084,\n",
       " 0.16705072059146767,\n",
       " 0.16696800990029986,\n",
       " 0.1668853835587301,\n",
       " 0.16680284036845552,\n",
       " 0.1667203791401284,\n",
       " 0.16663799869330642,\n",
       " 0.16655569785640323,\n",
       " 0.16647347546663915,\n",
       " 0.16639133036999237,\n",
       " 0.1663092614211494,\n",
       " 0.16622726748345656,\n",
       " 0.16614534742887102,\n",
       " 0.16606350013791216,\n",
       " 0.1659817244996131,\n",
       " 0.16590001941147237,\n",
       " 0.1658183837794053,\n",
       " 0.16573681651769656,\n",
       " 0.16565531654895135,\n",
       " 0.16557388280404825,\n",
       " 0.16549251422209113,\n",
       " 0.16541120975036147,\n",
       " 0.16532996834427133,\n",
       " 0.16524878896731554,\n",
       " 0.16516767059102508,\n",
       " 0.16508661219491957,\n",
       " 0.16500561276646075,\n",
       " 0.1649246713010054,\n",
       " 0.16484378680175923,\n",
       " 0.1647629582797301,\n",
       " 0.16468218475368163,\n",
       " 0.16460146525008762,\n",
       " 0.16452079880308537,\n",
       " 0.16444018445443045,\n",
       " 0.1643596212534507,\n",
       " 0.1642791082570007,\n",
       " 0.16419864452941668,\n",
       " 0.16411822914247104,\n",
       " 0.1640378611753274,\n",
       " 0.16395753971449578,\n",
       " 0.1638772638537878,\n",
       " 0.16379703269427223,\n",
       " 0.16371684534423048,\n",
       " 0.16363670091911242,\n",
       " 0.1635565985414923,\n",
       " 0.16347653734102485,\n",
       " 0.16339651645440167,\n",
       " 0.16331653502530755,\n",
       " 0.16323659220437697,\n",
       " 0.1631566871491512,\n",
       " 0.163076819024035,\n",
       " 0.16299698700025383,\n",
       " 0.16291719025581108,\n",
       " 0.1628374279754457,\n",
       " 0.1627576993505897,\n",
       " 0.16267800357932613,\n",
       " 0.1625983398663468,\n",
       " 0.16251870742291086,\n",
       " 0.16243910546680287,\n",
       " 0.16235953322229138,\n",
       " 0.16227998992008782,\n",
       " 0.16220047479730518,\n",
       " 0.16212098709741726,\n",
       " 0.16204152607021785,\n",
       " 0.16196209097178027,\n",
       " 0.1618826810644169,\n",
       " 0.16180329561663906,\n",
       " 0.16172393390311707,\n",
       " 0.16164459520464036,\n",
       " 0.16156527880807778,\n",
       " 0.16148598400633837,\n",
       " 0.16140671009833216,\n",
       " 0.1613274563889307,\n",
       " 0.1612482221889286,\n",
       " 0.1611690068150048,\n",
       " 0.16108980958968389,\n",
       " 0.16101062984129794,\n",
       " 0.1609314669039486,\n",
       " 0.1608523201174688,\n",
       " 0.16077318882738523,\n",
       " 0.16069407238488095,\n",
       " 0.1606149701467578,\n",
       " 0.1605358814753995,\n",
       " 0.16045680573873455,\n",
       " 0.16037774231019947,\n",
       " 0.16029869056870247,\n",
       " 0.1602196498985868,\n",
       " 0.1601406196895948,\n",
       " 0.1600615993368319,\n",
       " 0.15998258824073075,\n",
       " 0.15990358580701566,\n",
       " 0.1598245914466675,\n",
       " 0.1597456045758881,\n",
       " 0.15966662461606562,\n",
       " 0.1595876509937395,\n",
       " 0.15950868314056615,\n",
       " 0.1594297204932843,\n",
       " 0.15935076249368102,\n",
       " 0.15927180858855763,\n",
       " 0.15919285822969584,\n",
       " 0.1591139108738244,\n",
       " 0.15903496598258543,\n",
       " 0.15895602302250142,\n",
       " 0.15887708146494214,\n",
       " 0.1587981407860919,\n",
       " 0.15871920046691693,\n",
       " 0.15864025999313291,\n",
       " 0.15856131885517288,\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.run(X_train, y_train_cat, epochs=10000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1fa8d",
   "metadata": {},
   "source": [
    "# Loading fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2b19461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "23b2ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "99734569",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9ed1c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c21c18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[1]\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "74a82098",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(N_train, M*M, 1).squeeze()\n",
    "X_test = X_test.reshape(N_test, M*M, 1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a492dca",
   "metadata": {},
   "source": [
    "# Fashion MNIST with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a4825774",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f8fbdeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e5dd3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "93df5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 255\n",
    "X_train = X_train/ MAX\n",
    "X_test =X_test/ MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "07aa8737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "83c5af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "                delta_stop = 1e-7,\n",
    "                patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "55c00157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.05975585039746326\n"
     ]
    }
   ],
   "source": [
    "c = nn.run(X_train, y_train_cat, epochs=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "dfe542f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7943\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e3628e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "               hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "45a3633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 0.18843736743928718\n",
      "Loss after epoch 100 : 0.16592613469823828\n",
      "Loss after epoch 200 : 0.13875456927553473\n",
      "Loss after epoch 300 : 0.12651550170459827\n",
      "Loss after epoch 400 : 0.11982636770117769\n",
      "Loss after epoch 500 : 0.11525604453050124\n",
      "Loss after epoch 600 : 0.11176647256412618\n",
      "Loss after epoch 700 : 0.10896720689308095\n",
      "Loss after epoch 800 : 0.10668667748392244\n",
      "Loss after epoch 900 : 0.1047578062852407\n",
      "Loss after epoch 1000 : 0.10314623283878621\n",
      "Loss after epoch 1100 : 0.1017866798824543\n",
      "Loss after epoch 1200 : 0.1006526212316377\n",
      "Loss after epoch 1300 : 0.09967661168776303\n",
      "Loss after epoch 1400 : 0.09883177916143256\n",
      "Loss after epoch 1500 : 0.09807348522416472\n",
      "Loss after epoch 1600 : 0.09738700272392299\n",
      "Loss after epoch 1700 : 0.09675913008143594\n",
      "Loss after epoch 1800 : 0.09614779454789515\n",
      "Loss after epoch 1900 : 0.09557841675297843\n",
      "Loss after epoch 2000 : 0.09504176539328933\n",
      "Loss after epoch 2100 : 0.09455086220246572\n",
      "Loss after epoch 2200 : 0.09404775998001495\n",
      "Loss after epoch 2300 : 0.0935739413195061\n",
      "Loss after epoch 2400 : 0.09311643253027024\n",
      "Loss after epoch 2500 : 0.09265761909725345\n",
      "Loss after epoch 2600 : 0.09221290297606767\n",
      "Loss after epoch 2700 : 0.09178553408231646\n",
      "Loss after epoch 2800 : 0.09136239677269395\n",
      "Loss after epoch 2900 : 0.09094799188898957\n",
      "Loss after epoch 3000 : 0.09056435943329544\n",
      "Loss after epoch 3100 : 0.09017775299667102\n",
      "Loss after epoch 3200 : 0.08978470370823555\n",
      "Loss after epoch 3300 : 0.08940894035984141\n",
      "Loss after epoch 3400 : 0.08905325902500358\n",
      "Loss after epoch 3500 : 0.08868128006954192\n",
      "Loss after epoch 3600 : 0.08831916721810006\n",
      "Loss after epoch 3700 : 0.08796403749562592\n",
      "Loss after epoch 3800 : 0.08762845611539413\n",
      "Loss after epoch 3900 : 0.08729371655420333\n",
      "Loss after epoch 4000 : 0.08697064365982267\n",
      "Loss after epoch 4100 : 0.08665002779970375\n",
      "Loss after epoch 4200 : 0.08633235949917782\n",
      "Loss after epoch 4300 : 0.0860183190199416\n",
      "Loss after epoch 4400 : 0.08569352677481479\n",
      "Loss after epoch 4500 : 0.0853927408827997\n",
      "Loss after epoch 4600 : 0.08509385732847112\n",
      "Loss after epoch 4700 : 0.08480694198926494\n",
      "Loss after epoch 4800 : 0.08452492589029628\n",
      "Loss after epoch 4900 : 0.08423802832772753\n",
      "Loss after epoch 5000 : 0.08396605031380565\n",
      "Loss after epoch 5100 : 0.08369914550938969\n",
      "Loss after epoch 5200 : 0.08343564680910881\n",
      "Loss after epoch 5300 : 0.08318304296998921\n",
      "Loss after epoch 5400 : 0.08293168241508087\n",
      "Loss after epoch 5500 : 0.08269156563663828\n",
      "Loss after epoch 5600 : 0.08244571234208425\n",
      "Loss after epoch 5700 : 0.08221481076992111\n",
      "Loss after epoch 5800 : 0.08199474347028773\n",
      "Loss after epoch 5900 : 0.08178256747817522\n",
      "Loss after epoch 6000 : 0.08157699422941901\n",
      "Loss after epoch 6100 : 0.08136276521951476\n",
      "Loss after epoch 6200 : 0.0811573558481067\n",
      "Loss after epoch 6300 : 0.08095764624727125\n",
      "Loss after epoch 6400 : 0.08077317136927613\n",
      "Loss after epoch 6500 : 0.08057215489407805\n",
      "Loss after epoch 6600 : 0.08038817819025114\n",
      "Loss after epoch 6700 : 0.08020495505889245\n",
      "Loss after epoch 6800 : 0.08001958949686928\n",
      "Loss after epoch 6900 : 0.07983717337944803\n",
      "Loss after epoch 7000 : 0.07967498527825564\n",
      "Loss after epoch 7100 : 0.07951389351740196\n",
      "Loss after epoch 7200 : 0.07934553552532683\n",
      "Loss after epoch 7300 : 0.0791875980068836\n",
      "Loss after epoch 7400 : 0.07901886508829885\n",
      "Loss after epoch 7500 : 0.07886164893636231\n",
      "Loss after epoch 7600 : 0.07870731800638726\n",
      "Loss after epoch 7700 : 0.07854805124778151\n",
      "Loss after epoch 7800 : 0.0783968246012325\n",
      "Loss after epoch 7900 : 0.07825925372548181\n",
      "Loss after epoch 8000 : 0.07811647207643355\n",
      "Loss after epoch 8100 : 0.07797678203825054\n",
      "Loss after epoch 8200 : 0.07784557240394575\n",
      "Loss after epoch 8300 : 0.07771602563575598\n",
      "Loss after epoch 8400 : 0.07759754962132816\n",
      "Loss after epoch 8500 : 0.07747095261621845\n",
      "Loss after epoch 8600 : 0.07734094930197451\n",
      "Loss after epoch 8700 : 0.07721874125741632\n",
      "Loss after epoch 8800 : 0.07710213216643519\n",
      "Loss after epoch 8900 : 0.07699379811570334\n",
      "Loss after epoch 9000 : 0.07687325065807628\n",
      "Loss after epoch 9100 : 0.07674837973128198\n",
      "Loss after epoch 9200 : 0.0766387101961159\n",
      "Loss after epoch 9300 : 0.07651817096091547\n",
      "Loss after epoch 9400 : 0.0764064939385823\n",
      "Loss after epoch 9500 : 0.07630108155001289\n",
      "Loss after epoch 9600 : 0.07619203494718636\n",
      "Loss after epoch 9700 : 0.07608253647730168\n",
      "Loss after epoch 9800 : 0.07598068021094952\n",
      "Loss after epoch 9900 : 0.07588768236102994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18843736743928718,\n",
       " 0.18345465176131273,\n",
       " 0.17854443977547052,\n",
       " 0.17731379869242403,\n",
       " 0.17912367607988655,\n",
       " 0.1784227857420287,\n",
       " 0.18495501790625385,\n",
       " 0.18166183388831336,\n",
       " 0.18667794158128753,\n",
       " 0.1795715014498672,\n",
       " 0.17882431947870614,\n",
       " 0.17847113642184476,\n",
       " 0.18559499226651674,\n",
       " 0.1797201622667603,\n",
       " 0.181297993392288,\n",
       " 0.1818614678226376,\n",
       " 0.18692276367246194,\n",
       " 0.1823598412557701,\n",
       " 0.17754359666743783,\n",
       " 0.17641766188693117,\n",
       " 0.17897136491084922,\n",
       " 0.17538635597730332,\n",
       " 0.17963376212126506,\n",
       " 0.18158344422580788,\n",
       " 0.18983756832842197,\n",
       " 0.18326430418254028,\n",
       " 0.17770226264189778,\n",
       " 0.17692717107994757,\n",
       " 0.17995604496518755,\n",
       " 0.17433475779403082,\n",
       " 0.17496609060707932,\n",
       " 0.17525938591261372,\n",
       " 0.1814073868464169,\n",
       " 0.1760611235950174,\n",
       " 0.17845453332493888,\n",
       " 0.17873404298119872,\n",
       " 0.1833151182737271,\n",
       " 0.17844336623526083,\n",
       " 0.17478081326114908,\n",
       " 0.17184479426594557,\n",
       " 0.1727221246159429,\n",
       " 0.17223072749895332,\n",
       " 0.18126414685808784,\n",
       " 0.17938232325883538,\n",
       " 0.19358416965003802,\n",
       " 0.17565988047569966,\n",
       " 0.17035634231948704,\n",
       " 0.1700541700752825,\n",
       " 0.1772130896174163,\n",
       " 0.17790388161648887,\n",
       " 0.19553598464635663,\n",
       " 0.1753207561354923,\n",
       " 0.17256579543924672,\n",
       " 0.1684539103804672,\n",
       " 0.17295751051638567,\n",
       " 0.1770407202860386,\n",
       " 0.19396136973221811,\n",
       " 0.17592586036836272,\n",
       " 0.17207396955041085,\n",
       " 0.16762804623593866,\n",
       " 0.16684112741526558,\n",
       " 0.1670681096497452,\n",
       " 0.17411845709765317,\n",
       " 0.17453279512493802,\n",
       " 0.1931923307267939,\n",
       " 0.17485239109780987,\n",
       " 0.17713932161215531,\n",
       " 0.17033609839005023,\n",
       " 0.16977386985554485,\n",
       " 0.1632949116955858,\n",
       " 0.16215782460551278,\n",
       " 0.1628904257584885,\n",
       " 0.16641208993348672,\n",
       " 0.17055719434330202,\n",
       " 0.1884556315757496,\n",
       " 0.17591152756210532,\n",
       " 0.1886452219830823,\n",
       " 0.18486849954464374,\n",
       " 0.1821420021244633,\n",
       " 0.18106516871853837,\n",
       " 0.18259686122775734,\n",
       " 0.16845539544056345,\n",
       " 0.1602921657190089,\n",
       " 0.15899516516955658,\n",
       " 0.1589293705488149,\n",
       " 0.15954216753722267,\n",
       " 0.16112269629991202,\n",
       " 0.16696183413775548,\n",
       " 0.170653682270108,\n",
       " 0.19251428150074962,\n",
       " 0.17143355203568725,\n",
       " 0.1846903753985546,\n",
       " 0.16954538518231269,\n",
       " 0.17317752904378933,\n",
       " 0.16596074424418017,\n",
       " 0.16685777733874904,\n",
       " 0.1575239146748688,\n",
       " 0.15658400244338694,\n",
       " 0.15744256999494308,\n",
       " 0.1608388608478793,\n",
       " 0.16592613469823828,\n",
       " 0.183176411179633,\n",
       " 0.17410850570686623,\n",
       " 0.19748813861780934,\n",
       " 0.1704157172712891,\n",
       " 0.17268065750796754,\n",
       " 0.1661440999455395,\n",
       " 0.16040403886905477,\n",
       " 0.15661859429895528,\n",
       " 0.1595100724041196,\n",
       " 0.16252713463966206,\n",
       " 0.17482850037376768,\n",
       " 0.17094438928234912,\n",
       " 0.1920472563093511,\n",
       " 0.16686213793427676,\n",
       " 0.17458393011282136,\n",
       " 0.1646402398000289,\n",
       " 0.16830039017667783,\n",
       " 0.15344716389580415,\n",
       " 0.15203482297778012,\n",
       " 0.1522093237103445,\n",
       " 0.1537674018520183,\n",
       " 0.15520427365244413,\n",
       " 0.16076585120930806,\n",
       " 0.1610026598016089,\n",
       " 0.17325971122668243,\n",
       " 0.16945920668309528,\n",
       " 0.18945629753841253,\n",
       " 0.16559765624548234,\n",
       " 0.17725595909158776,\n",
       " 0.16200197154610577,\n",
       " 0.17022316359073086,\n",
       " 0.16027588544116975,\n",
       " 0.16705214218197365,\n",
       " 0.15629483485996926,\n",
       " 0.1606371745576551,\n",
       " 0.15410931566142153,\n",
       " 0.15782069687514996,\n",
       " 0.15790597791819297,\n",
       " 0.16487782349642358,\n",
       " 0.1610641270944681,\n",
       " 0.17001263512393852,\n",
       " 0.16267520505549718,\n",
       " 0.17192364698314233,\n",
       " 0.16157821385438548,\n",
       " 0.1691968686402777,\n",
       " 0.16035778328923153,\n",
       " 0.16650082915244518,\n",
       " 0.1586617045275056,\n",
       " 0.16341709661578804,\n",
       " 0.15766467524813457,\n",
       " 0.16162934659375924,\n",
       " 0.15642101550441093,\n",
       " 0.15957731583340332,\n",
       " 0.15535869605211663,\n",
       " 0.15794201934752,\n",
       " 0.15419350482455488,\n",
       " 0.1562502690453472,\n",
       " 0.15301675415326602,\n",
       " 0.1546423340681693,\n",
       " 0.15189649806309585,\n",
       " 0.1531737180931879,\n",
       " 0.15075107608572588,\n",
       " 0.1517439864064011,\n",
       " 0.14962984975119617,\n",
       " 0.15041927206717273,\n",
       " 0.14853772071606514,\n",
       " 0.14916621631401575,\n",
       " 0.14744196567052822,\n",
       " 0.14792991032025157,\n",
       " 0.1463605184769782,\n",
       " 0.14674318133114259,\n",
       " 0.14535621402954044,\n",
       " 0.1456733232556406,\n",
       " 0.1444467266736763,\n",
       " 0.14472423033988582,\n",
       " 0.14363852093998272,\n",
       " 0.14390413433602783,\n",
       " 0.1429105067879721,\n",
       " 0.14318746520943026,\n",
       " 0.14227701600389667,\n",
       " 0.14257584150414526,\n",
       " 0.14171713153521073,\n",
       " 0.142047048572222,\n",
       " 0.14123059578392572,\n",
       " 0.14160496998240535,\n",
       " 0.14080445086754315,\n",
       " 0.14123243330797808,\n",
       " 0.1404355349019449,\n",
       " 0.1409177017105207,\n",
       " 0.140109238619181,\n",
       " 0.14066272525305232,\n",
       " 0.13981677844686186,\n",
       " 0.14044604599844696,\n",
       " 0.13954404656778396,\n",
       " 0.14026212519183875,\n",
       " 0.13927867684689205,\n",
       " 0.1400971530204002,\n",
       " 0.13901554951063202,\n",
       " 0.13995539153436473,\n",
       " 0.13875456927553473,\n",
       " 0.13982591215993684,\n",
       " 0.13848633400547977,\n",
       " 0.13971640893827203,\n",
       " 0.1382321507410548,\n",
       " 0.139649153347947,\n",
       " 0.1380246652150754,\n",
       " 0.13965672577996743,\n",
       " 0.13789771371393572,\n",
       " 0.13978244163622774,\n",
       " 0.13789385103945262,\n",
       " 0.14003361039485035,\n",
       " 0.13804169043094136,\n",
       " 0.1403388982115474,\n",
       " 0.13835645913389888,\n",
       " 0.14067557621984172,\n",
       " 0.1387727601251729,\n",
       " 0.14091715222619128,\n",
       " 0.1391414902893368,\n",
       " 0.14093292953796957,\n",
       " 0.13932065714360173,\n",
       " 0.14067726537546255,\n",
       " 0.1391895277723437,\n",
       " 0.14017913357261902,\n",
       " 0.1387676771410622,\n",
       " 0.13950437607305963,\n",
       " 0.13814420147739676,\n",
       " 0.13875724415412824,\n",
       " 0.13740432901168453,\n",
       " 0.1380135669606996,\n",
       " 0.13665505728060068,\n",
       " 0.13730798619422585,\n",
       " 0.13592740548621984,\n",
       " 0.13669769378136667,\n",
       " 0.13526318078020552,\n",
       " 0.13615001929625095,\n",
       " 0.13466779295137832,\n",
       " 0.13569352494342055,\n",
       " 0.13416427165686148,\n",
       " 0.13535133583544975,\n",
       " 0.13376932063784963,\n",
       " 0.13512838957216827,\n",
       " 0.1335007607096952,\n",
       " 0.1350341435291631,\n",
       " 0.13335277923130578,\n",
       " 0.1350390923679222,\n",
       " 0.13332371023671152,\n",
       " 0.13511918207291007,\n",
       " 0.13337101126048378,\n",
       " 0.13520995367366995,\n",
       " 0.13343143005833613,\n",
       " 0.13523898555172542,\n",
       " 0.1334467129672062,\n",
       " 0.1351458550857084,\n",
       " 0.1333684797470033,\n",
       " 0.13488904871720925,\n",
       " 0.13316485941522468,\n",
       " 0.13449311454726096,\n",
       " 0.13285177471908582,\n",
       " 0.13398482491857389,\n",
       " 0.13243861219398378,\n",
       " 0.13339824371664244,\n",
       " 0.13197207962143326,\n",
       " 0.13279045642509443,\n",
       " 0.13148428141503252,\n",
       " 0.1321829183241952,\n",
       " 0.13099679929519042,\n",
       " 0.13160801583411424,\n",
       " 0.1305421853089263,\n",
       " 0.1310853780955761,\n",
       " 0.13011737438748824,\n",
       " 0.1306068965832635,\n",
       " 0.12972422384627494,\n",
       " 0.13018299510918138,\n",
       " 0.1293650274075979,\n",
       " 0.1297987962225689,\n",
       " 0.12903281042769374,\n",
       " 0.12945164277370322,\n",
       " 0.12873600403802746,\n",
       " 0.12915128251742886,\n",
       " 0.12846559726510667,\n",
       " 0.12888243308192748,\n",
       " 0.1282175496016282,\n",
       " 0.1286384959099594,\n",
       " 0.12798767244553863,\n",
       " 0.12841655275413819,\n",
       " 0.12777377596955258,\n",
       " 0.12821135979468118,\n",
       " 0.12757047845044756,\n",
       " 0.1280196574958673,\n",
       " 0.12737831353535026,\n",
       " 0.1278392155468173,\n",
       " 0.12719574024856675,\n",
       " 0.12766555735814655,\n",
       " 0.127018147731147,\n",
       " 0.12749678950798957,\n",
       " 0.12684696152550115,\n",
       " 0.12733342154637942,\n",
       " 0.12667995427169318,\n",
       " 0.12717511475691376,\n",
       " 0.12651550170459827,\n",
       " 0.12702003859329328,\n",
       " 0.12635513986608896,\n",
       " 0.1268673764859154,\n",
       " 0.1261963525545207,\n",
       " 0.1267158183825138,\n",
       " 0.1260393633426348,\n",
       " 0.12656349562850489,\n",
       " 0.12588209898360636,\n",
       " 0.12640777634269457,\n",
       " 0.12572491988467124,\n",
       " 0.12625117316168896,\n",
       " 0.12556717604090012,\n",
       " 0.12609367278461536,\n",
       " 0.12541015776625636,\n",
       " 0.1259372719292721,\n",
       " 0.12525315809763998,\n",
       " 0.125780612852071,\n",
       " 0.12509740826426705,\n",
       " 0.12562451367967645,\n",
       " 0.12494266511800849,\n",
       " 0.12546744048991657,\n",
       " 0.12478824092185918,\n",
       " 0.12531298244748718,\n",
       " 0.12463467371727051,\n",
       " 0.12515881306520912,\n",
       " 0.12448291202606201,\n",
       " 0.12500498518221873,\n",
       " 0.12433149868080551,\n",
       " 0.12485023376899496,\n",
       " 0.12418100061360356,\n",
       " 0.12470114620358422,\n",
       " 0.12403367555374042,\n",
       " 0.12455373885793679,\n",
       " 0.12388720920800891,\n",
       " 0.1244061610667353,\n",
       " 0.12374100763285295,\n",
       " 0.12425828472903379,\n",
       " 0.12359673197044606,\n",
       " 0.12411587561153155,\n",
       " 0.12345790687470248,\n",
       " 0.12398025406327547,\n",
       " 0.12332090200918479,\n",
       " 0.1238452407087773,\n",
       " 0.1231853142904613,\n",
       " 0.12370977917412526,\n",
       " 0.12304919978403132,\n",
       " 0.12357517827471544,\n",
       " 0.12291699749243802,\n",
       " 0.12344401785283936,\n",
       " 0.12278493231954453,\n",
       " 0.12330933342399467,\n",
       " 0.12265058469155166,\n",
       " 0.12317671960134195,\n",
       " 0.12251801584742013,\n",
       " 0.12304487503835146,\n",
       " 0.12238674998838736,\n",
       " 0.12291701174525842,\n",
       " 0.12225769051892242,\n",
       " 0.12279084432114125,\n",
       " 0.12213078624097497,\n",
       " 0.12266542079937676,\n",
       " 0.12200419956708689,\n",
       " 0.12253987799303956,\n",
       " 0.12187986706168505,\n",
       " 0.12241653836013824,\n",
       " 0.1217566288205972,\n",
       " 0.12229463475440154,\n",
       " 0.12163395842920179,\n",
       " 0.12217462836203674,\n",
       " 0.12151239746247755,\n",
       " 0.12205594385118594,\n",
       " 0.1213938440610784,\n",
       " 0.12193868763983182,\n",
       " 0.12127596474446557,\n",
       " 0.12182428974145337,\n",
       " 0.12116053244262187,\n",
       " 0.12171188982217601,\n",
       " 0.12104562831691515,\n",
       " 0.12160004437588294,\n",
       " 0.12093088147396758,\n",
       " 0.12148667675233238,\n",
       " 0.12081537308819476,\n",
       " 0.12137308348343578,\n",
       " 0.12070014664185327,\n",
       " 0.12126048444533628,\n",
       " 0.12058597659162408,\n",
       " 0.12114979791568485,\n",
       " 0.12047518650831825,\n",
       " 0.12104221516286491,\n",
       " 0.12036508352259935,\n",
       " 0.12093652885392886,\n",
       " 0.12025685321892275,\n",
       " 0.12083083528778531,\n",
       " 0.12014772971714309,\n",
       " 0.12072411632448415,\n",
       " 0.12004085465744804,\n",
       " 0.1206195316741183,\n",
       " 0.11993404875497041,\n",
       " 0.12051384424269818,\n",
       " 0.11982636770117769,\n",
       " 0.12040833292340712,\n",
       " 0.11971876840165029,\n",
       " 0.12030214390499726,\n",
       " 0.11961126627720056,\n",
       " 0.12019445363154148,\n",
       " 0.11950246104274838,\n",
       " 0.12008663360082443,\n",
       " 0.11939543587234283,\n",
       " 0.11997929394033384,\n",
       " 0.11928735589908482,\n",
       " 0.11987353697879295,\n",
       " 0.1191835705942921,\n",
       " 0.11977357162709712,\n",
       " 0.11908129156507159,\n",
       " 0.11967532136556491,\n",
       " 0.11897991605694323,\n",
       " 0.1195758321634083,\n",
       " 0.11887957259606262,\n",
       " 0.11947883839693975,\n",
       " 0.11877992459278668,\n",
       " 0.11938036621467632,\n",
       " 0.11868132598306157,\n",
       " 0.11928503427467094,\n",
       " 0.11858418252564122,\n",
       " 0.11919241058514439,\n",
       " 0.11848939570200018,\n",
       " 0.11910030905702453,\n",
       " 0.11839398654087822,\n",
       " 0.11900753872433212,\n",
       " 0.11829701389681999,\n",
       " 0.11891368856239952,\n",
       " 0.11820266202252594,\n",
       " 0.11882092014066217,\n",
       " 0.11810778720012145,\n",
       " 0.118726416826384,\n",
       " 0.11801227803633618,\n",
       " 0.11863249982829288,\n",
       " 0.11791702125688702,\n",
       " 0.11853890106120674,\n",
       " 0.11782121867183967,\n",
       " 0.1184444261856795,\n",
       " 0.11772637310878478,\n",
       " 0.11835086824502668,\n",
       " 0.11763155787343002,\n",
       " 0.11825678899809948,\n",
       " 0.11753693741466513,\n",
       " 0.1181627940391867,\n",
       " 0.11744236718136102,\n",
       " 0.11807052196461086,\n",
       " 0.11734905266686282,\n",
       " 0.11797861613816372,\n",
       " 0.11725687135932462,\n",
       " 0.11788815934590734,\n",
       " 0.1171653731275249,\n",
       " 0.11779844187081583,\n",
       " 0.1170747643468041,\n",
       " 0.11770879909788991,\n",
       " 0.11698387932945278,\n",
       " 0.11761944362101732,\n",
       " 0.11689403655167632,\n",
       " 0.11753137512336344,\n",
       " 0.11680519871849113,\n",
       " 0.1174424612822485,\n",
       " 0.1167165847507872,\n",
       " 0.11735547319702738,\n",
       " 0.11662828643220606,\n",
       " 0.11727008440776993,\n",
       " 0.1165421645238782,\n",
       " 0.11718510449556556,\n",
       " 0.1164631251533805,\n",
       " 0.11711045675070642,\n",
       " 0.11638425749664984,\n",
       " 0.11703501363531077,\n",
       " 0.11630470169526513,\n",
       " 0.1169572733466957,\n",
       " 0.11622358316557378,\n",
       " 0.11687705577862667,\n",
       " 0.11614154163649247,\n",
       " 0.11679307697205109,\n",
       " 0.11605674814859282,\n",
       " 0.116707882013677,\n",
       " 0.1159706331647342,\n",
       " 0.1166217419567353,\n",
       " 0.1158869934710903,\n",
       " 0.11653876326571626,\n",
       " 0.11580564300616354,\n",
       " 0.11645914352657678,\n",
       " 0.11572549794550148,\n",
       " 0.11637991449275005,\n",
       " 0.11564701595360716,\n",
       " 0.11630362648087202,\n",
       " 0.11556982174035174,\n",
       " 0.11622750997665911,\n",
       " 0.11549186927482745,\n",
       " 0.11615069744739104,\n",
       " 0.11541330673368992,\n",
       " 0.11607331158625013,\n",
       " 0.11533498130993365,\n",
       " 0.11599497824924428,\n",
       " 0.11525604453050124,\n",
       " 0.11591687376847515,\n",
       " 0.11517793140979077,\n",
       " 0.11583931322542237,\n",
       " 0.11509908039678751,\n",
       " 0.11576064588476348,\n",
       " 0.11502122606027713,\n",
       " 0.11568274301421008,\n",
       " 0.11494464361335968,\n",
       " 0.11560605604138327,\n",
       " 0.1148674616746603,\n",
       " 0.115529487252668,\n",
       " 0.11478978474033191,\n",
       " 0.11545221059074529,\n",
       " 0.11471236747827274,\n",
       " 0.11537479954081464,\n",
       " 0.11463577975217826,\n",
       " 0.11529899789943018,\n",
       " 0.11456008110923474,\n",
       " 0.11522341321514708,\n",
       " 0.11448517841093009,\n",
       " 0.11514788985700851,\n",
       " 0.1144112084221699,\n",
       " 0.11507445779987927,\n",
       " 0.11433709958224161,\n",
       " 0.11499999224650895,\n",
       " 0.11426238431704334,\n",
       " 0.1149249793334524,\n",
       " 0.114188512928226,\n",
       " 0.11485056873169795,\n",
       " 0.11411443008626784,\n",
       " 0.11477570968408128,\n",
       " 0.11404039926588257,\n",
       " 0.11470067152739971,\n",
       " 0.11396562566333858,\n",
       " 0.11462407478063741,\n",
       " 0.11389000962185496,\n",
       " 0.11454737840821083,\n",
       " 0.11381462900822684,\n",
       " 0.11447134658135731,\n",
       " 0.11373955624884285,\n",
       " 0.11439485715682143,\n",
       " 0.1136651209360649,\n",
       " 0.11431933815292418,\n",
       " 0.11359169257785785,\n",
       " 0.11424591441056475,\n",
       " 0.11351895791616214,\n",
       " 0.11417181329646667,\n",
       " 0.11344767894179172,\n",
       " 0.11410035380344523,\n",
       " 0.11337713727909592,\n",
       " 0.11402963192931512,\n",
       " 0.11330753736603452,\n",
       " 0.11395934079418699,\n",
       " 0.1132377499674419,\n",
       " 0.11388862351212176,\n",
       " 0.11316836461565458,\n",
       " 0.11381914571646026,\n",
       " 0.11310101222369706,\n",
       " 0.11375178071305757,\n",
       " 0.1130343187269145,\n",
       " 0.11368479809510386,\n",
       " 0.11296915645110969,\n",
       " 0.11361951057326071,\n",
       " 0.11290299029912218,\n",
       " 0.11355173643611319,\n",
       " 0.11283676369488477,\n",
       " 0.11348542275858226,\n",
       " 0.11277100374767093,\n",
       " 0.11341975735608549,\n",
       " 0.11270490570788269,\n",
       " 0.11335112734985198,\n",
       " 0.11263973250819218,\n",
       " 0.11328570291567058,\n",
       " 0.11257508526179964,\n",
       " 0.11322123889304181,\n",
       " 0.11251521476694087,\n",
       " 0.11316283474573005,\n",
       " 0.1124552735195948,\n",
       " 0.11310275394520296,\n",
       " 0.11239557875349225,\n",
       " 0.1130425128506809,\n",
       " 0.11233541001142502,\n",
       " 0.1129821587071268,\n",
       " 0.112273343466977,\n",
       " 0.1129179828343724,\n",
       " 0.1122102055027941,\n",
       " 0.11285331212247855,\n",
       " 0.11214614188841535,\n",
       " 0.11278775977545985,\n",
       " 0.1120826678130261,\n",
       " 0.11272407543032838,\n",
       " 0.11202084748988178,\n",
       " 0.11266103765464358,\n",
       " 0.11195835569714407,\n",
       " 0.11259689342692868,\n",
       " 0.11189479100125242,\n",
       " 0.11253107084477613,\n",
       " 0.11183062984717648,\n",
       " 0.11246362016425578,\n",
       " 0.11176647256412618,\n",
       " 0.11239768396514414,\n",
       " 0.11170366771919114,\n",
       " 0.11233283928757136,\n",
       " 0.11164114755840533,\n",
       " 0.11226992989189351,\n",
       " 0.11157951240329426,\n",
       " 0.11220668179659944,\n",
       " 0.11151759867075402,\n",
       " 0.11214242022796313,\n",
       " 0.11145564474723658,\n",
       " 0.11207917231555417,\n",
       " 0.11139438477793602,\n",
       " 0.11201559509996022,\n",
       " 0.11133287116057075,\n",
       " 0.11195285464770716,\n",
       " 0.1112727722179115,\n",
       " 0.11189090623593496,\n",
       " 0.11121389078379802,\n",
       " 0.11183090556334802,\n",
       " 0.111156568110723,\n",
       " 0.11177248900106376,\n",
       " 0.11110066795847097,\n",
       " 0.11171590898477599,\n",
       " 0.11104441411858723,\n",
       " 0.11165812168160756,\n",
       " 0.11098796363009063,\n",
       " 0.11160021299759706,\n",
       " 0.11093079179141663,\n",
       " 0.11154103037856893,\n",
       " 0.11087335474039321,\n",
       " 0.1114816610363403,\n",
       " 0.11081675933597934,\n",
       " 0.1114234654610358,\n",
       " 0.11076082590844295,\n",
       " 0.11136616789560129,\n",
       " 0.1107040055897186,\n",
       " 0.11130750003702022,\n",
       " 0.11064648768462466,\n",
       " 0.11124779779901056,\n",
       " 0.11058863765788056,\n",
       " 0.11118697107061624,\n",
       " 0.1105303226949955,\n",
       " 0.11112625810653466,\n",
       " 0.1104726621201924,\n",
       " 0.11106607654680013,\n",
       " 0.11041438658289747,\n",
       " 0.11100523138060099,\n",
       " 0.11035737706561635,\n",
       " 0.11094637763582929,\n",
       " 0.11030234662186264,\n",
       " 0.11089000396510766,\n",
       " 0.11024714551138008,\n",
       " 0.11083249730119266,\n",
       " 0.11019119579695129,\n",
       " 0.11077430379719626,\n",
       " 0.11013489625073523,\n",
       " 0.11071527488747862,\n",
       " 0.11007883156840861,\n",
       " 0.11065691071546076,\n",
       " 0.11002239587933459,\n",
       " 0.11059799735974599,\n",
       " 0.10996678614388067,\n",
       " 0.11054043361448751,\n",
       " 0.10991122136873992,\n",
       " 0.11048240157155427,\n",
       " 0.10985787110409166,\n",
       " 0.11042756580290365,\n",
       " 0.10980557848088238,\n",
       " 0.11037322065212951,\n",
       " 0.10975359162631794,\n",
       " 0.11031976758535714,\n",
       " 0.10970166878867178,\n",
       " 0.11026573823397433,\n",
       " 0.10964961702348396,\n",
       " 0.11021155649806091,\n",
       " 0.10959708832031648,\n",
       " 0.11015651879168606,\n",
       " 0.10954478885055789,\n",
       " 0.11010101355957047,\n",
       " 0.10949088759281488,\n",
       " 0.11004388563704005,\n",
       " 0.10943669910124683,\n",
       " 0.1099868052082088,\n",
       " 0.1093834158074991,\n",
       " 0.1099305035028986,\n",
       " 0.10933015568091416,\n",
       " 0.10987470790466668,\n",
       " 0.1092772336933955,\n",
       " 0.10981972513358017,\n",
       " 0.10922465212784226,\n",
       " 0.10976460657482173,\n",
       " 0.10917254645539354,\n",
       " 0.10971055197503211,\n",
       " 0.10912179018376642,\n",
       " 0.10965755827844482,\n",
       " 0.1090702460027545,\n",
       " 0.10960299707448576,\n",
       " 0.10901855630866218,\n",
       " 0.10954861506475065,\n",
       " 0.10896720689308095,\n",
       " 0.10949478192818608,\n",
       " 0.10891482470166551,\n",
       " 0.10943900641867808,\n",
       " 0.10886334701882532,\n",
       " 0.10938458341200412,\n",
       " 0.1088123676689372,\n",
       " 0.10933118858993368,\n",
       " 0.10876223531550935,\n",
       " 0.10927865782803511,\n",
       " 0.10871371766342758,\n",
       " 0.10922857050159182,\n",
       " 0.1086656664723915,\n",
       " 0.10917825381124462,\n",
       " 0.10861908186105075,\n",
       " 0.10912942481887151,\n",
       " 0.10857296047980566,\n",
       " 0.10908102900603871,\n",
       " 0.10852622654148593,\n",
       " 0.1090316103333042,\n",
       " 0.10847902248343319,\n",
       " 0.10898213350565149,\n",
       " 0.10843185422106873,\n",
       " 0.10893309625791742,\n",
       " 0.10838410148661852,\n",
       " 0.10888154563339598,\n",
       " 0.10833586365446177,\n",
       " 0.10883092169979679,\n",
       " 0.10828922974697981,\n",
       " 0.10878236890913234,\n",
       " 0.10824270964960898,\n",
       " 0.1087338471676293,\n",
       " 0.1081975943126725,\n",
       " 0.10868636180071997,\n",
       " 0.10815233855946738,\n",
       " 0.10863835511981773,\n",
       " 0.1081055414184635,\n",
       " 0.1085883040232158,\n",
       " 0.1080580600394728,\n",
       " 0.10853763038700567,\n",
       " 0.10801023468407235,\n",
       " 0.10848614830324811,\n",
       " 0.1079615812060746,\n",
       " 0.10843352307543051,\n",
       " 0.10791179729378812,\n",
       " 0.10838033059985128,\n",
       " 0.107863624981478,\n",
       " 0.10832986075275351,\n",
       " 0.10781614913393614,\n",
       " 0.10827983886044897,\n",
       " 0.10776998284582053,\n",
       " 0.10823108882761248,\n",
       " 0.10772434587016108,\n",
       " 0.10818283081484445,\n",
       " 0.10767925143681877,\n",
       " 0.10813520296074393,\n",
       " 0.10763381240279662,\n",
       " 0.10808676960377492,\n",
       " 0.10758870742150671,\n",
       " 0.1080388123648612,\n",
       " 0.10754374850171915,\n",
       " 0.10799098580800635,\n",
       " 0.10749893878861412,\n",
       " 0.10794286040154714,\n",
       " 0.10745396800619665,\n",
       " 0.1078947297987288,\n",
       " 0.10740831865039412,\n",
       " 0.10784557637278834,\n",
       " 0.1073656110791709,\n",
       " 0.10780113045270376,\n",
       " 0.10732326997716964,\n",
       " 0.10775631930801997,\n",
       " 0.10728043232558498,\n",
       " 0.10771099253255677,\n",
       " 0.10723718915724036,\n",
       " 0.10766617246713413,\n",
       " 0.10719734039489137,\n",
       " 0.10762463762980229,\n",
       " 0.10715772515050188,\n",
       " 0.10758244644186005,\n",
       " 0.10711674474385965,\n",
       " 0.10753852282564932,\n",
       " 0.10707416534793006,\n",
       " 0.1074924939816904,\n",
       " 0.10703144471356729,\n",
       " 0.1074493068841943,\n",
       " 0.10698911278313226,\n",
       " 0.10740369736551365,\n",
       " 0.1069479527052556,\n",
       " 0.10735998452242469,\n",
       " 0.10690599562869492,\n",
       " 0.1073148509698143,\n",
       " 0.10686400264478724,\n",
       " 0.10726901985885694,\n",
       " 0.10682064900518384,\n",
       " 0.10722228494072132,\n",
       " 0.10677654410329337,\n",
       " 0.10717552800393382,\n",
       " 0.10673202749153907,\n",
       " 0.1071270542892878,\n",
       " 0.10668667748392244,\n",
       " 0.10707804815799703,\n",
       " 0.10664317381684509,\n",
       " 0.10703130900277809,\n",
       " 0.10659879936649751,\n",
       " 0.1069837671355087,\n",
       " 0.10655604665372229,\n",
       " 0.10693850536430761,\n",
       " 0.10651436331164442,\n",
       " 0.10689374411073527,\n",
       " 0.10647269110322584,\n",
       " 0.10684909630654306,\n",
       " 0.10643110351260057,\n",
       " 0.10680462409822643,\n",
       " 0.10639008140576385,\n",
       " 0.10676059492313665,\n",
       " 0.10634852333247884,\n",
       " 0.10671568370440139,\n",
       " 0.10630724926235541,\n",
       " 0.106671792315637,\n",
       " 0.10626665010730796,\n",
       " 0.10662874593638545,\n",
       " 0.10622744583994034,\n",
       " 0.10658728728146365,\n",
       " 0.10618797244277133,\n",
       " 0.10654529519461871,\n",
       " 0.10614829713234503,\n",
       " 0.10650262628688326,\n",
       " 0.1061091328110471,\n",
       " 0.10646101722528994,\n",
       " 0.1060708968328897,\n",
       " 0.10642063136388086,\n",
       " 0.10603324288180759,\n",
       " 0.10638120773592077,\n",
       " 0.10599587538941012,\n",
       " 0.10634156334766383,\n",
       " 0.1059584567837895,\n",
       " 0.10630120287005494,\n",
       " 0.10591989198235822,\n",
       " 0.1062603103348929,\n",
       " 0.10588056186164116,\n",
       " 0.10621799081631839,\n",
       " 0.1058400786231792,\n",
       " 0.10617403733283826,\n",
       " 0.10580002809427601,\n",
       " 0.10613218843722025,\n",
       " 0.10576096174429828,\n",
       " 0.10608995788327953,\n",
       " 0.10572214752634135,\n",
       " 0.10604817404659794,\n",
       " 0.1056838099385144,\n",
       " 0.10600709720697188,\n",
       " 0.10564554440576533,\n",
       " 0.10596532169809121,\n",
       " 0.10560670717937322,\n",
       " 0.10592338353571258,\n",
       " 0.1055682641425892,\n",
       " 0.10588235127638045,\n",
       " 0.10552950339217833,\n",
       " 0.10584010415887878,\n",
       " 0.10549003948833491,\n",
       " 0.10579747975074819,\n",
       " 0.10545055524249752,\n",
       " 0.10575539213371579,\n",
       " 0.10541212848401435,\n",
       " 0.10571414060336465,\n",
       " 0.10537324725700306,\n",
       " 0.10567177493516325,\n",
       " 0.10533533190470906,\n",
       " 0.1056318695051621,\n",
       " 0.10529863141725496,\n",
       " 0.10559284166308631,\n",
       " 0.10526262578468995,\n",
       " 0.10555386283926971,\n",
       " 0.10522569909514946,\n",
       " 0.10551470359096221,\n",
       " 0.10518830947997584,\n",
       " 0.10547461170312367,\n",
       " 0.10515267168433524,\n",
       " 0.10543705044274504,\n",
       " 0.10511736168536516,\n",
       " 0.10539913771354405,\n",
       " 0.10508257797199319,\n",
       " 0.10536132316881966,\n",
       " 0.10504706526851097,\n",
       " 0.10532299511866361,\n",
       " 0.10501152890840752,\n",
       " 0.10528485791991142,\n",
       " 0.10497578832652738,\n",
       " 0.1052459443557097,\n",
       " 0.10493946648431672,\n",
       " 0.10520618314412272,\n",
       " 0.10490231408885511,\n",
       " 0.10516618513868267,\n",
       " 0.1048654222251274,\n",
       " 0.10512636288540252,\n",
       " 0.1048294391892816,\n",
       " 0.10508759382260022,\n",
       " 0.10479329848738274,\n",
       " 0.10504870112730542,\n",
       " 0.1047578062852407,\n",
       " 0.10501091099233685,\n",
       " 0.1047226192133564,\n",
       " 0.10497312439495071,\n",
       " 0.10468748015876735,\n",
       " 0.10493507899485706,\n",
       " 0.10465185165317378,\n",
       " 0.10489675368497445,\n",
       " 0.10461672455711168,\n",
       " 0.10485875853899558,\n",
       " 0.10458125622381925,\n",
       " 0.10482049183952885,\n",
       " 0.1045454725394986,\n",
       " 0.10478274341890756,\n",
       " 0.10451028715342844,\n",
       " 0.10474496780223623,\n",
       " 0.10447564967722771,\n",
       " 0.10470762336185287,\n",
       " 0.10444123420727396,\n",
       " 0.10467030651767342,\n",
       " 0.10440683251230438,\n",
       " 0.10463333827157632,\n",
       " 0.10437204485576498,\n",
       " 0.10459575690589697,\n",
       " 0.10433739364931371,\n",
       " 0.10455794962362404,\n",
       " 0.1043019767697438,\n",
       " 0.10451944857917138,\n",
       " 0.10426682786566002,\n",
       " 0.10448128351149011,\n",
       " 0.10423232218473873,\n",
       " 0.10444406837458135,\n",
       " 0.10419795865518812,\n",
       " 0.10440722753061624,\n",
       " 0.1041641878256283,\n",
       " 0.10437041195113778,\n",
       " 0.10412972749012833,\n",
       " 0.10433349295995378,\n",
       " 0.10409564871640467,\n",
       " 0.10429621775977263,\n",
       " 0.1040611612261233,\n",
       " 0.10425971123518128,\n",
       " 0.1040272314136624,\n",
       " 0.1042234051304495,\n",
       " 0.1039950819488855,\n",
       " 0.10418913408286874,\n",
       " 0.10396364161119863,\n",
       " 0.10415521015996639,\n",
       " 0.10393174124068423,\n",
       " 0.10412075602527586,\n",
       " 0.1038998646465808,\n",
       " 0.10408649971456295,\n",
       " 0.10386824112303124,\n",
       " 0.10405275472943336,\n",
       " 0.10383653058766681,\n",
       " 0.10401820403075453,\n",
       " 0.10380507880373643,\n",
       " 0.10398413544089957,\n",
       " 0.10377366395956435,\n",
       " 0.10395036702761093,\n",
       " 0.10374204045569567,\n",
       " 0.10391642517844792,\n",
       " 0.10371102934047255,\n",
       " 0.10388313043232815,\n",
       " 0.10368010783247877,\n",
       " 0.10384993799143193,\n",
       " 0.1036512298087886,\n",
       " 0.10381960034717408,\n",
       " 0.10362199071648022,\n",
       " 0.1037876237233139,\n",
       " 0.10359276227661063,\n",
       " 0.10375637157680939,\n",
       " 0.10356317670842201,\n",
       " 0.10372484992076916,\n",
       " 0.10353385724216138,\n",
       " 0.10369294747821328,\n",
       " 0.1035037618212108,\n",
       " 0.1036600862146999,\n",
       " 0.10347464996937868,\n",
       " 0.1036292346481401,\n",
       " 0.10344542541864603,\n",
       " 0.10359829011818035,\n",
       " 0.10341631196484657,\n",
       " 0.10356698274607323,\n",
       " 0.10338669342130039,\n",
       " 0.10353500462178437,\n",
       " 0.10335665942366604,\n",
       " 0.1035027195254264,\n",
       " 0.10332664399137753,\n",
       " 0.10347030917165771,\n",
       " 0.10329654025374524,\n",
       " 0.10343775243156292,\n",
       " 0.10326641719658233,\n",
       " 0.10340523727963098,\n",
       " 0.10323647444907012,\n",
       " 0.10337275526988096,\n",
       " 0.10320650135414602,\n",
       " 0.10334083016177693,\n",
       " 0.10317667552207038,\n",
       " 0.10330861350494978,\n",
       " ...]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.run(X_train, y_train_cat, epochs=10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "63b662cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7163\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "259e05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "# W1 of size hidden x features\n",
    "n = D * M\n",
    "W1 = tn.rvs(n).reshape((M, D )) # hidden x features\n",
    "# W2 of size output x hidden\n",
    "m = M  * K\n",
    "W2 = tn.rvs(m).reshape((K, M)) # output x hidden\n",
    "b1 = tn.rvs(M).reshape(-1,1) \n",
    "b2 = tn.rvs(K).reshape(-1,1) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4f97bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "Z1 = W1.dot(X.T) + b1 # Hidden x N_samples\n",
    "A1 = tanh(Z1)      # Hidden x N_samples\n",
    "Z2 = W2.dot(A1) + b2  # Output x N_samples\n",
    "A2 = softmax(Z2)      #Output x N_samples\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be0df2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8824e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88eed570",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "# deltas\n",
    "dZ2 = A2 - y_train_cat                                   #Output x N_samples\n",
    "dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "dZ1 = W2.T.dot(dZ2)*dt(Z1)     # Hidden x N_samples\n",
    "dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c721e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "lr = 0.01\n",
    "W2 -= lr*dW2\n",
    "b2 -= lr*db2\n",
    "W1 -= lr*dW1\n",
    "b1 -= lr*db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9935d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cross_entropy(y_train_cat, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c99fdfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23973236540442144"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc49b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            A2, Z2, A1, Z1 = self.forward(X_train)\n",
    "            cost = cross_entropy(target, A2)\n",
    "            costs.append(cost)\n",
    "            if i%100 == 0:\n",
    "                print(f'Loss after epoch {i} : {cost}')\n",
    "            self.backprop(X_train, target)\n",
    "        return costs  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
