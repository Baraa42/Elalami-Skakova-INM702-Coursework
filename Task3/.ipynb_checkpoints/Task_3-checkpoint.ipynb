{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69ce695",
   "metadata": {},
   "source": [
    "Used links:\n",
    "- https://intellipaat.com/community/9507/simple-multi-layer-neural-network-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97227f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7414c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions - forward \n",
    "#Sigmoid activation function with forward pass\n",
    "@np.vectorize\n",
    "def sigmoid (x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "#ReLU activation function with forward pass\n",
    "@np.vectorize\n",
    "def relu (x):\n",
    "  return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff71b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions - for backpropagation\n",
    "#Sigmoid activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_sigmoid (x):\n",
    "  return x*(1.0-x)\n",
    "\n",
    "#ReLU activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_relu (x):\n",
    "  if x<0:\n",
    "    return 0\n",
    "  if x>0:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d95b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss method - cross entropy\n",
    "def cross_entropy(output, target):\n",
    "    return -np.mean(target*np.log(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d6ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output function - softmax\n",
    "def softmax(x):\n",
    "    a = x - np.max(x, axis=0, keepdims=True)\n",
    "    new_a = np.exp(a)\n",
    "    result = new_a / np.new_a(new_a, axis=0, keepdims=True)\n",
    "#     exps = np.exp(X)\n",
    "#     s = exps / np.sum(exps)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4432939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Neural Networks for initiating an NN\n",
    "class NN:\n",
    "    def __init__ (self):\n",
    "        self.layers = []\n",
    "        self.n_layers = 0\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        \n",
    "    # function to add layers to Neural Network\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.n_layers += 1\n",
    "        \n",
    "     \n",
    "    def forward_pass(self, X_train):\n",
    "        for layer in self.layers:\n",
    "            op = layer.activation(X_train)\n",
    "        return op\n",
    "\n",
    "    \n",
    "     #method for activating layers in the backpropagation\n",
    "    def backprop(self, X, y, learning_rate):\n",
    "        # output forward pass\n",
    "        output = self.forward(X)\n",
    "\n",
    "        #backprop\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "\n",
    "            # output layer\n",
    "            if layer == self._layers[-1]:\n",
    "                layer.error = y - output\n",
    "                # The output = layer.last_activation in this case\n",
    "                layer.delta = layer.error * layer.d_final(output)\n",
    "            else:\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                layer.delta = layer.error * layer.d_fuction(layer.last_activation)\n",
    "\n",
    "        # Update the weights\n",
    "        for i in range(len(self._layers)):\n",
    "            layer = self._layers[i]\n",
    "            # The input is either the previous layers output or X itself (for the first hidden layer)\n",
    "            input_to_use = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)\n",
    "            layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "\n",
    "            \n",
    "    # training NN method\n",
    "    def train(self, X_train, y_train, learning_rate, epochs):    \n",
    "        # MSE list of errors\n",
    "        loss_across_epochs = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            for i in range(0,X_train.shape[0], batch_size):\n",
    "                #Extract train batch from X and Y\n",
    "                input_data = X_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "                labels = y_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "                #forward pass\n",
    "                output_data = self.forward_pass(X_train)\n",
    "                #calculate loss\n",
    "                loss = criterion(X_train, y_train)\n",
    "                #backpropagation\n",
    "                loss.backprop(X_train, y_train, learning_rate)\n",
    "                train_loss += loss.item() * batch_size\n",
    "                \n",
    "            print(\"Epoch: {} - Loss:{:.4f}\".format(epoch+1,train_loss ))\n",
    "            loss_across_epochs.extend([train_loss])\n",
    "\n",
    "        return loss_across_epochs\n",
    "    \n",
    "    #testing\n",
    "    def test (self, X_test, y_test): \n",
    "\n",
    "        output = self.forward(X_test)\n",
    "        y_pred = np.argmax(output, axis=0)\n",
    "        y_true = np.argmax(y_test, axis=0)\n",
    "        correct = 0\n",
    "        for pred, true in zip(y_pred, y_true):\n",
    "            correct += 1 if pred == true else 0\n",
    "        errors = y_test.shape[1] - correct\n",
    "        score = correct / y_test.shape[1]\n",
    "        \n",
    "        \n",
    "        print(f'Accuracy {score}')\n",
    "        print(f'Correct: {correct}')\n",
    "        print(f'Errors: {errors}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b11d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, function, weight, bias):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.function = function\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        \n",
    "        \n",
    "    #method returns layers bias\n",
    "    def f_bias (n_neurons):\n",
    "        bias = np.random.rand(n_neurons)\n",
    "        return bias \n",
    "\n",
    "    #method returns random number of weights of layers\n",
    "    def f_weight (n_inputs, n_neurons):\n",
    "        weight = np.random.rand(n_inputs, n_neurons)\n",
    "        return weight\n",
    "    \n",
    "    #forward pass calculation for each layer\n",
    "    def activation(self, n_inputs):\n",
    "        op = np.dot(n_inputs, self.weight) + self.bias\n",
    "        op = self.function(op)\n",
    "        return op\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
