{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a93c07",
   "metadata": {},
   "source": [
    "## Content:\n",
    "- [Part 1](#part1)- Importing the libraries, packages\n",
    "- [Part 2](#part2)- Useful Functions\n",
    "- [Part 3](#part3) -  One Hidden Layer Class\n",
    "- [Part 4](#part4) -  Two Hidden Layers Class \n",
    "- [Part 5](#part5) -  Loading Fashion MNIST\n",
    "- [Part 6](#part6) -  Fashion MNIST One Hidden Layer\n",
    "- [Part 7](#part7) -  Fashion MNIST Two Hidden Layers\n",
    "- [Part 8](#part8) -  Results \n",
    "- [Part 9](#part9) -  --\n",
    "- [Part 10](#part10) -  --\n",
    "- [Part 11](#part11) -  --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59615e",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "### Part 1 -   Importing the libraries, packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "037efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721fb2b",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "### Part 2 -   Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae6751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0936fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def softmax(X):\n",
    "    e = np.exp(X - np.max(X))\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(target, output):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "def cross_entropy_matrix(output, target):\n",
    "    target = np.array(target)\n",
    "    output = np.array(output)\n",
    "    product = target*np.log(output)\n",
    "    errors = -np.sum(product, axis=1)\n",
    "    m = len(errors)\n",
    "    errors = np.sum(errors) / m\n",
    "    return errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ds(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "\n",
    "def dr(x):\n",
    "    dr = (np.sign(x) + 1) / 2\n",
    "    return dr\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-x)\n",
    "    return (a-b)/(a+b)\n",
    "\n",
    "def dt(x):\n",
    "    return 1-tanh(x)**2\n",
    "    \n",
    "def leaky(x,a):\n",
    "    leaky = np.maximum(x,0)*x + a*np.minimum(x,0)\n",
    "    return leaky\n",
    "\n",
    "def dl(x,a):\n",
    "    dl = (np.sign(x)+1)/2 - a*(np.sign(x)-1)/2\n",
    "    return dl\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid :\n",
    "        return ds\n",
    "    if f == tanh :\n",
    "        return dt\n",
    "    if f == relu :\n",
    "        return dr\n",
    "    if f == leaky :\n",
    "        return dl\n",
    "    return None\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i][y[i]]=1\n",
    "    return ind\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed963ab4",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "### Part 3 -   One Hidden Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7220f",
   "metadata": {},
   "source": [
    "# One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5c79",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden x N_features\n",
    "- **b1**    : Hidden\n",
    "- **W2**    : Output x Hidden\n",
    "- **b2**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26933734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenOne:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes,\n",
    "                 activation_hidden,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes = hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.hidden_derivative = derivative(self.activation_hidden)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "        # W1 of size hidden x features\n",
    "        n = self.input_nodes * self.hidden_nodes\n",
    "        self.W1 = tn.rvs(n).reshape((self.hidden_nodes, self.input_nodes )) # hidden x features\n",
    "        # W2 of size output x hidden\n",
    "        m = self.hidden_nodes  * self.output_nodes\n",
    "        self.W2 = tn.rvs(m).reshape((self.output_nodes, self.hidden_nodes )) # output x hidden\n",
    "    \n",
    "    def create_biases(self):    \n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.b1 = tn.rvs(self.hidden_nodes).reshape(-1,1) \n",
    "        self.b2 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "          \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1 # Hidden x N_samples\n",
    "        A1 = self.activation_hidden(Z1)      # Hidden x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2  # Output x N_samples\n",
    "        A2 = softmax(Z2)      #Output x N_samples\n",
    "        return A2, Z2, A1, Z1\n",
    "    \n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backpropSGD(self, X, target):\n",
    "        m = X.shape[0]                  #N_samples\n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(m))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        for i in range(m) :\n",
    "            # Forward prop\n",
    "            x = X_SGD[i,:].reshape(1,-1)                   # 1 x N_features\n",
    "            a2, z2, a1, z1 = self.forward(x)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,i].reshape(-1,1), a2)/m\n",
    "            # deltas\n",
    "            dz2 = a2 - target[:,i].reshape(-1,1)                    #Output x 1\n",
    "            dW2 = dz2.dot(a1.T)                                     #Output x hidden\n",
    "            db2 = dz2                                               #Output x 1\n",
    "            dz1 = self.W2.T.dot(dz2)*self.hidden_derivative(z1)     # Hidden x 1\n",
    "            dW1 = dz1.dot(x)                                        # Hidden x N_Features\n",
    "            db1 = dz1                                               # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A2)/pass_length\n",
    "            # deltas\n",
    "            dZ2 = A2 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   #Output x hidden\n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)              # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              #Hidden x1                                            # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2    \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "    def predict(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    print(f'Loss after epoch {i} : {cost}')\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 3epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 4epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 5epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 6epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 7epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after 8epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "       \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f890e7",
   "metadata": {},
   "source": [
    "# Testing with Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8fdcb6",
   "metadata": {},
   "source": [
    "## Loading and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "30c5c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10b23d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "t = to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3037d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5\n",
    "D = data.shape[1]\n",
    "K = len(set(target))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data ,target ,test_size=0.25)\n",
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cde3d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "t = to_categorical(target)\n",
    "\n",
    "M = 5\n",
    "D = data.shape[1]\n",
    "K = len(set(target))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data ,target ,test_size=0.25)\n",
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb78eec",
   "metadata": {},
   "source": [
    "## One Hidden Layer \n",
    "### Activation Function Tests :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3928f8",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a893183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sigmoid = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "700d2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.35676017500207446\n",
      "Loss after epoch 200 : 0.3472682711296739\n",
      "Loss after epoch 300 : 0.33854210728765455\n",
      "Loss after epoch 400 : 0.3294690112848555\n",
      "Loss after epoch 500 : 0.31985351610787616\n",
      "Loss after epoch 600 : 0.30983931166269574\n",
      "Loss after epoch 700 : 0.2996361829855944\n",
      "Loss after epoch 800 : 0.28943582049763994\n",
      "Loss after epoch 900 : 0.2793878076310757\n",
      "Loss after 4epoch 1001 : 0.26969608896776515\n"
     ]
    }
   ],
   "source": [
    "c=nn_sigmoid.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "513912f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91570d5",
   "metadata": {},
   "source": [
    "#### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5c78d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "627ae396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.2744357421754285\n",
      "Loss after epoch 200 : 0.22683344791989082\n",
      "Loss after epoch 300 : 0.1972871159734545\n",
      "Loss after epoch 400 : 0.17864683118690947\n",
      "Loss after epoch 500 : 0.16575669158759046\n",
      "Loss after epoch 600 : 0.1557300948682545\n",
      "Loss after epoch 700 : 0.1470532213499895\n",
      "Loss after epoch 800 : 0.13896702172314118\n",
      "Loss after epoch 900 : 0.1311374054753101\n",
      "Loss after 4epoch 1001 : 0.12354874274310496\n"
     ]
    }
   ],
   "source": [
    "c=nn_tanh.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3ab2befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c0586",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9f4fc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               #optimizer='minibatch',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "754ee696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.3241253156822885\n",
      "Loss after epoch 200 : 0.2894202518695222\n",
      "Loss after epoch 300 : 0.2617697618570672\n",
      "Loss after epoch 400 : 0.24017915755583838\n",
      "Loss after epoch 500 : 0.22313809221428157\n",
      "Loss after epoch 600 : 0.20956649456747894\n",
      "Loss after epoch 700 : 0.1986137938463622\n",
      "Loss after epoch 800 : 0.1896541243394405\n",
      "Loss after epoch 900 : 0.18221320452530698\n",
      "Loss after 4epoch 1001 : 0.1759931220690625\n"
     ]
    }
   ],
   "source": [
    "c=nn_relu.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b4533c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6578947368421053\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928b112",
   "metadata": {},
   "source": [
    "### Conclusion :\n",
    "\n",
    "ReLU works better, need to confirm that later with Fashion-MNIST + multiple tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8278b",
   "metadata": {},
   "source": [
    "### Optimizer  Tests :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25de483",
   "metadata": {},
   "source": [
    "### SGD :\n",
    "\n",
    "Doing 1 sample each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4b24769",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_SGD = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='SGD',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8246f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.03827870972564428\n",
      "Loss after epoch 200 : 0.03618489968071316\n",
      "Loss after epoch 300 : 0.03341739996126434\n",
      "Loss after epoch 400 : 0.030471604346987674\n",
      "Loss after epoch 500 : 0.02878433407857863\n",
      "Loss after epoch 600 : 0.027608701361995336\n",
      "Loss after epoch 700 : 0.026784484092169845\n",
      "Loss after epoch 800 : 0.026105913799405033\n",
      "Loss after epoch 900 : 0.02550563780422803\n",
      "Loss after 2epoch 1001 : 0.02496559944908063\n"
     ]
    }
   ],
   "source": [
    "c=nn_SGD.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e101f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_SGD.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2a35bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_adam = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "               optimizer='adam',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b8c2e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 0.40682387171799034\n",
      "Loss after epoch 1 : 0.3829398441394841\n",
      "Loss after epoch 2 : 0.3609550589014274\n",
      "Loss after epoch 3 : 0.3497270430641767\n",
      "Loss after epoch 4 : 0.34750346341525057\n",
      "Loss after epoch 5 : 0.3403243537931096\n",
      "Loss after epoch 6 : 0.3252660275570091\n",
      "Loss after epoch 7 : 0.30335429251045526\n",
      "Loss after epoch 8 : 0.2785379921944592\n",
      "Loss after epoch 9 : 0.25355752281034544\n",
      "Loss after epoch 10 : 0.23528971354772685\n",
      "Loss after epoch 11 : 0.21860961788822308\n",
      "Loss after epoch 12 : 0.2052060248479696\n",
      "Loss after epoch 13 : 0.19583745591975235\n",
      "Loss after epoch 14 : 0.18472669140056347\n",
      "Loss after epoch 15 : 0.1730269104569562\n",
      "Loss after epoch 16 : 0.16516311340565826\n",
      "Loss after epoch 17 : 0.15597321446753562\n",
      "Loss after epoch 18 : 0.1481242514677245\n",
      "Loss after epoch 19 : 0.14264421381798462\n",
      "Loss after epoch 20 : 0.1358787110883372\n",
      "Loss after epoch 21 : 0.12981567229237945\n",
      "Loss after epoch 22 : 0.12466891321449515\n",
      "Loss after epoch 23 : 0.11758602509877214\n",
      "Loss after epoch 24 : 0.11252711784692397\n",
      "Loss after epoch 25 : 0.10641744494387717\n",
      "Loss after epoch 26 : 0.1014018533454859\n",
      "Loss after epoch 27 : 0.09668463968111052\n",
      "Loss after epoch 28 : 0.09205608774813887\n",
      "Loss after epoch 29 : 0.08835126145390557\n",
      "Loss after epoch 30 : 0.08415471080737376\n",
      "Loss after epoch 31 : 0.08105643990300429\n",
      "Loss after epoch 32 : 0.0772228238712722\n",
      "Loss after epoch 33 : 0.07444784979633348\n",
      "Loss after epoch 34 : 0.07098285179938388\n",
      "Loss after epoch 35 : 0.06850314716288872\n",
      "Loss after epoch 36 : 0.06543434107324295\n",
      "Loss after epoch 37 : 0.06324439132040387\n",
      "Loss after epoch 38 : 0.06059526951263768\n",
      "Loss after epoch 39 : 0.0586919969571846\n",
      "Loss after epoch 40 : 0.05643583325775649\n",
      "Loss after epoch 41 : 0.05478708170704969\n",
      "Loss after epoch 42 : 0.052884638765778924\n",
      "Loss after epoch 43 : 0.051441977957493945\n",
      "Loss after epoch 44 : 0.049834826474435455\n",
      "Loss after epoch 45 : 0.048563982042910576\n",
      "Loss after epoch 46 : 0.047210968318591645\n",
      "Loss after epoch 47 : 0.04607502480881275\n",
      "Loss after epoch 48 : 0.04493188899388537\n",
      "Loss after epoch 49 : 0.04391858076241194\n",
      "Loss after epoch 50 : 0.042952476688221\n",
      "Loss after epoch 51 : 0.042043626902239735\n",
      "Loss after epoch 52 : 0.04122399924933372\n",
      "Loss after epoch 53 : 0.04041496519607065\n",
      "Loss after epoch 54 : 0.03971297143250255\n",
      "Loss after epoch 55 : 0.038993423287281705\n",
      "Loss after epoch 56 : 0.03838749567301232\n",
      "Loss after epoch 57 : 0.037750036256421954\n",
      "Loss after epoch 58 : 0.03721849765238036\n",
      "Loss after epoch 59 : 0.03665457365144355\n",
      "Loss after epoch 60 : 0.03618334024261921\n",
      "Loss after epoch 61 : 0.03568331467703937\n",
      "Loss after epoch 62 : 0.03525943935710875\n",
      "Loss after epoch 63 : 0.03481555382523621\n",
      "Loss after epoch 64 : 0.034430738376574385\n",
      "Loss after epoch 65 : 0.03403426148126564\n",
      "Loss after epoch 66 : 0.033681952643397246\n",
      "Loss after epoch 67 : 0.0333264014125076\n",
      "Loss after epoch 68 : 0.033001962266261585\n",
      "Loss after epoch 69 : 0.03268088528531994\n",
      "Loss after epoch 70 : 0.032381050521362274\n",
      "Loss after epoch 71 : 0.03208947791329212\n",
      "Loss after epoch 72 : 0.031811527322110233\n",
      "Loss after epoch 73 : 0.03154510110267569\n",
      "Loss after epoch 74 : 0.03128704341367356\n",
      "Loss after epoch 75 : 0.031042159341612183\n",
      "Loss after epoch 76 : 0.030802145062610798\n",
      "Loss after epoch 77 : 0.03057591177668962\n",
      "Loss after epoch 78 : 0.030352363383402203\n",
      "Loss after epoch 79 : 0.03014224976001912\n",
      "Loss after epoch 80 : 0.02993371415644102\n",
      "Loss after epoch 81 : 0.02973777524861863\n",
      "Loss after epoch 82 : 0.02954286755968442\n",
      "Loss after epoch 83 : 0.029359355830593698\n",
      "Loss after epoch 84 : 0.02917686228044897\n",
      "Loss after epoch 85 : 0.029004444069692593\n",
      "Loss after epoch 86 : 0.028833172251016285\n",
      "Loss after epoch 87 : 0.028670642568138633\n",
      "Loss after epoch 88 : 0.028509581956016033\n",
      "Loss after epoch 89 : 0.028356007483797487\n",
      "Loss after epoch 90 : 0.028204180312555074\n",
      "Loss after epoch 91 : 0.028058714051459616\n",
      "Loss after epoch 92 : 0.027915299172841863\n",
      "Loss after epoch 93 : 0.027777268613029708\n",
      "Loss after epoch 94 : 0.027641494518628913\n",
      "Loss after epoch 95 : 0.027510280356959866\n",
      "Loss after epoch 96 : 0.027381493256767236\n",
      "Loss after epoch 97 : 0.02725659041077218\n",
      "Loss after epoch 98 : 0.027134192688439536\n",
      "Loss after epoch 99 : 0.027015125470885675\n",
      "Loss after epoch 100 : 0.026898600926710047\n",
      "Loss after epoch 100 : 0.026898600926710047\n",
      "Loss after epoch 101 : 0.02678497206423012\n",
      "Loss after epoch 102 : 0.02667385791642191\n",
      "Loss after epoch 103 : 0.026565285541723476\n",
      "Loss after epoch 104 : 0.026459172958003194\n",
      "Loss after epoch 105 : 0.026355332154122203\n",
      "Loss after epoch 106 : 0.026253858493426714\n",
      "Loss after epoch 107 : 0.026154433408718317\n",
      "Loss after epoch 108 : 0.026057271428235735\n",
      "Loss after epoch 109 : 0.0259619894881868\n",
      "Loss after epoch 110 : 0.025868849995763825\n",
      "Loss after epoch 111 : 0.025777445790828454\n",
      "Loss after epoch 112 : 0.025688063182226686\n",
      "Loss after epoch 113 : 0.025600305038187193\n",
      "Loss after epoch 114 : 0.025514444374900195\n",
      "Loss after epoch 115 : 0.02543010772096252\n",
      "Loss after epoch 116 : 0.025347550285956106\n",
      "Loss after epoch 117 : 0.025266437072996403\n",
      "Loss after epoch 118 : 0.025186988671618837\n",
      "Loss after epoch 119 : 0.025108908244320032\n",
      "Loss after epoch 120 : 0.02503238628201372\n",
      "Loss after epoch 121 : 0.024957168817935152\n",
      "Loss after epoch 122 : 0.02488341029577345\n",
      "Loss after epoch 123 : 0.024810893419012787\n",
      "Loss after epoch 124 : 0.024739743983919586\n",
      "Loss after epoch 125 : 0.024669781632361828\n",
      "Loss after epoch 126 : 0.024601102638057166\n",
      "Loss after epoch 127 : 0.02453355591847283\n",
      "Loss after epoch 128 : 0.024467215434707545\n",
      "Loss after epoch 129 : 0.024401958219919678\n",
      "Loss after epoch 130 : 0.024337836975536504\n",
      "Loss after epoch 131 : 0.024274749795081596\n",
      "Loss after epoch 132 : 0.024212733945938855\n",
      "Loss after epoch 133 : 0.02415170735748552\n",
      "Loss after epoch 134 : 0.024091693335235177\n",
      "Loss after epoch 135 : 0.02403262392815602\n",
      "Loss after epoch 136 : 0.023974512706094065\n",
      "Loss after epoch 137 : 0.023917305014600713\n",
      "Loss after epoch 138 : 0.023861005974168497\n",
      "Loss after epoch 139 : 0.02380556991570615\n",
      "Loss after epoch 140 : 0.02375099634708156\n",
      "Loss after epoch 141 : 0.023697248210300184\n",
      "Loss after epoch 142 : 0.02364432033967067\n",
      "Loss after epoch 143 : 0.02359218117548925\n",
      "Loss after epoch 144 : 0.023540822660877963\n",
      "Loss after epoch 145 : 0.023490218702246497\n",
      "Loss after epoch 146 : 0.023440358967652206\n",
      "Loss after epoch 147 : 0.023391220639817873\n",
      "Loss after epoch 148 : 0.02334279216046476\n",
      "Loss after epoch 149 : 0.02329505416552765\n",
      "Loss after epoch 150 : 0.02324799424424809\n",
      "Loss after epoch 151 : 0.02320159496541156\n",
      "Loss after epoch 152 : 0.023155843636767825\n",
      "Loss after epoch 153 : 0.02311072503665827\n",
      "Loss after epoch 154 : 0.023066226403889455\n",
      "Loss after epoch 155 : 0.023022333673845145\n",
      "Loss after epoch 156 : 0.022979034293930858\n",
      "Loss after epoch 157 : 0.022936315659252714\n",
      "Loss after epoch 158 : 0.022894165539618788\n",
      "Loss after epoch 159 : 0.02285257205729608\n",
      "Loss after epoch 160 : 0.022811523412154453\n",
      "Loss after epoch 161 : 0.022771008739641135\n",
      "Loss after epoch 162 : 0.022731016726983973\n",
      "Loss after epoch 163 : 0.02269153700692018\n",
      "Loss after epoch 164 : 0.02265255877002394\n",
      "Loss after epoch 165 : 0.02261407239546\n",
      "Loss after epoch 166 : 0.0225760676039027\n",
      "Loss after epoch 167 : 0.022538535151772218\n",
      "Loss after epoch 168 : 0.02250146526114353\n",
      "Loss after epoch 169 : 0.022464849271785964\n",
      "Loss after epoch 170 : 0.022428677918165756\n",
      "Loss after epoch 171 : 0.022392942854364894\n",
      "Loss after epoch 172 : 0.022357635280595724\n",
      "Loss after epoch 173 : 0.02232274732988713\n",
      "Loss after epoch 174 : 0.022288270671263082\n",
      "Loss after epoch 175 : 0.022254197716966746\n",
      "Loss after epoch 176 : 0.02222052055451422\n",
      "Loss after epoch 177 : 0.02218723200369788\n",
      "Loss after epoch 178 : 0.0221543245695486\n",
      "Loss after epoch 179 : 0.022121791328508108\n",
      "Loss after epoch 180 : 0.022089625156070887\n",
      "Loss after epoch 181 : 0.022057819483628848\n",
      "Loss after epoch 182 : 0.022026367553924817\n",
      "Loss after epoch 183 : 0.02199526303749676\n",
      "Loss after epoch 184 : 0.021964499503396306\n",
      "Loss after epoch 185 : 0.021934070935641762\n",
      "Loss after epoch 186 : 0.021903971224949872\n",
      "Loss after epoch 187 : 0.02187419457874241\n",
      "Loss after epoch 188 : 0.021844735175255887\n",
      "Loss after epoch 189 : 0.021815587500975536\n",
      "Loss after epoch 190 : 0.021786746015969437\n",
      "Loss after epoch 191 : 0.021758205414922297\n",
      "Loss after epoch 192 : 0.021729960412019158\n",
      "Loss after epoch 193 : 0.021702005951477567\n",
      "Loss after epoch 194 : 0.02167433699509776\n",
      "Loss after epoch 195 : 0.021646948680354886\n",
      "Loss after epoch 196 : 0.021619836194582943\n",
      "Loss after epoch 197 : 0.021592994898962263\n",
      "Loss after epoch 198 : 0.021566420199235603\n",
      "Loss after epoch 199 : 0.021540107635252895\n",
      "Loss after epoch 200 : 0.021514052813804394\n",
      "Loss after epoch 200 : 0.021514052813804394\n",
      "Loss after epoch 201 : 0.02148825147563\n",
      "Loss after epoch 202 : 0.0214626994210037\n",
      "Loss after epoch 203 : 0.021437392555288984\n",
      "Loss after epoch 204 : 0.02141232685872673\n",
      "Loss after epoch 205 : 0.021387498417319933\n",
      "Loss after epoch 206 : 0.021362903383456165\n",
      "Loss after epoch 207 : 0.021338537994427923\n",
      "Loss after epoch 208 : 0.021314398564322046\n",
      "Loss after epoch 209 : 0.021290481493057907\n",
      "Loss after epoch 210 : 0.02126678324851721\n",
      "Loss after epoch 211 : 0.0212433003694029\n",
      "Loss after epoch 212 : 0.021220029469357353\n",
      "Loss after epoch 213 : 0.02119696723367243\n",
      "Loss after epoch 214 : 0.021174110413917098\n",
      "Loss after epoch 215 : 0.021151455822548893\n",
      "Loss after epoch 216 : 0.021129000342901223\n",
      "Loss after epoch 217 : 0.021106740919732053\n",
      "Loss after epoch 218 : 0.021084674560507426\n",
      "Loss after epoch 219 : 0.021062798326434963\n",
      "Loss after epoch 220 : 0.021041109344383916\n",
      "Loss after epoch 221 : 0.021019604795125765\n",
      "Loss after epoch 222 : 0.02099828191761008\n",
      "Loss after epoch 223 : 0.020977137999225053\n",
      "Loss after epoch 224 : 0.020956170387352487\n",
      "Loss after epoch 225 : 0.020935376477583385\n",
      "Loss after epoch 226 : 0.020914753718799937\n",
      "Loss after epoch 227 : 0.020894299604219742\n",
      "Loss after epoch 228 : 0.020874011681378573\n",
      "Loss after epoch 229 : 0.020853887541555878\n",
      "Loss after epoch 230 : 0.020833924824459466\n",
      "Loss after epoch 231 : 0.020814121210795605\n",
      "Loss after epoch 232 : 0.020794474430178456\n",
      "Loss after epoch 233 : 0.02077498225231711\n",
      "Loss after epoch 234 : 0.02075564249073943\n",
      "Loss after epoch 235 : 0.02073645299711391\n",
      "Loss after epoch 236 : 0.020717411667025728\n",
      "Loss after epoch 237 : 0.02069851643304717\n",
      "Loss after epoch 238 : 0.020679765267338206\n",
      "Loss after epoch 239 : 0.020661156177635684\n",
      "Loss after epoch 240 : 0.020642687211095006\n",
      "Loss after epoch 241 : 0.020624356449110353\n",
      "Loss after epoch 242 : 0.02060616200886613\n",
      "Loss after epoch 243 : 0.02058810204074076\n",
      "Loss after epoch 244 : 0.02057017473054521\n",
      "Loss after epoch 245 : 0.02055237829582678\n",
      "Loss after epoch 246 : 0.02053471098656649\n",
      "Loss after epoch 247 : 0.020517171083674686\n",
      "Loss after epoch 248 : 0.020499756900003455\n",
      "Loss after epoch 249 : 0.020482466777817534\n",
      "Loss after epoch 250 : 0.020465299088877186\n",
      "Loss after epoch 251 : 0.020448252233696333\n",
      "Loss after epoch 252 : 0.020431324641695826\n",
      "Loss after epoch 253 : 0.02041451476952955\n",
      "Loss after epoch 254 : 0.020397821100785085\n",
      "Loss after epoch 255 : 0.02038124214571122\n",
      "Loss after epoch 256 : 0.020364776440832247\n",
      "Loss after epoch 257 : 0.020348422547856085\n",
      "Loss after epoch 258 : 0.02033217905318808\n",
      "Loss after epoch 259 : 0.02031604456789245\n",
      "Loss after epoch 260 : 0.02030001772702345\n",
      "Loss after epoch 261 : 0.020284097188898082\n",
      "Loss after epoch 262 : 0.02026828163456637\n",
      "Loss after epoch 263 : 0.020252569767838804\n",
      "Loss after epoch 264 : 0.020236960314519034\n",
      "Loss after epoch 265 : 0.020221452021885005\n",
      "Loss after epoch 266 : 0.020206043658204546\n",
      "Loss after epoch 267 : 0.02019073401272557\n",
      "Loss after epoch 268 : 0.020175521894928973\n",
      "Loss after epoch 269 : 0.020160406134117775\n",
      "Loss after epoch 270 : 0.020145385579019133\n",
      "Loss after epoch 271 : 0.020130459097689086\n",
      "Loss after epoch 272 : 0.020115625576848616\n",
      "Loss after epoch 273 : 0.02010088392152651\n",
      "Loss after epoch 274 : 0.020086233054751587\n",
      "Loss after epoch 275 : 0.020071671917365745\n",
      "Loss after epoch 276 : 0.020057199467464367\n",
      "Loss after epoch 277 : 0.020042814680070742\n",
      "Loss after epoch 278 : 0.020028516546900212\n",
      "Loss after epoch 279 : 0.02001430407610002\n",
      "Loss after epoch 280 : 0.020000176291789823\n",
      "Loss after epoch 281 : 0.01998613223376368\n",
      "Loss after epoch 282 : 0.019972170957298654\n",
      "Loss after epoch 283 : 0.019958291532851197\n",
      "Loss after epoch 284 : 0.019944493045680706\n",
      "Loss after epoch 285 : 0.019930774595582602\n",
      "Loss after epoch 286 : 0.019917135296714492\n",
      "Loss after epoch 287 : 0.019903574277279523\n",
      "Loss after epoch 288 : 0.01989009067921326\n",
      "Loss after epoch 289 : 0.019876683657950336\n",
      "Loss after epoch 290 : 0.01986335238224934\n",
      "Loss after epoch 291 : 0.019850096033886905\n",
      "Loss after epoch 292 : 0.019836913807391363\n",
      "Loss after epoch 293 : 0.019823804909840786\n",
      "Loss after epoch 294 : 0.019810768560678666\n",
      "Loss after epoch 295 : 0.019797803991432272\n",
      "Loss after epoch 296 : 0.019784910445482126\n",
      "Loss after epoch 297 : 0.019772087177885506\n",
      "Loss after epoch 298 : 0.01975933345518242\n",
      "Loss after epoch 299 : 0.019746648555144533\n",
      "Loss after epoch 300 : 0.019734031766573512\n",
      "Loss after epoch 300 : 0.019734031766573512\n",
      "Loss after epoch 301 : 0.019721482389141367\n",
      "Loss after epoch 302 : 0.01970899973319255\n",
      "Loss after epoch 303 : 0.019696583119523266\n",
      "Loss after epoch 304 : 0.019684231879204376\n",
      "Loss after epoch 305 : 0.019671945353430483\n",
      "Loss after epoch 306 : 0.019659722893325737\n",
      "Loss after epoch 307 : 0.01964756385975059\n",
      "Loss after epoch 308 : 0.01963546762314525\n",
      "Loss after epoch 309 : 0.019623433563382135\n",
      "Loss after epoch 310 : 0.01961146106958208\n",
      "Loss after epoch 311 : 0.019599549539944645\n",
      "Loss after epoch 312 : 0.01958769838160755\n",
      "Loss after epoch 313 : 0.019575907010501128\n",
      "Loss after epoch 314 : 0.019564174851178603\n",
      "Loss after epoch 315 : 0.01955250133666637\n",
      "Loss after epoch 316 : 0.01954088590833525\n",
      "Loss after epoch 317 : 0.01952932801575717\n",
      "Loss after epoch 318 : 0.019517827116551693\n",
      "Loss after epoch 319 : 0.019506382676252983\n",
      "Loss after epoch 320 : 0.019494994168188368\n",
      "Loss after epoch 321 : 0.0194836610733403\n",
      "Loss after epoch 322 : 0.019472382880208444\n",
      "Loss after epoch 323 : 0.019461159084690354\n",
      "Loss after epoch 324 : 0.019449989189964646\n",
      "Loss after epoch 325 : 0.01943887270636073\n",
      "Loss after epoch 326 : 0.019427809151235348\n",
      "Loss after epoch 327 : 0.019416798048863565\n",
      "Loss after epoch 328 : 0.019405838930325704\n",
      "Loss after epoch 329 : 0.019394931333386697\n",
      "Loss after epoch 330 : 0.019384074802385114\n",
      "Loss after epoch 331 : 0.01937326888813152\n",
      "Loss after epoch 332 : 0.01936251314779986\n",
      "Loss after epoch 333 : 0.019351807144816827\n",
      "Loss after epoch 334 : 0.01934115044876138\n",
      "Loss after epoch 335 : 0.019330542635268348\n",
      "Loss after epoch 336 : 0.01931998328592533\n",
      "Loss after epoch 337 : 0.01930947198817153\n",
      "Loss after epoch 338 : 0.019299008335206085\n",
      "Loss after epoch 339 : 0.019288591925895417\n",
      "Loss after epoch 340 : 0.019278222364676346\n",
      "Loss after epoch 341 : 0.019267899261464314\n",
      "Loss after epoch 342 : 0.01925762223156748\n",
      "Loss after epoch 343 : 0.01924739089559821\n",
      "Loss after epoch 344 : 0.01923720487938282\n",
      "Loss after epoch 345 : 0.019227063813877527\n",
      "Loss after epoch 346 : 0.01921696733508723\n",
      "Loss after epoch 347 : 0.019206915083980678\n",
      "Loss after epoch 348 : 0.019196906706407474\n",
      "Loss after epoch 349 : 0.019186941853019866\n",
      "Loss after epoch 350 : 0.019177020179195012\n",
      "Loss after epoch 351 : 0.019167141344954817\n",
      "Loss after epoch 352 : 0.019157305014888833\n",
      "Loss after epoch 353 : 0.019147510858080953\n",
      "Loss after epoch 354 : 0.01913775854803444\n",
      "Loss after epoch 355 : 0.01912804776259646\n",
      "Loss after epoch 356 : 0.019118378183886446\n",
      "Loss after epoch 357 : 0.019108749498225653\n",
      "Loss after epoch 358 : 0.01909916139606536\n",
      "Loss after epoch 359 : 0.019089613571915937\n",
      "Loss after epoch 360 : 0.019080105724278835\n",
      "Loss after epoch 361 : 0.01907063755557879\n",
      "Loss after epoch 362 : 0.01906120877209475\n",
      "Loss after epoch 363 : 0.0190518190838928\n",
      "Loss after epoch 364 : 0.01904246820476079\n",
      "Loss after epoch 365 : 0.019033155852142276\n",
      "Loss after epoch 366 : 0.019023881747070065\n",
      "Loss after epoch 367 : 0.019014645614102002\n",
      "Loss after epoch 368 : 0.019005447181256952\n",
      "Loss after epoch 369 : 0.01899628617995004\n",
      "Loss after epoch 370 : 0.01898716234492828\n",
      "Loss after epoch 371 : 0.018978075414207755\n",
      "Loss after epoch 372 : 0.018969025129010095\n",
      "Loss after epoch 373 : 0.018960011233698254\n",
      "Loss after epoch 374 : 0.018951033475713284\n",
      "Loss after epoch 375 : 0.018942091605511316\n",
      "Loss after epoch 376 : 0.018933185376499545\n",
      "Loss after epoch 377 : 0.018924314544971635\n",
      "Loss after epoch 378 : 0.018915478870044085\n",
      "Loss after epoch 379 : 0.01890667811359134\n",
      "Loss after epoch 380 : 0.01889791204018004\n",
      "Loss after epoch 381 : 0.01888918041700269\n",
      "Loss after epoch 382 : 0.01888048301381113\n",
      "Loss after epoch 383 : 0.01887181960284843\n",
      "Loss after epoch 384 : 0.018863189958779274\n",
      "Loss after epoch 385 : 0.018854593858619664\n",
      "Loss after epoch 386 : 0.018846031081664976\n",
      "Loss after epoch 387 : 0.018837501409416018\n",
      "Loss after epoch 388 : 0.018829004625502938\n",
      "Loss after epoch 389 : 0.018820540515607546\n",
      "Loss after epoch 390 : 0.018812108867382632\n",
      "Loss after epoch 391 : 0.018803709470368548\n",
      "Loss after epoch 392 : 0.018795342115906517\n",
      "Loss after epoch 393 : 0.018787006597048867\n",
      "Loss after epoch 394 : 0.018778702708464927\n",
      "Loss after epoch 395 : 0.018770430246342257\n",
      "Loss after epoch 396 : 0.018762189008283754\n",
      "Loss after epoch 397 : 0.018753978793198706\n",
      "Loss after epoch 398 : 0.018745799401187795\n",
      "Loss after epoch 399 : 0.018737650633421254\n",
      "Loss after epoch 400 : 0.01872953229200976\n",
      "Loss after epoch 400 : 0.01872953229200976\n",
      "Loss after epoch 401 : 0.01872144417986658\n",
      "Loss after epoch 402 : 0.01871338610056006\n",
      "Loss after epoch 403 : 0.01870535785815596\n",
      "Loss after epoch 404 : 0.018697359257047914\n",
      "Loss after epoch 405 : 0.018689390101774256\n",
      "Loss after epoch 406 : 0.018681450196820385\n",
      "Loss after epoch 407 : 0.01867353934640492\n",
      "Loss after epoch 408 : 0.018665657354246423\n",
      "Loss after epoch 409 : 0.01865780402330994\n",
      "Loss after epoch 410 : 0.018649979155529704\n",
      "Loss after epoch 411 : 0.018642182551505744\n",
      "Loss after epoch 412 : 0.018634414010170373\n",
      "Loss after epoch 413 : 0.01862667332842128\n",
      "Loss after epoch 414 : 0.018618960300716702\n",
      "Loss after epoch 415 : 0.018611274718627457\n",
      "Loss after epoch 416 : 0.01860361637034014\n",
      "Loss after epoch 417 : 0.018595985040105376\n",
      "Loss after epoch 418 : 0.0185883805076227\n",
      "Loss after epoch 419 : 0.01858080254735387\n",
      "Loss after epoch 420 : 0.018573250927754515\n",
      "Loss after epoch 421 : 0.018565725410412015\n",
      "Loss after epoch 422 : 0.018558225749076146\n",
      "Loss after epoch 423 : 0.01855075168856723\n",
      "Loss after epoch 424 : 0.01854330296354255\n",
      "Loss after epoch 425 : 0.018535879297100384\n",
      "Loss after epoch 426 : 0.01852848039919647\n",
      "Loss after epoch 427 : 0.018521105964843295\n",
      "Loss after epoch 428 : 0.01851375567205908\n",
      "Loss after epoch 429 : 0.018506429179524983\n",
      "Loss after epoch 430 : 0.01849912612390528\n",
      "Loss after epoch 431 : 0.018491846116773507\n",
      "Loss after epoch 432 : 0.018484588741081703\n",
      "Loss after epoch 433 : 0.01847735354709492\n",
      "Loss after epoch 434 : 0.018470140047702997\n",
      "Loss after epoch 435 : 0.018462947713004337\n",
      "Loss after epoch 436 : 0.01845577596403908\n",
      "Loss after epoch 437 : 0.018448624165529694\n",
      "Loss after epoch 438 : 0.018441491617462946\n",
      "Loss after epoch 439 : 0.018434377545323732\n",
      "Loss after epoch 440 : 0.01842728108876542\n",
      "Loss after epoch 441 : 0.018420201288475848\n",
      "Loss after epoch 442 : 0.018413137070979007\n",
      "Loss after epoch 443 : 0.018406087231101615\n",
      "Loss after epoch 444 : 0.018399050411846557\n",
      "Loss after epoch 445 : 0.01839202508146434\n",
      "Loss after epoch 446 : 0.018385009507630126\n",
      "Loss after epoch 447 : 0.018378001728861306\n",
      "Loss after epoch 448 : 0.018370999523722977\n",
      "Loss after epoch 449 : 0.018364000379072883\n",
      "Loss after epoch 450 : 0.018357001459778435\n",
      "Loss after epoch 451 : 0.018349999584257343\n",
      "Loss after epoch 452 : 0.018342991213260933\n",
      "Loss after epoch 453 : 0.018335972464125397\n",
      "Loss after epoch 454 : 0.01832893917007887\n",
      "Loss after epoch 455 : 0.018321887015151618\n",
      "Loss after epoch 456 : 0.018314811790867285\n",
      "Loss after epoch 457 : 0.018307709841722714\n",
      "Loss after epoch 458 : 0.01830057879092395\n",
      "Loss after epoch 459 : 0.018293418658897567\n",
      "Loss after epoch 460 : 0.01828623348572867\n",
      "Loss after epoch 461 : 0.018279033503468715\n",
      "Loss after epoch 462 : 0.01827183769991233\n",
      "Loss after epoch 463 : 0.018264676165229754\n",
      "Loss after epoch 464 : 0.018257590836211066\n",
      "Loss after epoch 465 : 0.01825063228194941\n",
      "Loss after epoch 466 : 0.018243849712003886\n",
      "Loss after epoch 467 : 0.018237272988058256\n",
      "Loss after epoch 468 : 0.01823089075517969\n",
      "Loss after epoch 469 : 0.018224636764072658\n",
      "Loss after epoch 470 : 0.01821839988074716\n",
      "Loss after epoch 471 : 0.018212062800346825\n",
      "Loss after epoch 472 : 0.01820555228416678\n",
      "Loss after epoch 473 : 0.018198869751328877\n",
      "Loss after epoch 474 : 0.018192082775836876\n",
      "Loss after epoch 475 : 0.018185286226345715\n",
      "Loss after epoch 476 : 0.018178559963757102\n",
      "Loss after epoch 477 : 0.018171945068143944\n",
      "Loss after epoch 478 : 0.018165442873450177\n",
      "Loss after epoch 479 : 0.018159027854829004\n",
      "Loss after epoch 480 : 0.018152663298374803\n",
      "Loss after epoch 481 : 0.018146313313458617\n",
      "Loss after epoch 482 : 0.018139949777513837\n",
      "Loss after epoch 483 : 0.018133555397248403\n",
      "Loss after epoch 484 : 0.018127124495531326\n",
      "Loss after epoch 485 : 0.01812066252681967\n",
      "Loss after epoch 486 : 0.018114184514436037\n",
      "Loss after epoch 487 : 0.01810771202331643\n",
      "Loss after epoch 488 : 0.018101268211937086\n",
      "Loss after epoch 489 : 0.018094871198656166\n",
      "Loss after epoch 490 : 0.01808852740356581\n",
      "Loss after epoch 491 : 0.018082227943147174\n",
      "Loss after epoch 492 : 0.018075951088025906\n",
      "Loss after epoch 493 : 0.018069671141738006\n",
      "Loss after epoch 494 : 0.018063369915038912\n",
      "Loss after epoch 495 : 0.0180570445217295\n",
      "Loss after epoch 496 : 0.018050707088739883\n",
      "Loss after epoch 497 : 0.01804437706859712\n",
      "Loss after epoch 498 : 0.01803807115939907\n",
      "Loss after epoch 499 : 0.018031796341614156\n",
      "Loss after epoch 500 : 0.018025548682208827\n",
      "Loss after epoch 500 : 0.018025548682208827\n",
      "Loss after epoch 501 : 0.018019317067054823\n",
      "Loss after epoch 502 : 0.018013089106410314\n",
      "Loss after epoch 503 : 0.018006856431924116\n",
      "Loss after epoch 504 : 0.01800061757763757\n",
      "Loss after epoch 505 : 0.01799437775900429\n",
      "Loss after epoch 506 : 0.017988145862202626\n",
      "Loss after epoch 507 : 0.01798192989592089\n",
      "Loss after epoch 508 : 0.017975732926060826\n",
      "Loss after epoch 509 : 0.01796955159090202\n",
      "Loss after epoch 510 : 0.017963378163416657\n",
      "Loss after epoch 511 : 0.0179572050632745\n",
      "Loss after epoch 512 : 0.017951029055817476\n",
      "Loss after epoch 513 : 0.0179448525273031\n",
      "Loss after epoch 514 : 0.01793868125002556\n",
      "Loss after epoch 515 : 0.017932520404171343\n",
      "Loss after epoch 516 : 0.017926371540729496\n",
      "Loss after epoch 517 : 0.017920232189493217\n",
      "Loss after epoch 518 : 0.01791409792971867\n",
      "Loss after epoch 519 : 0.01790796531650974\n",
      "Loss after epoch 520 : 0.01790183381236048\n",
      "Loss after epoch 521 : 0.017895705690201817\n",
      "Loss after epoch 522 : 0.01788958416492458\n",
      "Loss after epoch 523 : 0.01788347107507284\n",
      "Loss after epoch 524 : 0.0178773656778864\n",
      "Loss after epoch 525 : 0.01787126532808126\n",
      "Loss after epoch 526 : 0.017865167450663785\n",
      "Loss after epoch 527 : 0.017859071287217657\n",
      "Loss after epoch 528 : 0.017852978150962345\n",
      "Loss after epoch 529 : 0.017846890156290957\n",
      "Loss after epoch 530 : 0.017840808551406136\n",
      "Loss after epoch 531 : 0.01783473295100009\n",
      "Loss after epoch 532 : 0.01782866191782518\n",
      "Loss after epoch 533 : 0.017822594263256566\n",
      "Loss after epoch 534 : 0.017816529974142563\n",
      "Loss after epoch 535 : 0.01781047009147532\n",
      "Loss after epoch 536 : 0.01780441575531673\n",
      "Loss after epoch 537 : 0.017798367274387293\n",
      "Loss after epoch 538 : 0.017792323986817342\n",
      "Loss after epoch 539 : 0.017786284956636606\n",
      "Loss after epoch 540 : 0.017780249835434664\n",
      "Loss after epoch 541 : 0.017774219135803435\n",
      "Loss after epoch 542 : 0.017768193750318198\n",
      "Loss after epoch 543 : 0.01776217422892046\n",
      "Loss after epoch 544 : 0.017756160477140442\n",
      "Loss after epoch 545 : 0.017750152078296798\n",
      "Loss after epoch 546 : 0.017744148857492446\n",
      "Loss after epoch 547 : 0.017738151136230722\n",
      "Loss after epoch 548 : 0.017732159480239335\n",
      "Loss after epoch 549 : 0.01772617424057798\n",
      "Loss after epoch 550 : 0.017720195352469305\n",
      "Loss after epoch 551 : 0.017714222560424965\n",
      "Loss after epoch 552 : 0.017708255806210434\n",
      "Loss after epoch 553 : 0.017702295384323757\n",
      "Loss after epoch 554 : 0.017696341742728557\n",
      "Loss after epoch 555 : 0.017690395171829135\n",
      "Loss after epoch 556 : 0.017684455703393536\n",
      "Loss after epoch 557 : 0.01767852328523617\n",
      "Loss after epoch 558 : 0.01767259800724022\n",
      "Loss after epoch 559 : 0.017666680132733262\n",
      "Loss after epoch 560 : 0.017660769927802706\n",
      "Loss after epoch 561 : 0.01765486749923502\n",
      "Loss after epoch 562 : 0.017648972818708215\n",
      "Loss after epoch 563 : 0.017643085888077583\n",
      "Loss after epoch 564 : 0.01763720685101235\n",
      "Loss after epoch 565 : 0.0176313359372294\n",
      "Loss after epoch 566 : 0.017625473322800542\n",
      "Loss after epoch 567 : 0.01761961907021848\n",
      "Loss after epoch 568 : 0.01761377319885061\n",
      "Loss after epoch 569 : 0.017607935781672773\n",
      "Loss after epoch 570 : 0.017602106950038013\n",
      "Loss after epoch 571 : 0.017596286813670325\n",
      "Loss after epoch 572 : 0.017590475404213678\n",
      "Loss after epoch 573 : 0.017584672710042614\n",
      "Loss after epoch 574 : 0.017578878752191093\n",
      "Loss after epoch 575 : 0.01757309360689262\n",
      "Loss after epoch 576 : 0.01756731735532318\n",
      "Loss after epoch 577 : 0.01756155003030784\n",
      "Loss after epoch 578 : 0.017555791622527447\n",
      "Loss after epoch 579 : 0.017550042125806575\n",
      "Loss after epoch 580 : 0.01754430155626525\n",
      "Loss after epoch 581 : 0.01753856992380253\n",
      "Loss after epoch 582 : 0.017532847201025416\n",
      "Loss after epoch 583 : 0.017527133332217268\n",
      "Loss after epoch 584 : 0.017521428269059235\n",
      "Loss after epoch 585 : 0.017515731982788992\n",
      "Loss after epoch 586 : 0.017510044443420063\n",
      "Loss after epoch 587 : 0.017504365592109692\n",
      "Loss after epoch 588 : 0.017498695350722588\n",
      "Loss after epoch 589 : 0.017493033632719913\n",
      "Loss after epoch 590 : 0.01748738036659821\n",
      "Loss after epoch 591 : 0.017481735452973674\n",
      "Loss after epoch 592 : 0.017476098830526154\n",
      "Loss after epoch 593 : 0.017470470442343518\n",
      "Loss after epoch 594 : 0.017464850699383423\n",
      "Loss after epoch 595 : 0.01745924107418165\n",
      "Loss after epoch 596 : 0.017453648678354344\n",
      "Loss after epoch 597 : 0.01744810082295371\n",
      "Loss after epoch 598 : 0.017442712109412385\n",
      "Loss after epoch 599 : 0.017437959310208202\n",
      "Loss after epoch 600 : 0.017435707673285863\n",
      "Loss after epoch 600 : 0.017435707673285863\n",
      "Loss after epoch 601 : 0.01744326391888646\n",
      "Loss after epoch 602 : 0.017474416901647406\n",
      "Loss after epoch 603 : 0.017540415472965858\n",
      "Loss after epoch 604 : 0.01755462956997347\n",
      "Loss after epoch 605 : 0.01747994340082066\n",
      "Loss after epoch 606 : 0.017402515666490665\n",
      "Loss after epoch 607 : 0.017435080971278036\n",
      "Loss after epoch 608 : 0.017483482533611865\n",
      "Loss after epoch 609 : 0.017433384496385547\n",
      "Loss after epoch 610 : 0.017381074114728553\n",
      "Loss after epoch 611 : 0.017407625460859884\n",
      "Loss after epoch 612 : 0.01743015429683528\n",
      "Loss after epoch 613 : 0.01739162106272751\n",
      "Loss after epoch 614 : 0.01736269787024879\n",
      "Loss after epoch 615 : 0.017385405950128538\n",
      "Loss after epoch 616 : 0.017390870588591175\n",
      "Loss after epoch 617 : 0.01735655040638801\n",
      "Loss after epoch 618 : 0.017347863096603362\n",
      "Loss after epoch 619 : 0.017364741998296573\n",
      "Loss after epoch 620 : 0.017353571913151726\n",
      "Loss after epoch 621 : 0.017330985120835356\n",
      "Loss after epoch 622 : 0.01733440557819402\n",
      "Loss after epoch 623 : 0.017339620612523657\n",
      "Loss after epoch 624 : 0.017323804769852506\n",
      "Loss after epoch 625 : 0.017312578982110843\n",
      "Loss after epoch 626 : 0.017317690578151784\n",
      "Loss after epoch 627 : 0.01731486678086904\n",
      "Loss after epoch 628 : 0.01730073302196019\n",
      "Loss after epoch 629 : 0.017296252267435305\n",
      "Loss after epoch 630 : 0.017298511990775183\n",
      "Loss after epoch 631 : 0.017291322811986223\n",
      "Loss after epoch 632 : 0.017281235292323793\n",
      "Loss after epoch 633 : 0.017279531025211868\n",
      "Loss after epoch 634 : 0.017278328770999933\n",
      "Loss after epoch 635 : 0.017270369586593486\n",
      "Loss after epoch 636 : 0.017263503794670027\n",
      "Loss after epoch 637 : 0.017261994033344826\n",
      "Loss after epoch 638 : 0.017258593494276074\n",
      "Loss after epoch 639 : 0.017251237432312612\n",
      "Loss after epoch 640 : 0.01724618686207975\n",
      "Loss after epoch 641 : 0.017243995076080876\n",
      "Loss after epoch 642 : 0.01723955878528133\n",
      "Loss after epoch 643 : 0.01723315740950047\n",
      "Loss after epoch 644 : 0.017228810077116692\n",
      "Loss after epoch 645 : 0.017225880520752294\n",
      "Loss after epoch 646 : 0.017221199342273414\n",
      "Loss after epoch 647 : 0.017215540575910657\n",
      "Loss after epoch 648 : 0.01721141017371773\n",
      "Loss after epoch 649 : 0.01720798391722054\n",
      "Loss after epoch 650 : 0.017203301743010557\n",
      "Loss after epoch 651 : 0.017198123377271696\n",
      "Loss after epoch 652 : 0.017193981614168758\n",
      "Loss after epoch 653 : 0.01719025643669751\n",
      "Loss after epoch 654 : 0.017185719146564277\n",
      "Loss after epoch 655 : 0.01718083058835172\n",
      "Loss after epoch 656 : 0.01717659530255336\n",
      "Loss after epoch 657 : 0.017172688510815052\n",
      "Loss after epoch 658 : 0.017168286844983904\n",
      "Loss after epoch 659 : 0.017163592272797932\n",
      "Loss after epoch 660 : 0.01715927474530226\n",
      "Loss after epoch 661 : 0.017155242757429343\n",
      "Loss after epoch 662 : 0.01715094861113782\n",
      "Loss after epoch 663 : 0.017146392171083602\n",
      "Loss after epoch 664 : 0.01714201172390908\n",
      "Loss after epoch 665 : 0.017137876673709966\n",
      "Loss after epoch 666 : 0.017133650192406716\n",
      "Loss after epoch 667 : 0.017129210879295986\n",
      "Loss after epoch 668 : 0.01712479930060536\n",
      "Loss after epoch 669 : 0.0171205671385454\n",
      "Loss after epoch 670 : 0.017116359144566422\n",
      "Loss after epoch 671 : 0.017112014147017504\n",
      "Loss after epoch 672 : 0.017107611824331365\n",
      "Loss after epoch 673 : 0.017103301982231478\n",
      "Loss after epoch 674 : 0.01709906629139651\n",
      "Loss after epoch 675 : 0.017094779400286586\n",
      "Loss after epoch 676 : 0.017090413905932602\n",
      "Loss after epoch 677 : 0.017086059029859747\n",
      "Loss after epoch 678 : 0.017081768873396214\n",
      "Loss after epoch 679 : 0.0170774929065572\n",
      "Loss after epoch 680 : 0.01707316761768954\n",
      "Loss after epoch 681 : 0.017068806204144648\n",
      "Loss after epoch 682 : 0.017064464292454885\n",
      "Loss after epoch 683 : 0.017060156062758024\n",
      "Loss after epoch 684 : 0.017055843688956476\n",
      "Loss after epoch 685 : 0.017051497239116267\n",
      "Loss after epoch 686 : 0.01704713151150338\n",
      "Loss after epoch 687 : 0.01704277713538166\n",
      "Loss after epoch 688 : 0.017038438459106596\n",
      "Loss after epoch 689 : 0.017034093305219853\n",
      "Loss after epoch 690 : 0.01702972524495771\n",
      "Loss after epoch 691 : 0.017025341969823016\n",
      "Loss after epoch 692 : 0.017020960450273986\n",
      "Loss after epoch 693 : 0.0170165848286599\n",
      "Loss after epoch 694 : 0.017012203969684354\n",
      "Loss after epoch 695 : 0.017007806964269498\n",
      "Loss after epoch 696 : 0.017003395067734618\n",
      "Loss after epoch 697 : 0.01699897715163861\n",
      "Loss after epoch 698 : 0.016994558194799853\n",
      "Loss after epoch 699 : 0.016990134371799054\n",
      "Loss after epoch 700 : 0.016985698594011955\n",
      "Loss after epoch 700 : 0.016985698594011955\n",
      "Loss after epoch 701 : 0.016981248276331025\n",
      "Loss after epoch 702 : 0.016976786470397232\n",
      "Loss after epoch 703 : 0.01697231735266454\n",
      "Loss after epoch 704 : 0.016967841438622892\n",
      "Loss after epoch 705 : 0.016963355560337894\n",
      "Loss after epoch 706 : 0.01695885635777847\n",
      "Loss after epoch 707 : 0.016954342991656407\n",
      "Loss after epoch 708 : 0.016949817101855075\n",
      "Loss after epoch 709 : 0.016945280328786876\n",
      "Loss after epoch 710 : 0.016940732606157884\n",
      "Loss after epoch 711 : 0.016936172238342814\n",
      "Loss after epoch 712 : 0.01693159732365357\n",
      "Loss after epoch 713 : 0.0169270070755052\n",
      "Loss after epoch 714 : 0.016922401777264652\n",
      "Loss after epoch 715 : 0.01691778205140171\n",
      "Loss after epoch 716 : 0.016913147923857764\n",
      "Loss after epoch 717 : 0.01690849864848922\n",
      "Loss after epoch 718 : 0.01690383311290832\n",
      "Loss after epoch 719 : 0.01689915035324129\n",
      "Loss after epoch 720 : 0.016894449920002663\n",
      "Loss after epoch 721 : 0.01688973168670324\n",
      "Loss after epoch 722 : 0.016884995606624466\n",
      "Loss after epoch 723 : 0.016880241381464062\n",
      "Loss after epoch 724 : 0.01687546842676772\n",
      "Loss after epoch 725 : 0.01687067599790676\n",
      "Loss after epoch 726 : 0.01686586335811092\n",
      "Loss after epoch 727 : 0.01686102994076056\n",
      "Loss after epoch 728 : 0.016856175298283343\n",
      "Loss after epoch 729 : 0.01685129907223319\n",
      "Loss after epoch 730 : 0.016846400854029338\n",
      "Loss after epoch 731 : 0.01684148016506999\n",
      "Loss after epoch 732 : 0.016836536436283012\n",
      "Loss after epoch 733 : 0.016831569043727443\n",
      "Loss after epoch 734 : 0.016826577361008108\n",
      "Loss after epoch 735 : 0.01682156077400817\n",
      "Loss after epoch 736 : 0.01681651871735288\n",
      "Loss after epoch 737 : 0.016811450636262406\n",
      "Loss after epoch 738 : 0.016806355993387704\n",
      "Loss after epoch 739 : 0.016801234227859577\n",
      "Loss after epoch 740 : 0.01679608476581\n",
      "Loss after epoch 741 : 0.016790907006367536\n",
      "Loss after epoch 742 : 0.016785700330911386\n",
      "Loss after epoch 743 : 0.01678046410659599\n",
      "Loss after epoch 744 : 0.016775197690528106\n",
      "Loss after epoch 745 : 0.016769900440124344\n",
      "Loss after epoch 746 : 0.01676457170612941\n",
      "Loss after epoch 747 : 0.01675921084038334\n",
      "Loss after epoch 748 : 0.016753817185483605\n",
      "Loss after epoch 749 : 0.01674839008272938\n",
      "Loss after epoch 750 : 0.01674292886326783\n",
      "Loss after epoch 751 : 0.016737432852667985\n",
      "Loss after epoch 752 : 0.01673190136549569\n",
      "Loss after epoch 753 : 0.01672633370974272\n",
      "Loss after epoch 754 : 0.016720729184194667\n",
      "Loss after epoch 755 : 0.016715087080141043\n",
      "Loss after epoch 756 : 0.016709406680345436\n",
      "Loss after epoch 757 : 0.016703687259718387\n",
      "Loss after epoch 758 : 0.016697928086682048\n",
      "Loss after epoch 759 : 0.01669212842114486\n",
      "Loss after epoch 760 : 0.016686287517515126\n",
      "Loss after epoch 761 : 0.016680404621634063\n",
      "Loss after epoch 762 : 0.016674478976609747\n",
      "Loss after epoch 763 : 0.01666850981741534\n",
      "Loss after epoch 764 : 0.0166624963811513\n",
      "Loss after epoch 765 : 0.01665643789949883\n",
      "Loss after epoch 766 : 0.016650333620205505\n",
      "Loss after epoch 767 : 0.01664418279731305\n",
      "Loss after epoch 768 : 0.016637984738639543\n",
      "Loss after epoch 769 : 0.01663173880411231\n",
      "Loss after epoch 770 : 0.01662544452433748\n",
      "Loss after epoch 771 : 0.016619101651778774\n",
      "Loss after epoch 772 : 0.016612710502899475\n",
      "Loss after epoch 773 : 0.016606272253508428\n",
      "Loss after epoch 774 : 0.016599790073044317\n",
      "Loss after epoch 775 : 0.016593270452965863\n",
      "Loss after epoch 776 : 0.016586727431446797\n",
      "Loss after epoch 777 : 0.01658018814665026\n",
      "Loss after epoch 778 : 0.01657371021883337\n",
      "Loss after epoch 779 : 0.01656740363691802\n",
      "Loss after epoch 780 : 0.016561507915277725\n",
      "Loss after epoch 781 : 0.016556464492930585\n",
      "Loss after epoch 782 : 0.01655326538609495\n",
      "Loss after epoch 783 : 0.01655348879150956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 784 : 0.016560603141897313\n",
      "Loss after epoch 785 : 0.01657703934231696\n",
      "Loss after epoch 786 : 0.01660622535746788\n",
      "Loss after epoch 787 : 0.016629840257530375\n",
      "Loss after epoch 788 : 0.016628380356321495\n",
      "Loss after epoch 789 : 0.016574770316826474\n",
      "Loss after epoch 790 : 0.01651102189355972\n",
      "Loss after epoch 791 : 0.016488139090638447\n",
      "Loss after epoch 792 : 0.016511749399734592\n",
      "Loss after epoch 793 : 0.01653710822926187\n",
      "Loss after epoch 794 : 0.01652140879839141\n",
      "Loss after epoch 795 : 0.016479634712216223\n",
      "Loss after epoch 796 : 0.016456487535797756\n",
      "Loss after epoch 797 : 0.01646739445970464\n",
      "Loss after epoch 798 : 0.01648068114200846\n",
      "Loss after epoch 799 : 0.01646631877765051\n",
      "Loss after epoch 800 : 0.016438324282598567\n",
      "Loss after epoch 800 : 0.016438324282598567\n",
      "Loss after epoch 801 : 0.016426598000353603\n",
      "Loss after epoch 802 : 0.016433732523502807\n",
      "Loss after epoch 803 : 0.016435977524846372\n",
      "Loss after epoch 804 : 0.01642032669040346\n",
      "Loss after epoch 805 : 0.016401924273051125\n",
      "Loss after epoch 806 : 0.016396863707588794\n",
      "Loss after epoch 807 : 0.016399571812501196\n",
      "Loss after epoch 808 : 0.016394782563659307\n",
      "Loss after epoch 809 : 0.016380473586093167\n",
      "Loss after epoch 810 : 0.016368704774073112\n",
      "Loss after epoch 811 : 0.01636558132905546\n",
      "Loss after epoch 812 : 0.016364013008694746\n",
      "Loss after epoch 813 : 0.01635620744311073\n",
      "Loss after epoch 814 : 0.01634439105652543\n",
      "Loss after epoch 815 : 0.016336046430228797\n",
      "Loss after epoch 816 : 0.016332383579634038\n",
      "Loss after epoch 817 : 0.016327974803858766\n",
      "Loss after epoch 818 : 0.016319492165663134\n",
      "Loss after epoch 819 : 0.01630965413120768\n",
      "Loss after epoch 820 : 0.01630249731550991\n",
      "Loss after epoch 821 : 0.01629767060379666\n",
      "Loss after epoch 822 : 0.016291788442089027\n",
      "Loss after epoch 823 : 0.016283501641016013\n",
      "Loss after epoch 824 : 0.016274779738716782\n",
      "Loss after epoch 825 : 0.0162677518125341\n",
      "Loss after epoch 826 : 0.016261926972786285\n",
      "Loss after epoch 827 : 0.0162553543033206\n",
      "Loss after epoch 828 : 0.016247372760311293\n",
      "Loss after epoch 829 : 0.016239128381366548\n",
      "Loss after epoch 830 : 0.01623182920580633\n",
      "Loss after epoch 831 : 0.01622525261871156\n",
      "Loss after epoch 832 : 0.016218311397688208\n",
      "Loss after epoch 833 : 0.016210524460387368\n",
      "Loss after epoch 834 : 0.01620243643477733\n",
      "Loss after epoch 835 : 0.01619478795447312\n",
      "Loss after epoch 836 : 0.016187620504349715\n",
      "Loss after epoch 837 : 0.016180376168987647\n",
      "Loss after epoch 838 : 0.016172647062600395\n",
      "Loss after epoch 839 : 0.01616458479438097\n",
      "Loss after epoch 840 : 0.016156624737347948\n",
      "Loss after epoch 841 : 0.01614895242057658\n",
      "Loss after epoch 842 : 0.016141362520752885\n",
      "Loss after epoch 843 : 0.016133557278820116\n",
      "Loss after epoch 844 : 0.016125466626170124\n",
      "Loss after epoch 845 : 0.01611727335979583\n",
      "Loss after epoch 846 : 0.01610917252059781\n",
      "Loss after epoch 847 : 0.016101180177541648\n",
      "Loss after epoch 848 : 0.016093155081166884\n",
      "Loss after epoch 849 : 0.016084965145350474\n",
      "Loss after epoch 850 : 0.01607661024039842\n",
      "Loss after epoch 851 : 0.016068190652102052\n",
      "Loss after epoch 852 : 0.016059795907882656\n",
      "Loss after epoch 853 : 0.01605142608740346\n",
      "Loss after epoch 854 : 0.016043011963686165\n",
      "Loss after epoch 855 : 0.01603448927784983\n",
      "Loss after epoch 856 : 0.016025849600628854\n",
      "Loss after epoch 857 : 0.016017135720877967\n",
      "Loss after epoch 858 : 0.016008394310685563\n",
      "Loss after epoch 859 : 0.015999638542459955\n",
      "Loss after epoch 860 : 0.015990844736601017\n",
      "Loss after epoch 861 : 0.015981978422862003\n",
      "Loss after epoch 862 : 0.015973022335322336\n",
      "Loss after epoch 863 : 0.015963983560284674\n",
      "Loss after epoch 864 : 0.015954883449069063\n",
      "Loss after epoch 865 : 0.01594573873474757\n",
      "Loss after epoch 866 : 0.015936551453053442\n",
      "Loss after epoch 867 : 0.0159273105729796\n",
      "Loss after epoch 868 : 0.01591800186354146\n",
      "Loss after epoch 869 : 0.015908617491944114\n",
      "Loss after epoch 870 : 0.015899158276633425\n",
      "Loss after epoch 871 : 0.01588963147760819\n",
      "Loss after epoch 872 : 0.015880044364792227\n",
      "Loss after epoch 873 : 0.015870400338978043\n",
      "Loss after epoch 874 : 0.015860697618528378\n",
      "Loss after epoch 875 : 0.015850931378721154\n",
      "Loss after epoch 876 : 0.015841096796557968\n",
      "Loss after epoch 877 : 0.015831190825569646\n",
      "Loss after epoch 878 : 0.01582121337663927\n",
      "Loss after epoch 879 : 0.0158111659416436\n",
      "Loss after epoch 880 : 0.015801050752179118\n",
      "Loss after epoch 881 : 0.015790869155022775\n",
      "Loss after epoch 882 : 0.015780621470583803\n",
      "Loss after epoch 883 : 0.01577030683486366\n",
      "Loss after epoch 884 : 0.015759923728581628\n",
      "Loss after epoch 885 : 0.01574947066864162\n",
      "Loss after epoch 886 : 0.015738946348177532\n",
      "Loss after epoch 887 : 0.015728350100427464\n",
      "Loss after epoch 888 : 0.015717681513506547\n",
      "Loss after epoch 889 : 0.01570694066594038\n",
      "Loss after epoch 890 : 0.015696127518215454\n",
      "Loss after epoch 891 : 0.015685242184415926\n",
      "Loss after epoch 892 : 0.015674284506982286\n",
      "Loss after epoch 893 : 0.015663254319168457\n",
      "Loss after epoch 894 : 0.015652151247243822\n",
      "Loss after epoch 895 : 0.0156409749253723\n",
      "Loss after epoch 896 : 0.015629724916107143\n",
      "Loss after epoch 897 : 0.015618400787950579\n",
      "Loss after epoch 898 : 0.015607002160354726\n",
      "Loss after epoch 899 : 0.015595528633798242\n",
      "Loss after epoch 900 : 0.015583979907781693\n",
      "Loss after epoch 900 : 0.015583979907781693\n",
      "Loss after epoch 901 : 0.015572355639302947\n",
      "Loss after epoch 902 : 0.015560655622710275\n",
      "Loss after epoch 903 : 0.015548879565508114\n",
      "Loss after epoch 904 : 0.015537027377093907\n",
      "Loss after epoch 905 : 0.015525098867476463\n",
      "Loss after epoch 906 : 0.015513094147890866\n",
      "Loss after epoch 907 : 0.01550101328530917\n",
      "Loss after epoch 908 : 0.015488856891382411\n",
      "Loss after epoch 909 : 0.01547662573179403\n",
      "Loss after epoch 910 : 0.015464321772528303\n",
      "Loss after epoch 911 : 0.01545194786524099\n",
      "Loss after epoch 912 : 0.015439509949340624\n",
      "Loss after epoch 913 : 0.015427017411896832\n",
      "Loss after epoch 914 : 0.015414488787960989\n",
      "Loss after epoch 915 : 0.015401954884293821\n",
      "Loss after epoch 916 : 0.015389476625311482\n",
      "Loss after epoch 917 : 0.01537715788558891\n",
      "Loss after epoch 918 : 0.015365208692428908\n",
      "Loss after epoch 919 : 0.015353983833022439\n",
      "Loss after epoch 920 : 0.015344217914097177\n",
      "Loss after epoch 921 : 0.015337039961672043\n",
      "Loss after epoch 922 : 0.015334749690005187\n",
      "Loss after epoch 923 : 0.01533959573845568\n",
      "Loss after epoch 924 : 0.015355115013750762\n",
      "Loss after epoch 925 : 0.015375246583474566\n",
      "Loss after epoch 926 : 0.015388964972385687\n",
      "Loss after epoch 927 : 0.015364428918644892\n",
      "Loss after epoch 928 : 0.01530359131042798\n",
      "Loss after epoch 929 : 0.015238652802806684\n",
      "Loss after epoch 930 : 0.015214812651094494\n",
      "Loss after epoch 931 : 0.015231461592542826\n",
      "Loss after epoch 932 : 0.015248900300806783\n",
      "Loss after epoch 933 : 0.015233756080170576\n",
      "Loss after epoch 934 : 0.01518971620569553\n",
      "Loss after epoch 935 : 0.015155992752658038\n",
      "Loss after epoch 936 : 0.015153251224140133\n",
      "Loss after epoch 937 : 0.015162592915043328\n",
      "Loss after epoch 938 : 0.01515484237516044\n",
      "Loss after epoch 939 : 0.015125535635597653\n",
      "Loss after epoch 940 : 0.015098536600941207\n",
      "Loss after epoch 941 : 0.015090422090491627\n",
      "Loss after epoch 942 : 0.015091585713569071\n",
      "Loss after epoch 943 : 0.015083062933302274\n",
      "Loss after epoch 944 : 0.015061074045370451\n",
      "Loss after epoch 945 : 0.015039822567658945\n",
      "Loss after epoch 946 : 0.015029845125808874\n",
      "Loss after epoch 947 : 0.015025750220000375\n",
      "Loss after epoch 948 : 0.01501602955469839\n",
      "Loss after epoch 949 : 0.014998178945587264\n",
      "Loss after epoch 950 : 0.01498038958346599\n",
      "Loss after epoch 951 : 0.014969126125256246\n",
      "Loss after epoch 952 : 0.014961694859445974\n",
      "Loss after epoch 953 : 0.01495134813947904\n",
      "Loss after epoch 954 : 0.014936050233531242\n",
      "Loss after epoch 955 : 0.014920140729960719\n",
      "Loss after epoch 956 : 0.014907781324189295\n",
      "Loss after epoch 957 : 0.014898150415197955\n",
      "Loss after epoch 958 : 0.014887422407276299\n",
      "Loss after epoch 959 : 0.014873728864980682\n",
      "Loss after epoch 960 : 0.014858949226656664\n",
      "Loss after epoch 961 : 0.014845754815259475\n",
      "Loss after epoch 962 : 0.01483450850505357\n",
      "Loss after epoch 963 : 0.014823329690787907\n",
      "Loss after epoch 964 : 0.014810592053978297\n",
      "Loss after epoch 965 : 0.014796648115738428\n",
      "Loss after epoch 966 : 0.014783002408650502\n",
      "Loss after epoch 967 : 0.014770515898465312\n",
      "Loss after epoch 968 : 0.014758642499477205\n",
      "Loss after epoch 969 : 0.014746284942907618\n",
      "Loss after epoch 970 : 0.014733017539072876\n",
      "Loss after epoch 971 : 0.014719348706026465\n",
      "Loss after epoch 972 : 0.014706041236266463\n",
      "Loss after epoch 973 : 0.014693311773179023\n",
      "Loss after epoch 974 : 0.014680751034215548\n",
      "Loss after epoch 975 : 0.014667841285935875\n",
      "Loss after epoch 976 : 0.014654443778555211\n",
      "Loss after epoch 977 : 0.014640847771783629\n",
      "Loss after epoch 978 : 0.014627414954426371\n",
      "Loss after epoch 979 : 0.014614253842842106\n",
      "Loss after epoch 980 : 0.014601182913311653\n",
      "Loss after epoch 981 : 0.014587946771971743\n",
      "Loss after epoch 982 : 0.014574441058553912\n",
      "Loss after epoch 983 : 0.014560758313678188\n",
      "Loss after epoch 984 : 0.014547075232080991\n",
      "Loss after epoch 985 : 0.014533494677174361\n",
      "Loss after epoch 986 : 0.014519987213911203\n",
      "Loss after epoch 987 : 0.014506443382316\n",
      "Loss after epoch 988 : 0.014492769184050614\n",
      "Loss after epoch 989 : 0.014478949072169784\n",
      "Loss after epoch 990 : 0.014465036668199585\n",
      "Loss after epoch 991 : 0.01445110423620088\n",
      "Loss after epoch 992 : 0.014437190455748887\n",
      "Loss after epoch 993 : 0.014423285150594862\n",
      "Loss after epoch 994 : 0.014409346837885084\n",
      "Loss after epoch 995 : 0.014395334728258494\n",
      "Loss after epoch 996 : 0.014381232845281644\n",
      "Loss after epoch 997 : 0.014367051610273547\n",
      "Loss after epoch 998 : 0.01435281662027786\n",
      "Loss after epoch 999 : 0.014338550994396126\n",
      "Loss after 1epoch 1001 : 0.014338550994396126\n"
     ]
    }
   ],
   "source": [
    "c=nn_adam.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a3cbaccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_adam.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109243c",
   "metadata": {},
   "source": [
    "### SGD :\n",
    "\n",
    "Testing different minibatch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6437f",
   "metadata": {},
   "source": [
    "#### Minibatch size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35910974",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mini2 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 2,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ae8db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.039968910926929654\n",
      "Loss after epoch 200 : 0.03776559212464175\n",
      "Loss after epoch 300 : 0.03546285589429685\n",
      "Loss after epoch 400 : 0.03364404483304866\n",
      "Loss after epoch 500 : 0.03226520398508366\n",
      "Loss after epoch 600 : 0.03119983447174907\n",
      "Loss after epoch 700 : 0.030373181868311932\n",
      "Loss after epoch 800 : 0.0296842286398998\n",
      "Loss after epoch 900 : 0.02910116205558126\n",
      "Loss after epoch 1001 : 0.02860504375086494\n"
     ]
    }
   ],
   "source": [
    "c=nn_mini2.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6adc7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453651c",
   "metadata": {},
   "source": [
    "#### Minibatch size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98cdf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.11888176773455536\n",
      "Loss after epoch 200 : 0.06132759649653567\n",
      "Loss after epoch 300 : 0.04623305383163878\n",
      "Loss after epoch 400 : 0.04046450039785886\n",
      "Loss after epoch 500 : 0.03746791143051225\n",
      "Loss after epoch 600 : 0.03558795780184025\n",
      "Loss after epoch 700 : 0.034264738269116496\n",
      "Loss after epoch 800 : 0.03326249394273867\n",
      "Loss after epoch 900 : 0.03246390753995277\n",
      "Loss after epoch 1001 : 0.03180929064900143\n"
     ]
    }
   ],
   "source": [
    "nn_mini8 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 8,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini8.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b459e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini8.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1a623",
   "metadata": {},
   "source": [
    "#### minibatch size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "382411ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.3243897917934674\n",
      "Loss after epoch 200 : 0.2561084343985056\n",
      "Loss after epoch 300 : 0.2136039045778299\n",
      "Loss after epoch 400 : 0.18848324876452216\n",
      "Loss after epoch 500 : 0.17213766425768745\n",
      "Loss after epoch 600 : 0.16027435713105817\n",
      "Loss after epoch 700 : 0.1496555757990457\n",
      "Loss after epoch 800 : 0.13870465628954357\n",
      "Loss after epoch 900 : 0.1274764365402452\n",
      "Loss after epoch 1001 : 0.11588206617477194\n"
     ]
    }
   ],
   "source": [
    "nn_mini16 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 16,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini16.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "655c5ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini16.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46241f6",
   "metadata": {},
   "source": [
    "#### Minibatch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f656934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.24423174555697824\n",
      "Loss after epoch 200 : 0.10864390175002289\n",
      "Loss after epoch 300 : 0.07464147547420144\n",
      "Loss after epoch 400 : 0.05800286097313573\n",
      "Loss after epoch 500 : 0.04877020258397195\n",
      "Loss after epoch 600 : 0.04319664204297226\n",
      "Loss after epoch 700 : 0.03958215983536185\n",
      "Loss after epoch 800 : 0.03710860405512004\n",
      "Loss after epoch 900 : 0.03531851438494933\n",
      "Loss after epoch 1001 : 0.03398528894579797\n"
     ]
    }
   ],
   "source": [
    "nn_mini32 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 32,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini32.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa8c3b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini32.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d307c12",
   "metadata": {},
   "source": [
    "#### Minibatch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fb94ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.26834528254850826\n",
      "Loss after epoch 200 : 0.2058438974711505\n",
      "Loss after epoch 300 : 0.1689162735929759\n",
      "Loss after epoch 400 : 0.14561402715011804\n",
      "Loss after epoch 500 : 0.12773139351036716\n",
      "Loss after epoch 600 : 0.11274114420052123\n",
      "Loss after epoch 700 : 0.09998996333689264\n",
      "Loss after epoch 800 : 0.08925000737231119\n",
      "Loss after epoch 900 : 0.0803031409095878\n",
      "Loss after epoch 1001 : 0.07295611140430698\n"
     ]
    }
   ],
   "source": [
    "nn_mini64 = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "               batch_size = 64,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini64.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "231d13cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini64.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af548c3",
   "metadata": {},
   "source": [
    "#### Minibactch size not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d36d408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.362080412007166\n",
      "Loss after epoch 200 : 0.3553161314697229\n",
      "Loss after epoch 300 : 0.3485307292971733\n",
      "Loss after epoch 400 : 0.3399950463358105\n",
      "Loss after epoch 500 : 0.33008496343476673\n",
      "Loss after epoch 600 : 0.3192536157965336\n",
      "Loss after epoch 700 : 0.30788814053331465\n",
      "Loss after epoch 800 : 0.29630978638834077\n",
      "Loss after epoch 900 : 0.2848041400661859\n",
      "Loss after epoch 1001 : 0.273725451778514\n"
     ]
    }
   ],
   "source": [
    "nn_mini = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid,\n",
    "               optimizer='minibatch',\n",
    "               #batch_size = 64,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )\n",
    "c=nn_mini.run(X_train, y_train_cat, epochs=1000 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd0dfa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "acc = nn_mini.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04b011",
   "metadata": {},
   "source": [
    "## To add :\n",
    "- Stopping test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b91fe",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "### Part 4 -   Two Hidden Layers Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1ee59",
   "metadata": {},
   "source": [
    "# Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1383e",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden1 x N_features\n",
    "- **b1**    : Hidden1\n",
    "- **W2**    : Hidden2 x Hidden1\n",
    "- **b2**    : Hidden2\n",
    "- **W3**    : Output x Hidden\n",
    "- **b3**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "de86d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenTwo:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes_1,\n",
    "                 hidden_nodes_2,\n",
    "                 activation_hidden_1,\n",
    "                 activation_hidden_2,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                 \n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes_1 = hidden_nodes_1    \n",
    "        self.hidden_nodes_2 = hidden_nodes_2    \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden_1 = activation_hidden_1\n",
    "        self.activation_hidden_2 = activation_hidden_2\n",
    "        self.hidden_derivative_1 = derivative(self.activation_hidden_1)\n",
    "        self.hidden_derivative_2 = derivative(self.activation_hidden_2)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "        # W1 of size hidden x features\n",
    "        n1 = self.input_nodes * self.hidden_nodes_1\n",
    "        self.W1 = tn.rvs(n1).reshape((self.hidden_nodes_1, self.input_nodes )) # hidden1 x features\n",
    "        # W2 of size hidden2 x hidden1\n",
    "        n2 = self.hidden_nodes_2 * self.hidden_nodes_1\n",
    "        self.W2 = tn.rvs(n2).reshape((self.hidden_nodes_2, self.hidden_nodes_1 )) # hidden1 x features\n",
    "        # W3 of size output x hidden2\n",
    "        n3 = self.hidden_nodes_2  * self.output_nodes\n",
    "        self.W3 = tn.rvs(n3).reshape((self.output_nodes, self.hidden_nodes_2 )) # output x hidden\n",
    "    \n",
    "    def create_biases(self):    \n",
    "        tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.b1 = tn.rvs(self.hidden_nodes_1).reshape(-1,1) \n",
    "        self.b2 = tn.rvs(self.hidden_nodes_2).reshape(-1,1) \n",
    "        self.b3 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "        \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Vdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Vdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Sdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Sdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1      # Hidden1 x N_samples\n",
    "        A1 = self.activation_hidden_1(Z1)      # Hidden1 x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2      # Hidden2 x N_samples\n",
    "        A2 = self.activation_hidden_2(Z2)      # Hidden2 x N_samples\n",
    "        Z3 = self.W3.dot(A2) + self.b3       # Output x N_samples\n",
    "        A3 = softmax(Z3)                     #Output x N_samples\n",
    "        return A3, Z3, A2, Z2, A1, Z1\n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N_samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "     \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr*dW3\n",
    "        self.b3 -= lr*db3\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A3)/pass_length\n",
    "            # deltas\n",
    "            dZ3 = A3 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW3 = dZ3.dot(A2.T)/batch_size                                   #Output x hidden_2\n",
    "            db3 = np.sum(dZ3, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)            # Hidden2 x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   # Hidden2 x Hidden1 \n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              # Hidden2 x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)            # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              # Hidden x 1                        \n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W3 -= lr*dW3\n",
    "            self.b3 -= lr*db3\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N samples\n",
    "        m = X.shape[0]   \n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdw3 = beta1*self.Vdw3 + (1-beta1)*dW3\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        self.Vdb3 = beta1*self.Vdb3 + (1-beta1)*db3\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdw3 = beta2*self.Sdw3 + (1-beta2)*dW3**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2\n",
    "        self.Sdb3 = beta2*self.Sdb3 + (1-beta2)*db3**2  \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr * self.Vdw3 / (np.sqrt(self.Sdw3)+1e-8)\n",
    "        self.b3 -= lr * self.Vdb3 / (np.sqrt(self.Sdb3)+1e-8)\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "      \n",
    "    def predict(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A3\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def xrun(self, X_train, target, epochs=10):\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X_train)\n",
    "            cost = cross_entropy(target, A3)\n",
    "            costs.append(cost)\n",
    "            if i%100 == 0:\n",
    "                print(f'Loss after epoch {i} : {cost}')\n",
    "            self.backprop(X_train, target)\n",
    "        return costs  \n",
    "         \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    print(f'Loss after epoch {i} : {cost}')\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 3epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 4epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 5epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 6epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 7epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%100 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after 8epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "896a8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "               hidden_nodes_2 = M-1,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0f73c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 1.233171162332996\n",
      "Loss after epoch 100 : 0.21597558644323173\n",
      "Loss after epoch 200 : 0.20028702353854805\n",
      "Loss after epoch 300 : 0.1869508340775123\n",
      "Loss after epoch 400 : 0.15817168500537604\n",
      "Loss after epoch 500 : 0.14096785701019318\n",
      "Loss after epoch 600 : 0.13359149740797818\n",
      "Loss after epoch 700 : 0.1273766388234808\n",
      "Loss after epoch 800 : 0.12225297351401736\n",
      "Loss after epoch 900 : 0.11798193686301858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.233171162332996,\n",
       " 1.0249252712277452,\n",
       " 0.7202910852378479,\n",
       " 0.5210404751424742,\n",
       " 0.6043331680601531,\n",
       " 0.26305352151937117,\n",
       " 0.2429786069297435,\n",
       " 0.23868718338907358,\n",
       " 0.23648939443658523,\n",
       " 0.23528441132509573,\n",
       " 0.23456695947894565,\n",
       " 0.23407789196230452,\n",
       " 0.23369525776347358,\n",
       " 0.23336250273311468,\n",
       " 0.23305723366877573,\n",
       " 0.23277026554252842,\n",
       " 0.23249479451062963,\n",
       " 0.23222888540560277,\n",
       " 0.23197088509871053,\n",
       " 0.23171946063335602,\n",
       " 0.23147408152418353,\n",
       " 0.23123465165915696,\n",
       " 0.23100072188221152,\n",
       " 0.23077271409728117,\n",
       " 0.23055115605667006,\n",
       " 0.23033615241504443,\n",
       " 0.2301263551636262,\n",
       " 0.22992124959263174,\n",
       " 0.22971997703263794,\n",
       " 0.2295215798755038,\n",
       " 0.22932447456614924,\n",
       " 0.2291273009964786,\n",
       " 0.22892893999897793,\n",
       " 0.22872848811272906,\n",
       " 0.22852499774299626,\n",
       " 0.22831825901967226,\n",
       " 0.22810915671130716,\n",
       " 0.22789830757102314,\n",
       " 0.2276857328511048,\n",
       " 0.2274723671593447,\n",
       " 0.2272601303827865,\n",
       " 0.2270492609006314,\n",
       " 0.22683985074388754,\n",
       " 0.22663194642386325,\n",
       " 0.22642575440701077,\n",
       " 0.22622108763449278,\n",
       " 0.2260179880564934,\n",
       " 0.2258160557669016,\n",
       " 0.22561520157421086,\n",
       " 0.22541537633570405,\n",
       " 0.22521645723361178,\n",
       " 0.22501837424968568,\n",
       " 0.22482113419545682,\n",
       " 0.22462466371390735,\n",
       " 0.22442900347397624,\n",
       " 0.22423404290045626,\n",
       " 0.22403982624936378,\n",
       " 0.2238463939248302,\n",
       " 0.22365363733565372,\n",
       " 0.2234616073949778,\n",
       " 0.22327017316856862,\n",
       " 0.22307940042327803,\n",
       " 0.22288922251633586,\n",
       " 0.2226995721807663,\n",
       " 0.22251051028226274,\n",
       " 0.22232206808995528,\n",
       " 0.2221342005445115,\n",
       " 0.22194689750340965,\n",
       " 0.22176012616782712,\n",
       " 0.22157384433021962,\n",
       " 0.2213879467408013,\n",
       " 0.22120244393174793,\n",
       " 0.22101732682201433,\n",
       " 0.22083259728254725,\n",
       " 0.22064824279907969,\n",
       " 0.2204642466680105,\n",
       " 0.2202806682742527,\n",
       " 0.22009747996043563,\n",
       " 0.21991469895934818,\n",
       " 0.21973214339638453,\n",
       " 0.21954995759676627,\n",
       " 0.21936811384717142,\n",
       " 0.21918668858247076,\n",
       " 0.2190057098264038,\n",
       " 0.21882510194056298,\n",
       " 0.2186448958280164,\n",
       " 0.21846504661636704,\n",
       " 0.21828555032255575,\n",
       " 0.21810639357808995,\n",
       " 0.2179275038183515,\n",
       " 0.2177487896368805,\n",
       " 0.21757030810034259,\n",
       " 0.21739203559649256,\n",
       " 0.21721399583314438,\n",
       " 0.2170362892124755,\n",
       " 0.21685884209648437,\n",
       " 0.21668165105720275,\n",
       " 0.21650472299406773,\n",
       " 0.2163280034141002,\n",
       " 0.21615165644763154,\n",
       " 0.21597558644323173,\n",
       " 0.21579977313313276,\n",
       " 0.21562421691584038,\n",
       " 0.21544884915100423,\n",
       " 0.21527367220444923,\n",
       " 0.21509876858668978,\n",
       " 0.21492415147459545,\n",
       " 0.21474980423766543,\n",
       " 0.21457592628164562,\n",
       " 0.21440227822417066,\n",
       " 0.21422892767987592,\n",
       " 0.21405581364045587,\n",
       " 0.21388299649084114,\n",
       " 0.21371051418286383,\n",
       " 0.2135383622249725,\n",
       " 0.21336647282135537,\n",
       " 0.21319499846242182,\n",
       " 0.21302385740443786,\n",
       " 0.21285320647348271,\n",
       " 0.21268293158618296,\n",
       " 0.21251297007913306,\n",
       " 0.21234341999693057,\n",
       " 0.2121742675030152,\n",
       " 0.2120055228870204,\n",
       " 0.21183714506714665,\n",
       " 0.21166911004966185,\n",
       " 0.21150141616595755,\n",
       " 0.2113341651343438,\n",
       " 0.21116731530330157,\n",
       " 0.2110008596382232,\n",
       " 0.21083488725169428,\n",
       " 0.21066929084712424,\n",
       " 0.21050414495666914,\n",
       " 0.21033941831339575,\n",
       " 0.21017518933886714,\n",
       " 0.21001135007400903,\n",
       " 0.20984786536319044,\n",
       " 0.20968471848136364,\n",
       " 0.20952203146354867,\n",
       " 0.209359814676088,\n",
       " 0.20919788281139745,\n",
       " 0.20903634894138104,\n",
       " 0.20887531946942772,\n",
       " 0.20871478908155117,\n",
       " 0.20855472258830143,\n",
       " 0.20839512344919273,\n",
       " 0.20823605948352306,\n",
       " 0.20807746494740664,\n",
       " 0.20791933605149948,\n",
       " 0.2077616921957163,\n",
       " 0.20760445127019128,\n",
       " 0.20744765443421673,\n",
       " 0.20729123084609252,\n",
       " 0.20713526882184466,\n",
       " 0.20697981028449292,\n",
       " 0.2068247185226735,\n",
       " 0.2066700004645755,\n",
       " 0.20651577060331905,\n",
       " 0.2063619882753001,\n",
       " 0.20620858971333125,\n",
       " 0.20605551081170678,\n",
       " 0.20590293107340127,\n",
       " 0.20575087815818674,\n",
       " 0.20559920078694974,\n",
       " 0.20544800725684023,\n",
       " 0.20529725998570741,\n",
       " 0.20514688758356464,\n",
       " 0.20499689320528525,\n",
       " 0.20484723241749667,\n",
       " 0.2046980794813284,\n",
       " 0.20454936692806874,\n",
       " 0.20440108842993665,\n",
       " 0.20425317912303395,\n",
       " 0.20410572147957273,\n",
       " 0.20395875826322687,\n",
       " 0.20381227217326914,\n",
       " 0.20366620832134996,\n",
       " 0.2035205817902144,\n",
       " 0.2033754284107456,\n",
       " 0.2032307340164918,\n",
       " 0.20308645165337946,\n",
       " 0.20294265988531654,\n",
       " 0.20279930974221708,\n",
       " 0.2026563255149233,\n",
       " 0.2025137470670563,\n",
       " 0.2023716362840288,\n",
       " 0.2022299117334407,\n",
       " 0.20208868879776526,\n",
       " 0.20194782468358674,\n",
       " 0.20180727657183298,\n",
       " 0.20166714521397353,\n",
       " 0.20152745371480058,\n",
       " 0.20138823634318173,\n",
       " 0.20124938089164507,\n",
       " 0.2011108811165026,\n",
       " 0.20097276771556533,\n",
       " 0.20083498091577084,\n",
       " 0.20069747965249973,\n",
       " 0.20056031657691145,\n",
       " 0.20042346676106906,\n",
       " 0.20028702353854805,\n",
       " 0.2001509631718736,\n",
       " 0.2000151855419317,\n",
       " 0.19987979477938744,\n",
       " 0.19974473005221602,\n",
       " 0.19961001616519336,\n",
       " 0.19947552795722673,\n",
       " 0.19934140545824167,\n",
       " 0.19920761191900013,\n",
       " 0.19907409542525054,\n",
       " 0.19894087235456218,\n",
       " 0.19880788429668492,\n",
       " 0.1986751069209269,\n",
       " 0.19854258807462838,\n",
       " 0.19841032185724827,\n",
       " 0.19827825138751837,\n",
       " 0.19814642679603,\n",
       " 0.19801480772661906,\n",
       " 0.19788332372438633,\n",
       " 0.1977520896256672,\n",
       " 0.1976209632132302,\n",
       " 0.19748991229432633,\n",
       " 0.19735904774956486,\n",
       " 0.19722850482413212,\n",
       " 0.19709817035076563,\n",
       " 0.1969680445491393,\n",
       " 0.19683805029356852,\n",
       " 0.1967081663472901,\n",
       " 0.19657842524076466,\n",
       " 0.19644878326472995,\n",
       " 0.19631915333062555,\n",
       " 0.19618948517599466,\n",
       " 0.19605985227938189,\n",
       " 0.19593036091712926,\n",
       " 0.19580099031916626,\n",
       " 0.19567149807522702,\n",
       " 0.19554203578138338,\n",
       " 0.1954126837573751,\n",
       " 0.19528333103041431,\n",
       " 0.1951540785748366,\n",
       " 0.1950248311677358,\n",
       " 0.194895623175834,\n",
       " 0.19476645327296965,\n",
       " 0.19463740062364698,\n",
       " 0.1945083139253388,\n",
       " 0.19437920750952464,\n",
       " 0.1942500823534018,\n",
       " 0.19412095298332746,\n",
       " 0.1939917352683217,\n",
       " 0.19386251838279914,\n",
       " 0.19373329165713865,\n",
       " 0.19360390905882816,\n",
       " 0.19347440748340938,\n",
       " 0.1933448678969991,\n",
       " 0.19321528581953237,\n",
       " 0.19308564133617384,\n",
       " 0.1929558929931308,\n",
       " 0.19282594888749005,\n",
       " 0.19269577245546426,\n",
       " 0.19256546981479533,\n",
       " 0.19243508836568485,\n",
       " 0.19230453363220537,\n",
       " 0.19217382034719832,\n",
       " 0.19204298565111846,\n",
       " 0.19191197179367786,\n",
       " 0.19178074215233715,\n",
       " 0.1916492679764181,\n",
       " 0.1915175571226538,\n",
       " 0.19138560473195637,\n",
       " 0.1912533326262848,\n",
       " 0.19112078889837938,\n",
       " 0.19098803402484557,\n",
       " 0.19085505489289176,\n",
       " 0.19072177313662875,\n",
       " 0.19058821143081528,\n",
       " 0.19045428341799484,\n",
       " 0.19031998205183612,\n",
       " 0.1901852117436826,\n",
       " 0.19005006147830528,\n",
       " 0.18991455407864757,\n",
       " 0.1897786446389446,\n",
       " 0.1896423698767978,\n",
       " 0.18950568538871582,\n",
       " 0.18936861146256057,\n",
       " 0.1892309930258937,\n",
       " 0.18909287004025563,\n",
       " 0.18895434878212275,\n",
       " 0.18881533934075082,\n",
       " 0.18867573833777473,\n",
       " 0.18853561016241083,\n",
       " 0.18839478374760316,\n",
       " 0.18825322534169608,\n",
       " 0.18811116700739675,\n",
       " 0.18796854712986896,\n",
       " 0.18782525624637192,\n",
       " 0.1876813584092388,\n",
       " 0.18753678271758323,\n",
       " 0.18739150986516184,\n",
       " 0.187245433961192,\n",
       " 0.187098551264497,\n",
       " 0.1869508340775123,\n",
       " 0.18680235122941008,\n",
       " 0.1866530742679999,\n",
       " 0.18650299932144965,\n",
       " 0.18635205009509123,\n",
       " 0.18620014092990705,\n",
       " 0.1860472022358473,\n",
       " 0.18589307579154213,\n",
       " 0.185737961690578,\n",
       " 0.18558177203262602,\n",
       " 0.18542471543471425,\n",
       " 0.1852667270716815,\n",
       " 0.18510763797788693,\n",
       " 0.18494741326493847,\n",
       " 0.18478599435768572,\n",
       " 0.18462315110610739,\n",
       " 0.18445893969736138,\n",
       " 0.1842933811077814,\n",
       " 0.18412650130608244,\n",
       " 0.18395830806457386,\n",
       " 0.18378864118617844,\n",
       " 0.18361705170257747,\n",
       " 0.18344375826811318,\n",
       " 0.183269015893567,\n",
       " 0.18309254115922782,\n",
       " 0.18291436208048903,\n",
       " 0.18273422874153783,\n",
       " 0.1825521980968181,\n",
       " 0.18236821595593944,\n",
       " 0.182181906148083,\n",
       " 0.18199347644918218,\n",
       " 0.18180274040443017,\n",
       " 0.18160962864186223,\n",
       " 0.18141384784039366,\n",
       " 0.18121458894908643,\n",
       " 0.18101187053317133,\n",
       " 0.18080589708590308,\n",
       " 0.18059678766583975,\n",
       " 0.18038438019772055,\n",
       " 0.18016813666467124,\n",
       " 0.1799475218114465,\n",
       " 0.1797225168510555,\n",
       " 0.17949237667581336,\n",
       " 0.17925679720159993,\n",
       " 0.17901514128820323,\n",
       " 0.17876623168662276,\n",
       " 0.17850966106712682,\n",
       " 0.17824507968770975,\n",
       " 0.17796935428332078,\n",
       " 0.17768263579871482,\n",
       " 0.17738304394140203,\n",
       " 0.17706934428556773,\n",
       " 0.17673478663326248,\n",
       " 0.17637679160103414,\n",
       " 0.17599359259040157,\n",
       " 0.1755831056927089,\n",
       " 0.17515368478419371,\n",
       " 0.17471030687127026,\n",
       " 0.17425925744585047,\n",
       " 0.1737996073139086,\n",
       " 0.17333193777184153,\n",
       " 0.1728572168877398,\n",
       " 0.17237944328138793,\n",
       " 0.17190041190030944,\n",
       " 0.17141830984030543,\n",
       " 0.1709347416247391,\n",
       " 0.1704508598687512,\n",
       " 0.16997024652291426,\n",
       " 0.1694943048305255,\n",
       " 0.16902311695095143,\n",
       " 0.16855838219561056,\n",
       " 0.16810031164383663,\n",
       " 0.16764932250012196,\n",
       " 0.16720811749114328,\n",
       " 0.16677689476251129,\n",
       " 0.1663577386114014,\n",
       " 0.1659503098694712,\n",
       " 0.16555392871454364,\n",
       " 0.16516928615965668,\n",
       " 0.16479468222529495,\n",
       " 0.1644293938133092,\n",
       " 0.16407233073813576,\n",
       " 0.16372345191494722,\n",
       " 0.1633817821255884,\n",
       " 0.1630464819573682,\n",
       " 0.1627166088119584,\n",
       " 0.16239146715747732,\n",
       " 0.16207075930592985,\n",
       " 0.1617542188876685,\n",
       " 0.16144109708644283,\n",
       " 0.16113098587907274,\n",
       " 0.16082355351134378,\n",
       " 0.16051854275120297,\n",
       " 0.16021618563454898,\n",
       " 0.15991633094121396,\n",
       " 0.15961928204614584,\n",
       " 0.15932502031869658,\n",
       " 0.15903308843739242,\n",
       " 0.15874344738910276,\n",
       " 0.15845624620579046,\n",
       " 0.15817168500537604,\n",
       " 0.15788952751626328,\n",
       " 0.15760987450291783,\n",
       " 0.15733300216608348,\n",
       " 0.15705898319228126,\n",
       " 0.15678735722916126,\n",
       " 0.1565182399990708,\n",
       " 0.1562517622695015,\n",
       " 0.15598805943209046,\n",
       " 0.15572685071463496,\n",
       " 0.15546815142780115,\n",
       " 0.15521198693439117,\n",
       " 0.15495834593136035,\n",
       " 0.15470714130517985,\n",
       " 0.1544584804939739,\n",
       " 0.15421229147673948,\n",
       " 0.15396897695048797,\n",
       " 0.1537277459760409,\n",
       " 0.15348904625493862,\n",
       " 0.15325203525175274,\n",
       " 0.1530179372004173,\n",
       " 0.15278553708731038,\n",
       " 0.15255561758437738,\n",
       " 0.15232719634968048,\n",
       " 0.1521016052399795,\n",
       " 0.1518770298084207,\n",
       " 0.15165527935003306,\n",
       " 0.15143438294571956,\n",
       " 0.15121671539560821,\n",
       " 0.15099909942459944,\n",
       " 0.15078493575877813,\n",
       " 0.15057047349494876,\n",
       " 0.15035995451759634,\n",
       " 0.15014907716210887,\n",
       " 0.14994180099581345,\n",
       " 0.14973320366352544,\n",
       " 0.14952902471841292,\n",
       " 0.14932305952350564,\n",
       " 0.14912167552748612,\n",
       " 0.14891859228573534,\n",
       " 0.1487203016715421,\n",
       " 0.14851975182051044,\n",
       " 0.14832541655501358,\n",
       " 0.14812669651621638,\n",
       " 0.1479351380874391,\n",
       " 0.14773906936459918,\n",
       " 0.14755001092430797,\n",
       " 0.14735635444695447,\n",
       " 0.1471700565321307,\n",
       " 0.14697879847864015,\n",
       " 0.14679507284002383,\n",
       " 0.14660608275740883,\n",
       " 0.14642471426758566,\n",
       " 0.1462381798635553,\n",
       " 0.14605948765628712,\n",
       " 0.14587554324965196,\n",
       " 0.1456997539641968,\n",
       " 0.14551771731315483,\n",
       " 0.1453449194121774,\n",
       " 0.145165643722076,\n",
       " 0.1449961549771266,\n",
       " 0.1448193701915397,\n",
       " 0.14465311968102476,\n",
       " 0.14447868720344684,\n",
       " 0.14431512605557328,\n",
       " 0.14414330981418788,\n",
       " 0.14398355837051946,\n",
       " 0.14381419309672425,\n",
       " 0.14365829951550008,\n",
       " 0.1434913898587123,\n",
       " 0.14334011083821435,\n",
       " 0.14317574936221752,\n",
       " 0.14302939364889078,\n",
       " 0.14286783835677572,\n",
       " 0.14272685922628828,\n",
       " 0.14256851093840991,\n",
       " 0.14243444421496754,\n",
       " 0.14227934529608705,\n",
       " 0.14215387186646375,\n",
       " 0.1420023870232464,\n",
       " 0.14188691682705867,\n",
       " 0.1417395301057695,\n",
       " 0.14163678864025384,\n",
       " 0.1414945955551531,\n",
       " 0.14140869580490573,\n",
       " 0.14127137251780314,\n",
       " 0.14120645966406645,\n",
       " 0.14107445841073035,\n",
       " 0.14103684822776583,\n",
       " 0.14091128606104916,\n",
       " 0.14090576119762735,\n",
       " 0.14078493201797196,\n",
       " 0.1408194801127456,\n",
       " 0.1407027013072156,\n",
       " 0.14078631699819463,\n",
       " 0.14066560302390074,\n",
       " 0.14080525322516796,\n",
       " 0.14067388535868564,\n",
       " 0.14087034031632292,\n",
       " 0.1407158664359133,\n",
       " 0.14096785701019318,\n",
       " 0.140772355016418,\n",
       " 0.14107030682403426,\n",
       " 0.14081936773633336,\n",
       " 0.14114306834931153,\n",
       " 0.14083097830308863,\n",
       " 0.14116197045847767,\n",
       " 0.14078492761612507,\n",
       " 0.1411065595888472,\n",
       " 0.14067996546673717,\n",
       " 0.14098345405565973,\n",
       " 0.14052535613190142,\n",
       " 0.14080649953948116,\n",
       " 0.1403348884393957,\n",
       " 0.14059699243270202,\n",
       " 0.14012904813394597,\n",
       " 0.14037569781510642,\n",
       " 0.13991941446192527,\n",
       " 0.1401541145023381,\n",
       " 0.1397133803775049,\n",
       " 0.13993827583072832,\n",
       " 0.13951542376237255,\n",
       " 0.13973201313273764,\n",
       " 0.1393273632932334,\n",
       " 0.13953629143481924,\n",
       " 0.1391476039262786,\n",
       " 0.13934693476245572,\n",
       " 0.13897412074647894,\n",
       " 0.13916434061964886,\n",
       " 0.13880560800136266,\n",
       " 0.1389855147371067,\n",
       " 0.1386393959897482,\n",
       " 0.1388081596532833,\n",
       " 0.1384742409274298,\n",
       " 0.13863131381743393,\n",
       " 0.13831263854443618,\n",
       " 0.13845799823915497,\n",
       " 0.13815298845077958,\n",
       " 0.1382871833755505,\n",
       " 0.13799630007969896,\n",
       " 0.13811844887594832,\n",
       " 0.13784334842940904,\n",
       " 0.13795463290815282,\n",
       " 0.1376926726551841,\n",
       " 0.13779173841968814,\n",
       " 0.13754215286492147,\n",
       " 0.13762820054159747,\n",
       " 0.1373922435044297,\n",
       " 0.13746544702705485,\n",
       " 0.13724410540606788,\n",
       " 0.1373040153212787,\n",
       " 0.13709800722592466,\n",
       " 0.1371447883903581,\n",
       " 0.13695305484729411,\n",
       " 0.13698687749626004,\n",
       " 0.13680857459624007,\n",
       " 0.13682801246539228,\n",
       " 0.13666504572010338,\n",
       " 0.1366704862242124,\n",
       " 0.13652059179935336,\n",
       " 0.13651381306649824,\n",
       " 0.13637831473370146,\n",
       " 0.13635791766620042,\n",
       " 0.13623704403746,\n",
       " 0.136202953234816,\n",
       " 0.13609629926275194,\n",
       " 0.13604874812319973,\n",
       " 0.13595834811222599,\n",
       " 0.13589776833856831,\n",
       " 0.13582289392273006,\n",
       " 0.13574867566985627,\n",
       " 0.13568854407818182,\n",
       " 0.13560003343224755,\n",
       " 0.13555474642223236,\n",
       " 0.135452405568992,\n",
       " 0.13542133319908778,\n",
       " 0.1353046399512579,\n",
       " 0.13528806977006658,\n",
       " 0.1351577186946745,\n",
       " 0.13515697888783318,\n",
       " 0.13501320237831335,\n",
       " 0.1350261068892139,\n",
       " 0.13486876778915632,\n",
       " 0.1348935176029967,\n",
       " 0.13472236958595207,\n",
       " 0.13476215565435035,\n",
       " 0.13457826485494154,\n",
       " 0.13463276230837637,\n",
       " 0.13443520609963427,\n",
       " 0.13450354724451843,\n",
       " 0.13429248502210936,\n",
       " 0.13437353567282678,\n",
       " 0.13414990957693362,\n",
       " 0.13424594130246206,\n",
       " 0.1340102408472403,\n",
       " 0.13412028565461317,\n",
       " 0.13387176657166394,\n",
       " 0.1339942420818722,\n",
       " 0.13373188081724335,\n",
       " 0.13386666565227268,\n",
       " 0.13359149740797818,\n",
       " 0.13373911038821454,\n",
       " 0.1334515314675559,\n",
       " 0.13361081314366632,\n",
       " 0.13331075166606843,\n",
       " 0.13348301871379792,\n",
       " 0.13317217662297387,\n",
       " 0.13335935520177652,\n",
       " 0.1330377446836927,\n",
       " 0.13323615059684676,\n",
       " 0.1329027820973023,\n",
       " 0.1331142358435588,\n",
       " 0.13276863900639688,\n",
       " 0.13299296742292466,\n",
       " 0.13263480370751227,\n",
       " 0.13287001988251038,\n",
       " 0.13250001051261912,\n",
       " 0.13274633808969993,\n",
       " 0.13236644613970608,\n",
       " 0.132623634767999,\n",
       " 0.13223338533035406,\n",
       " 0.13250391648095214,\n",
       " 0.1321029452922806,\n",
       " 0.1323865527590764,\n",
       " 0.13197467287735132,\n",
       " 0.1322685378186567,\n",
       " 0.13184369017019734,\n",
       " 0.13215038276910085,\n",
       " 0.13171429541976862,\n",
       " 0.13203130469317964,\n",
       " 0.13158577818543382,\n",
       " 0.1319134022892186,\n",
       " 0.1314581163791089,\n",
       " 0.1317956263970639,\n",
       " 0.13132988435095042,\n",
       " 0.1316787181525562,\n",
       " 0.13120339685685128,\n",
       " 0.13156261269502217,\n",
       " 0.13107687460011058,\n",
       " 0.1314462320649244,\n",
       " 0.13095023225692423,\n",
       " 0.13132774865877989,\n",
       " 0.13082314931544914,\n",
       " 0.13120932387572454,\n",
       " 0.13069586791572554,\n",
       " 0.13109116344612787,\n",
       " 0.1305695554361822,\n",
       " 0.13097614750886455,\n",
       " 0.13044697656712695,\n",
       " 0.13086204400286497,\n",
       " 0.1303247724736768,\n",
       " 0.13074741439955193,\n",
       " 0.13020107557069366,\n",
       " 0.1306318459957829,\n",
       " 0.1300764058667418,\n",
       " 0.1305153802014027,\n",
       " 0.12995188671111707,\n",
       " 0.13039856560078186,\n",
       " 0.12982798608360657,\n",
       " 0.130281538601631,\n",
       " 0.12970506945780574,\n",
       " 0.13016713907672622,\n",
       " 0.12958297223369802,\n",
       " 0.13005240636563925,\n",
       " 0.12946072569557426,\n",
       " 0.12993804912843165,\n",
       " 0.12934038352667757,\n",
       " 0.12982470971468454,\n",
       " 0.12922021520868834,\n",
       " 0.12971206317253536,\n",
       " 0.12910217929827036,\n",
       " 0.12959950465421535,\n",
       " 0.12898269949504187,\n",
       " 0.12948665935615244,\n",
       " 0.12886231649043972,\n",
       " 0.12937306087882053,\n",
       " 0.12874344066224608,\n",
       " 0.12926145211672121,\n",
       " 0.12862750652333707,\n",
       " 0.12915290624668438,\n",
       " 0.1285127712991888,\n",
       " 0.12904476058821882,\n",
       " 0.12839795191592032,\n",
       " 0.1289360513782487,\n",
       " 0.12828332620969154,\n",
       " 0.1288268847639815,\n",
       " 0.1281688522013361,\n",
       " 0.1287179321493859,\n",
       " 0.12805528615733172,\n",
       " 0.12860925556487923,\n",
       " 0.12794037377317935,\n",
       " 0.12849974308405482,\n",
       " 0.1278260739744504,\n",
       " 0.12838969272026468,\n",
       " 0.1277117920285004,\n",
       " 0.1282804345717178,\n",
       " 0.12759907931956496,\n",
       " 0.1281728746752753,\n",
       " 0.12748705510947705,\n",
       " 0.12806658862988937,\n",
       " 0.1273766388234808,\n",
       " 0.12796076957057592,\n",
       " 0.12726528736099094,\n",
       " 0.12785450670289822,\n",
       " 0.12715468361788912,\n",
       " 0.12774802104422964,\n",
       " 0.12704380447366545,\n",
       " 0.12764061974450325,\n",
       " 0.1269325212927978,\n",
       " 0.12753313807350664,\n",
       " 0.12682217066344562,\n",
       " 0.12742712053309496,\n",
       " 0.12671295355023318,\n",
       " 0.1273221175391043,\n",
       " 0.12660587710976035,\n",
       " 0.12721916265298003,\n",
       " 0.12649953595414357,\n",
       " 0.1271148289907287,\n",
       " 0.12639122915201056,\n",
       " 0.12701026999423165,\n",
       " 0.12628418303825592,\n",
       " 0.12690455382910457,\n",
       " 0.12617395775220172,\n",
       " 0.12679807508501204,\n",
       " 0.1260652790285942,\n",
       " 0.12669152538109313,\n",
       " 0.1259567633902944,\n",
       " 0.12658580986351145,\n",
       " 0.12584923078324156,\n",
       " 0.12648115676179741,\n",
       " 0.12574323492588707,\n",
       " 0.12637765602722947,\n",
       " 0.12563771959432385,\n",
       " 0.12627246288702057,\n",
       " 0.1255304802609555,\n",
       " 0.12616810758123798,\n",
       " 0.12542449254481028,\n",
       " 0.12606373391116343,\n",
       " 0.12531851187495124,\n",
       " 0.1259602315176693,\n",
       " 0.12521462151424576,\n",
       " 0.12585618228681084,\n",
       " 0.12510819392305583,\n",
       " 0.1257513778415581,\n",
       " 0.12500261126753362,\n",
       " 0.12564826578183172,\n",
       " 0.12489961565132765,\n",
       " 0.12554633723408193,\n",
       " 0.12479704760849136,\n",
       " 0.12544437385235735,\n",
       " 0.12469360926817548,\n",
       " 0.12534322286101957,\n",
       " 0.12459118691332721,\n",
       " 0.1252410213416137,\n",
       " 0.12448709501774448,\n",
       " 0.12513838807627847,\n",
       " 0.1243831875337204,\n",
       " 0.12503573990714248,\n",
       " 0.1242811805789032,\n",
       " 0.12493549076462374,\n",
       " 0.12418014151602413,\n",
       " 0.12483589905849107,\n",
       " 0.12408134518361462,\n",
       " 0.12473683130724457,\n",
       " 0.12398072560761013,\n",
       " 0.1246378825563074,\n",
       " 0.12388177609650297,\n",
       " 0.1245394401762303,\n",
       " 0.123783219702473,\n",
       " 0.12444148654053958,\n",
       " 0.12368517767066903,\n",
       " 0.12434320924109163,\n",
       " 0.12358722815454702,\n",
       " 0.12424447434113892,\n",
       " 0.12348903717892001,\n",
       " 0.12414521005883083,\n",
       " 0.12338876572291362,\n",
       " 0.12404452235547192,\n",
       " 0.12328860250575784,\n",
       " 0.12394491800657585,\n",
       " 0.12319149671144078,\n",
       " 0.12384719418758257,\n",
       " 0.12309531561796563,\n",
       " 0.12375035831914992,\n",
       " 0.12299858099360449,\n",
       " 0.12365333933729696,\n",
       " 0.12290285486869539,\n",
       " 0.12355748379441595,\n",
       " 0.12280928703201542,\n",
       " 0.123463871088037,\n",
       " 0.12271594044695455,\n",
       " 0.12337064029117736,\n",
       " 0.12262311253492034,\n",
       " 0.12327842544620805,\n",
       " 0.12253117575756277,\n",
       " 0.12318478078064363,\n",
       " 0.12244048454399441,\n",
       " 0.12309161223233013,\n",
       " 0.12234517336996427,\n",
       " 0.12299644794111887,\n",
       " 0.12225297351401736,\n",
       " 0.1229012880499572,\n",
       " 0.12215747311118665,\n",
       " 0.12280638353406483,\n",
       " 0.12206539481551629,\n",
       " 0.12271379268543811,\n",
       " 0.12197492663800867,\n",
       " 0.12262155926363062,\n",
       " 0.12188410676893698,\n",
       " 0.12252921758485275,\n",
       " 0.121792228758851,\n",
       " 0.12243588261354972,\n",
       " 0.12169925566254533,\n",
       " 0.12234301496390598,\n",
       " 0.12161007683161998,\n",
       " 0.12225110764156491,\n",
       " 0.12152126727449941,\n",
       " 0.122159885078384,\n",
       " 0.12143022400914369,\n",
       " 0.12206689472683514,\n",
       " 0.1213389802618284,\n",
       " 0.12197438069385712,\n",
       " 0.12124792130718917,\n",
       " 0.12188203032754905,\n",
       " 0.12115720663710677,\n",
       " 0.12179049014116584,\n",
       " 0.12106756793506453,\n",
       " 0.12169819430016704,\n",
       " 0.12097777057727516,\n",
       " 0.12160780073321849,\n",
       " 0.12089044411329815,\n",
       " 0.12151856579191565,\n",
       " 0.12080287154684394,\n",
       " 0.12142920472483953,\n",
       " 0.12071533732163676,\n",
       " 0.12133949430966191,\n",
       " 0.12062719701183815,\n",
       " 0.12125004547626066,\n",
       " 0.12053975527835566,\n",
       " 0.1211619320971544,\n",
       " 0.12045488216485918,\n",
       " 0.12107462568930887,\n",
       " 0.12037084679720265,\n",
       " 0.12098655205764121,\n",
       " 0.1202856703522763,\n",
       " 0.12089816630758081,\n",
       " 0.12019905763259368,\n",
       " 0.12080801315170414,\n",
       " 0.12011051011306154,\n",
       " 0.1207160987404063,\n",
       " 0.12002031655134483,\n",
       " 0.12062438073745235,\n",
       " 0.11993384141246759,\n",
       " 0.12053615397127784,\n",
       " 0.11985011634783208,\n",
       " 0.12044920487122826,\n",
       " 0.11976557374161365,\n",
       " 0.12036169856533611,\n",
       " 0.1196814542222789,\n",
       " 0.12027355646774633,\n",
       " 0.11959566693483023,\n",
       " 0.12018510574312236,\n",
       " 0.11951240491353572,\n",
       " 0.12010023350100472,\n",
       " 0.11943062777611473,\n",
       " 0.12001563104475602,\n",
       " 0.11934956012958527,\n",
       " 0.11993177452537622,\n",
       " 0.11926829090143348,\n",
       " 0.11984746136679589,\n",
       " 0.11918715901458561,\n",
       " 0.11976324328492566,\n",
       " 0.11910504398839984,\n",
       " 0.11967928897850756,\n",
       " 0.11902625025441654,\n",
       " 0.1195959378518429,\n",
       " 0.11894308935825328,\n",
       " 0.11950957334690597,\n",
       " 0.11885920044301043,\n",
       " 0.11942361556620817,\n",
       " 0.11877530424081335,\n",
       " 0.11933699306941821,\n",
       " 0.1186929703861946,\n",
       " 0.11925339124877059,\n",
       " 0.11861398094026876,\n",
       " 0.11917045362182417,\n",
       " 0.11853527331612665,\n",
       " 0.11908851484293324,\n",
       " 0.11845679553450895,\n",
       " 0.11900595743782838,\n",
       " 0.11837740582667496,\n",
       " 0.1189236740526237,\n",
       " 0.11829852825430709,\n",
       " 0.11884083668136002,\n",
       " 0.11821906729549946,\n",
       " 0.11875791264845934,\n",
       " 0.11814007453673903,\n",
       " 0.11867574993162693,\n",
       " 0.11806204016710743,\n",
       " 0.11859333950557356,\n",
       " 0.11798193686301858,\n",
       " 0.11851006761711091,\n",
       " 0.11790504238978361,\n",
       " 0.11842870877073129,\n",
       " 0.11782800516785179,\n",
       " 0.11835005580039702,\n",
       " 0.11775376237257207,\n",
       " 0.11827094812109566,\n",
       " 0.11767803819588679,\n",
       " 0.11819338741792405,\n",
       " 0.11760472020388865,\n",
       " 0.11811819320552788,\n",
       " 0.11753071057556554,\n",
       " 0.11804200289019041,\n",
       " 0.11745677535470185,\n",
       " 0.11796431485958463,\n",
       " 0.11738343709729047,\n",
       " 0.11788655926927567,\n",
       " 0.1173093735067338,\n",
       " 0.11780888262520389,\n",
       " 0.11723542266251284,\n",
       " 0.1177318743550053,\n",
       " 0.1171649766952185,\n",
       " 0.11765503969020069,\n",
       " 0.11708935643508372,\n",
       " 0.11757842348434673,\n",
       " 0.1170198915616053,\n",
       " 0.11750259404401625,\n",
       " 0.1169446573466466,\n",
       " 0.11742413132577434,\n",
       " 0.11687257186291695,\n",
       " 0.11734849961574774,\n",
       " 0.11680023226721893,\n",
       " 0.11727279094618588,\n",
       " 0.11673064007378857,\n",
       " 0.1171961444155275,\n",
       " 0.11665331950904527,\n",
       " 0.11712081113910702,\n",
       " 0.11658831366510218,\n",
       " 0.11704698424481716,\n",
       " 0.11651462388798206,\n",
       " 0.11697138755442664,\n",
       " 0.11644328702209476,\n",
       " 0.11689675277625923,\n",
       " 0.11637444747949995,\n",
       " 0.1168227464329884,\n",
       " 0.11630229208224366,\n",
       " 0.11674849261438855,\n",
       " 0.11623345083750197,\n",
       " 0.11667575622119504,\n",
       " 0.11616421125091071,\n",
       " 0.11660216152916254,\n",
       " 0.11609425466338333,\n",
       " 0.11652672907557989,\n",
       " 0.11602230650193478,\n",
       " 0.11645008595102486,\n",
       " 0.11595141615833243,\n",
       " 0.11637350714689686,\n",
       " 0.11588019300861102,\n",
       " 0.11629952684322545,\n",
       " 0.11581170455350456,\n",
       " 0.11622711045495278,\n",
       " 0.11574478022558146,\n",
       " 0.11615680242180718,\n",
       " 0.11567865346392861,\n",
       " 0.11608527301429494,\n",
       " 0.11560974325981707,\n",
       " 0.11601204647202942,\n",
       " 0.11553942427100432,\n",
       " 0.11593829217599601,\n",
       " 0.11547438213959664,\n",
       " 0.11586582952721483,\n",
       " 0.115400130120432,\n",
       " 0.11579261326549235,\n",
       " 0.11533814124895173,\n",
       " 0.11572081448513903,\n",
       " 0.11526601962226751,\n",
       " 0.1156477181361929,\n",
       " 0.11519946231121266,\n",
       " 0.11557666304524175,\n",
       " 0.11513064030842997,\n",
       " 0.11550499421536409,\n",
       " 0.11506296885272141,\n",
       " 0.1154338509005778,\n",
       " 0.11499806604974845,\n",
       " 0.11536536259687986,\n",
       " 0.11493344308235984,\n",
       " 0.11529597737314762,\n",
       " 0.11486933283755812,\n",
       " 0.11522577486950412,\n",
       " 0.11480231389168112,\n",
       " 0.11515626220309258,\n",
       " 0.11473731574457598,\n",
       " 0.11508640100148737,\n",
       " 0.11467208888447725,\n",
       " 0.11501799961609084,\n",
       " 0.11461082469369491,\n",
       " 0.11495227549486403,\n",
       " 0.11454766790387451,\n",
       " 0.114886035725037]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "14eafde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5174\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca59cb4",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "### Part 5 -  Loading Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2b19461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cfcacf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8bf5c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "58f6e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c94784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[1]\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d4d38a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(N_train, M*M, 1).squeeze()\n",
    "X_test = X_test.reshape(N_test, M*M, 1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3363904",
   "metadata": {},
   "source": [
    "# Fashion MNIST with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9846a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ba333f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1e4d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "20578ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 255\n",
    "X_train = X_train/ MAX\n",
    "X_test =X_test/ MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e552d3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1168acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "                delta_stop = 1e-7,\n",
    "                patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5a65f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.058933279699759554\n",
      "Loss after 7epoch 201 : 0.053734693616789286\n"
     ]
    }
   ],
   "source": [
    "c = nn.run(X_train, y_train_cat, epochs=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d7256149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.799\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed9118",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "02038fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_SGD = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='SGD',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6c2a83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.05166687255871911\n",
      "Loss after epoch 200 : 0.0519095842593376\n",
      "Loss after 2epoch 301 : 0.05161651827328838\n"
     ]
    }
   ],
   "source": [
    "c = nn_SGD.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d150eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8019\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af4b28",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ecd257f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_adam = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='adam',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6431ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 3.2217382596449964\n",
      "Loss after epoch 1 : 2.016094800870134\n",
      "Loss after epoch 2 : 0.8334400666002773\n",
      "Loss after epoch 3 : 0.6489400468550212\n",
      "Loss after epoch 4 : 0.4847626263412157\n",
      "Loss after epoch 5 : 0.31312675858433453\n",
      "Loss after epoch 6 : 0.22652070062418284\n",
      "Loss after epoch 7 : 0.2310943814192747\n",
      "Loss after epoch 8 : 0.23151645106743207\n",
      "Loss after epoch 9 : 0.23187100720981477\n",
      "Loss after epoch 10 : 0.23219146863839502\n",
      "Loss after epoch 11 : 0.23246233230605784\n",
      "Loss after epoch 12 : 0.2326751100799611\n",
      "Loss after epoch 13 : 0.2328266577928193\n",
      "Loss after epoch 14 : 0.2329174611060478\n",
      "Loss after epoch 15 : 0.23294982194050157\n",
      "Loss after epoch 16 : 0.23292833213103503\n",
      "Loss after epoch 17 : 0.23285896208261053\n",
      "Loss after epoch 18 : 0.23274942869402696\n",
      "Loss after epoch 19 : 0.23260678766768073\n",
      "Loss after epoch 20 : 0.23243856229710835\n",
      "Loss after epoch 21 : 0.2322525300155777\n",
      "Loss after epoch 22 : 0.2320554668081128\n",
      "Loss after epoch 23 : 0.23185295750386228\n",
      "Loss after epoch 24 : 0.23165034246417127\n",
      "Loss after epoch 25 : 0.23145225086763924\n",
      "Loss after epoch 26 : 0.23126215987818433\n",
      "Loss after epoch 27 : 0.23108280841129802\n",
      "Loss after epoch 28 : 0.2309166379389709\n",
      "Loss after epoch 29 : 0.23076508103070184\n",
      "Loss after epoch 30 : 0.23062910536399042\n",
      "Loss after epoch 31 : 0.23050896398549225\n",
      "Loss after epoch 32 : 0.2304045863565013\n",
      "Loss after epoch 33 : 0.2303153926077346\n",
      "Loss after epoch 34 : 0.23024040620412645\n",
      "Loss after epoch 35 : 0.23017829354322836\n",
      "Loss after epoch 36 : 0.23012746406373602\n",
      "Loss after epoch 37 : 0.23008619340807945\n",
      "Loss after epoch 38 : 0.23005272090780646\n",
      "Loss after epoch 39 : 0.23002514721848502\n",
      "Loss after epoch 40 : 0.23000159774304843\n",
      "Loss after epoch 41 : 0.22997991768256393\n",
      "Loss after epoch 42 : 0.22995883847919463\n",
      "Loss after epoch 43 : 0.22993690934908897\n",
      "Loss after epoch 44 : 0.22991208613818595\n",
      "Loss after epoch 45 : 0.22988486885585566\n",
      "Loss after epoch 46 : 0.2298529014623162\n",
      "Loss after epoch 47 : 0.22981639417944777\n",
      "Loss after epoch 48 : 0.22977566148588952\n",
      "Loss after epoch 49 : 0.22972936658032467\n",
      "Loss after epoch 50 : 0.22967721212265543\n",
      "Loss after epoch 51 : 0.22961852463669896\n",
      "Loss after epoch 52 : 0.22955334993313645\n",
      "Loss after epoch 53 : 0.22948332622703033\n",
      "Loss after epoch 54 : 0.22940350751525068\n",
      "Loss after epoch 55 : 0.22931447138310382\n",
      "Loss after epoch 56 : 0.2292175422508031\n",
      "Loss after epoch 57 : 0.22911189648275754\n",
      "Loss after epoch 58 : 0.2289967732432434\n",
      "Loss after epoch 59 : 0.22886718287574664\n",
      "Loss after epoch 60 : 0.22871900713861526\n",
      "Loss after epoch 61 : 0.2285529754428049\n",
      "Loss after epoch 62 : 0.22836795901986262\n",
      "Loss after epoch 63 : 0.22816074890320442\n",
      "Loss after epoch 64 : 0.22792521168967228\n",
      "Loss after epoch 65 : 0.22766449471434957\n",
      "Loss after epoch 66 : 0.22738326878843304\n",
      "Loss after epoch 67 : 0.22707269845523317\n",
      "Loss after epoch 68 : 0.22673620362122834\n",
      "Loss after epoch 69 : 0.22638009843697626\n",
      "Loss after epoch 70 : 0.22600832907576915\n",
      "Loss after epoch 71 : 0.2256329603811221\n",
      "Loss after epoch 72 : 0.22527297637757165\n",
      "Loss after epoch 73 : 0.22494235594649997\n",
      "Loss after epoch 74 : 0.2246342008747975\n",
      "Loss after epoch 75 : 0.22435387753986677\n",
      "Loss after epoch 76 : 0.2241105432171535\n",
      "Loss after epoch 77 : 0.22390476783233215\n",
      "Loss after epoch 78 : 0.22373011053701256\n",
      "Loss after epoch 79 : 0.22357839028646986\n",
      "Loss after epoch 80 : 0.22342762363242455\n",
      "Loss after epoch 81 : 0.22327355878986996\n",
      "Loss after epoch 82 : 0.22310384351563087\n",
      "Loss after epoch 83 : 0.22291641979366875\n",
      "Loss after epoch 84 : 0.2227117856442281\n",
      "Loss after epoch 85 : 0.22249601212548903\n",
      "Loss after epoch 86 : 0.22227116396062924\n",
      "Loss after epoch 87 : 0.22204054146156108\n",
      "Loss after epoch 88 : 0.221803691853312\n",
      "Loss after epoch 89 : 0.22156231933366066\n",
      "Loss after epoch 90 : 0.2213137353263531\n",
      "Loss after epoch 91 : 0.22105057075967038\n",
      "Loss after epoch 92 : 0.22076829039801252\n",
      "Loss after epoch 93 : 0.2204606407772421\n",
      "Loss after epoch 94 : 0.22012443992703454\n",
      "Loss after epoch 95 : 0.21976547480000788\n",
      "Loss after epoch 96 : 0.2193963625039915\n",
      "Loss after epoch 97 : 0.21902239816278074\n",
      "Loss after epoch 98 : 0.218641144696122\n",
      "Loss after epoch 99 : 0.218233662863062\n",
      "Loss after epoch 100 : 0.21778437939017115\n",
      "Loss after epoch 100 : 0.21778437939017115\n",
      "Loss after epoch 101 : 0.21729867903940855\n",
      "Loss after epoch 102 : 0.2167805909989908\n",
      "Loss after epoch 103 : 0.2162345662200626\n",
      "Loss after epoch 104 : 0.21567035011195532\n",
      "Loss after epoch 105 : 0.21508463074172646\n",
      "Loss after epoch 106 : 0.21448057880812318\n",
      "Loss after epoch 107 : 0.21386415729563507\n",
      "Loss after epoch 108 : 0.21323848978817708\n",
      "Loss after epoch 109 : 0.21262957267744223\n",
      "Loss after epoch 110 : 0.21204286984464005\n",
      "Loss after epoch 111 : 0.21147774359404325\n",
      "Loss after epoch 112 : 0.21093671926297036\n",
      "Loss after epoch 113 : 0.21041807865602824\n",
      "Loss after epoch 114 : 0.2099069161211319\n",
      "Loss after epoch 115 : 0.209394891072577\n",
      "Loss after epoch 116 : 0.20887389043867838\n",
      "Loss after epoch 117 : 0.2083278599619116\n",
      "Loss after epoch 118 : 0.20775375484993155\n",
      "Loss after epoch 119 : 0.2071667265335978\n",
      "Loss after epoch 120 : 0.20657375457339203\n",
      "Loss after epoch 121 : 0.2059845851051727\n",
      "Loss after epoch 122 : 0.20541273236263619\n",
      "Loss after epoch 123 : 0.20486210695059226\n",
      "Loss after epoch 124 : 0.20433032872001583\n",
      "Loss after epoch 125 : 0.20380829408898984\n",
      "Loss after epoch 126 : 0.2032791015955871\n",
      "Loss after epoch 127 : 0.20274591465983158\n",
      "Loss after epoch 128 : 0.20221610740922444\n",
      "Loss after epoch 129 : 0.20169388115676018\n",
      "Loss after epoch 130 : 0.20118655683178177\n",
      "Loss after epoch 131 : 0.20070042628839074\n",
      "Loss after epoch 132 : 0.2002353940078233\n",
      "Loss after epoch 133 : 0.19978660706955095\n",
      "Loss after epoch 134 : 0.19935445847618213\n",
      "Loss after epoch 135 : 0.19893376200882418\n",
      "Loss after epoch 136 : 0.19852388620723993\n",
      "Loss after epoch 137 : 0.19812779274806852\n",
      "Loss after epoch 138 : 0.1977402896507923\n",
      "Loss after epoch 139 : 0.19735729794594617\n",
      "Loss after epoch 140 : 0.196982124485526\n",
      "Loss after epoch 141 : 0.1966161552985515\n",
      "Loss after epoch 142 : 0.19626079701316737\n",
      "Loss after epoch 143 : 0.19591316434239786\n",
      "Loss after epoch 144 : 0.19557235966160863\n",
      "Loss after epoch 145 : 0.19523585465269497\n",
      "Loss after epoch 146 : 0.1949007533207395\n",
      "Loss after epoch 147 : 0.19456531254398185\n",
      "Loss after epoch 148 : 0.19422738923638055\n",
      "Loss after epoch 149 : 0.1938888671336519\n",
      "Loss after epoch 150 : 0.19355035380672897\n",
      "Loss after epoch 151 : 0.19321092367274448\n",
      "Loss after epoch 152 : 0.1928695473104192\n",
      "Loss after epoch 153 : 0.1925248011112687\n",
      "Loss after epoch 154 : 0.19217604798718543\n",
      "Loss after epoch 155 : 0.19182128279213365\n",
      "Loss after epoch 156 : 0.1914598920137333\n",
      "Loss after epoch 157 : 0.1910878234859918\n",
      "Loss after epoch 158 : 0.19070600366572085\n",
      "Loss after epoch 159 : 0.19031606733778672\n",
      "Loss after epoch 160 : 0.18991415458421398\n",
      "Loss after epoch 161 : 0.18949958113367668\n",
      "Loss after epoch 162 : 0.1890709819680194\n",
      "Loss after epoch 163 : 0.18862346479350536\n",
      "Loss after epoch 164 : 0.18815935178320808\n",
      "Loss after epoch 165 : 0.18767945461246704\n",
      "Loss after epoch 166 : 0.18718244443188986\n",
      "Loss after epoch 167 : 0.18666435173298904\n",
      "Loss after epoch 168 : 0.18612384030003606\n",
      "Loss after epoch 169 : 0.18556319746705463\n",
      "Loss after epoch 170 : 0.18498326643835616\n",
      "Loss after epoch 171 : 0.1843885934281755\n",
      "Loss after epoch 172 : 0.18378405500815026\n",
      "Loss after epoch 173 : 0.183170436223102\n",
      "Loss after epoch 174 : 0.18255202706450227\n",
      "Loss after epoch 175 : 0.18193876357821478\n",
      "Loss after epoch 176 : 0.1813379348775035\n",
      "Loss after epoch 177 : 0.18075224441323096\n",
      "Loss after epoch 178 : 0.1801879219549508\n",
      "Loss after epoch 179 : 0.17964395554295115\n",
      "Loss after epoch 180 : 0.1791204649472794\n",
      "Loss after epoch 181 : 0.17861028937722356\n",
      "Loss after epoch 182 : 0.17811016641831415\n",
      "Loss after epoch 183 : 0.17761497151106734\n",
      "Loss after epoch 184 : 0.17712626723568484\n",
      "Loss after epoch 185 : 0.17664769499207914\n",
      "Loss after epoch 186 : 0.17618048603932274\n",
      "Loss after epoch 187 : 0.1757403744584739\n",
      "Loss after epoch 188 : 0.17532776761908692\n",
      "Loss after epoch 189 : 0.1749407758724522\n",
      "Loss after epoch 190 : 0.17457494969725137\n",
      "Loss after epoch 191 : 0.17422339234135586\n",
      "Loss after epoch 192 : 0.17387935133750118\n",
      "Loss after epoch 193 : 0.17354174041654352\n",
      "Loss after epoch 194 : 0.17321778975955227\n",
      "Loss after epoch 195 : 0.17291062681235397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 196 : 0.17261708488557384\n",
      "Loss after epoch 197 : 0.1723337713577593\n",
      "Loss after epoch 198 : 0.1720582110326311\n",
      "Loss after epoch 199 : 0.1717935099727183\n",
      "Loss after epoch 200 : 0.1715395109307802\n",
      "Loss after epoch 200 : 0.1715395109307802\n",
      "Loss after epoch 201 : 0.17130442638492654\n",
      "Loss after epoch 202 : 0.17108599151222426\n",
      "Loss after epoch 203 : 0.17088283613209343\n",
      "Loss after epoch 204 : 0.17068983835851842\n",
      "Loss after epoch 205 : 0.17050161868140704\n",
      "Loss after epoch 206 : 0.1703184498648945\n",
      "Loss after epoch 207 : 0.17014119009398904\n",
      "Loss after epoch 208 : 0.16997239700996924\n",
      "Loss after epoch 209 : 0.16981278082002035\n",
      "Loss after epoch 210 : 0.16966045691870263\n",
      "Loss after epoch 211 : 0.16951430112721788\n",
      "Loss after epoch 212 : 0.169372385279318\n",
      "Loss after epoch 213 : 0.1692343912017149\n",
      "Loss after epoch 214 : 0.1691008165677335\n",
      "Loss after epoch 215 : 0.1689711313563907\n",
      "Loss after epoch 216 : 0.16884537502049257\n",
      "Loss after epoch 217 : 0.16872368416678404\n",
      "Loss after epoch 218 : 0.1686052963073704\n",
      "Loss after epoch 219 : 0.1684901230245405\n",
      "Loss after epoch 220 : 0.168378022822343\n",
      "Loss after epoch 221 : 0.16826928303467856\n",
      "Loss after epoch 222 : 0.16816338105196343\n",
      "Loss after epoch 223 : 0.16806026867054724\n",
      "Loss after epoch 224 : 0.16795936199104766\n",
      "Loss after epoch 225 : 0.16786071285209644\n",
      "Loss after epoch 226 : 0.16776449721639805\n",
      "Loss after epoch 227 : 0.16767101288720498\n",
      "Loss after epoch 228 : 0.16757962669947285\n",
      "Loss after epoch 229 : 0.16749044752941233\n",
      "Loss after epoch 230 : 0.16740346712764298\n",
      "Loss after epoch 231 : 0.16731858447950815\n",
      "Loss after epoch 232 : 0.16723556474219775\n",
      "Loss after epoch 233 : 0.16715429822766253\n",
      "Loss after epoch 234 : 0.16707457558242106\n",
      "Loss after epoch 235 : 0.1669965544498841\n",
      "Loss after epoch 236 : 0.16692004363509533\n",
      "Loss after epoch 237 : 0.16684539506220047\n",
      "Loss after epoch 238 : 0.1667723511491689\n",
      "Loss after epoch 239 : 0.1667010354142232\n",
      "Loss after epoch 240 : 0.16663128438392752\n",
      "Loss after epoch 241 : 0.16656271656703034\n",
      "Loss after epoch 242 : 0.16649547103812384\n",
      "Loss after epoch 243 : 0.16642936296309593\n",
      "Loss after epoch 244 : 0.1663644436497526\n",
      "Loss after epoch 245 : 0.1663002344269336\n",
      "Loss after epoch 246 : 0.16623719048310068\n",
      "Loss after epoch 247 : 0.16617529022603916\n",
      "Loss after epoch 248 : 0.1661142432290841\n",
      "Loss after epoch 249 : 0.1660542486042274\n",
      "Loss after epoch 250 : 0.1659953318823464\n",
      "Loss after epoch 251 : 0.16593733995691623\n",
      "Loss after epoch 252 : 0.16588024495453982\n",
      "Loss after epoch 253 : 0.16582390728408028\n",
      "Loss after epoch 254 : 0.16576821388167576\n",
      "Loss after epoch 255 : 0.16571329940261376\n",
      "Loss after epoch 256 : 0.16565906385465815\n",
      "Loss after epoch 257 : 0.16560552582254873\n",
      "Loss after epoch 258 : 0.16555275473239128\n",
      "Loss after epoch 259 : 0.1655006751952482\n",
      "Loss after epoch 260 : 0.16544927919440208\n",
      "Loss after epoch 261 : 0.16539836335724087\n",
      "Loss after epoch 262 : 0.16534808053182493\n",
      "Loss after epoch 263 : 0.16529839318301262\n",
      "Loss after epoch 264 : 0.16524926530742967\n",
      "Loss after epoch 265 : 0.16520057524185772\n",
      "Loss after epoch 266 : 0.16515230706313425\n",
      "Loss after epoch 267 : 0.1651044879928213\n",
      "Loss after epoch 268 : 0.16505726965393963\n",
      "Loss after epoch 269 : 0.16501052809890804\n",
      "Loss after epoch 270 : 0.164964163905835\n",
      "Loss after epoch 271 : 0.16491826059774983\n",
      "Loss after epoch 272 : 0.16487272488185645\n",
      "Loss after epoch 273 : 0.16482757786198524\n",
      "Loss after epoch 274 : 0.16478294873136926\n",
      "Loss after epoch 275 : 0.16473877943900153\n",
      "Loss after epoch 276 : 0.16469502850060846\n",
      "Loss after epoch 277 : 0.1646517135643468\n",
      "Loss after epoch 278 : 0.16460886517363046\n",
      "Loss after epoch 279 : 0.16456643026951842\n",
      "Loss after epoch 280 : 0.16452439554611697\n",
      "Loss after epoch 281 : 0.16448279635198243\n",
      "Loss after epoch 282 : 0.16444161019255651\n",
      "Loss after epoch 283 : 0.1644006604599751\n",
      "Loss after epoch 284 : 0.16436001118210503\n",
      "Loss after epoch 285 : 0.16431966257236072\n",
      "Loss after epoch 286 : 0.16427956196790097\n",
      "Loss after epoch 287 : 0.16423979024094204\n",
      "Loss after epoch 288 : 0.16420038481287225\n",
      "Loss after epoch 289 : 0.164161214948568\n",
      "Loss after epoch 290 : 0.16412218540174275\n",
      "Loss after epoch 291 : 0.1640834820148133\n",
      "Loss after epoch 292 : 0.1640451015130979\n",
      "Loss after epoch 293 : 0.1640069426277012\n",
      "Loss after epoch 294 : 0.16396893885926897\n",
      "Loss after epoch 295 : 0.163931241540779\n",
      "Loss after epoch 296 : 0.1638938133577861\n",
      "Loss after epoch 297 : 0.16385668975250683\n",
      "Loss after epoch 298 : 0.16381983463171199\n",
      "Loss after epoch 299 : 0.16378320711024758\n",
      "Loss after 1epoch 301 : 0.16378320711024758\n"
     ]
    }
   ],
   "source": [
    "c = nn_adam.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4400e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.799\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf6df1",
   "metadata": {},
   "source": [
    "# 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "14c1d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "               hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8dc1dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 100 : 0.23209225137947403\n",
      "Loss after epoch 200 : 0.22986403893323532\n",
      "Loss after epoch 300 : 0.22702817902183567\n",
      "Loss after epoch 400 : 0.2224261825710151\n",
      "Loss after epoch 500 : 0.21352530665882163\n",
      "Loss after epoch 600 : 0.2067784009860351\n",
      "Loss after epoch 700 : 0.2024421780077031\n",
      "Loss after epoch 800 : 0.19911217753688473\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/170441750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/3482327002.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X_train, target, epochs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/3482327002.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, X, target)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/3482327002.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m      \u001b[0;31m# Hidden1 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_hidden_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Hidden1 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m      \u001b[0;31m# Hidden2 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.run(X_train, y_train_cat, epochs=10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "221879da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7163\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c34da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "# W1 of size hidden x features\n",
    "n = D * M\n",
    "W1 = tn.rvs(n).reshape((M, D )) # hidden x features\n",
    "# W2 of size output x hidden\n",
    "m = M  * K\n",
    "W2 = tn.rvs(m).reshape((K, M)) # output x hidden\n",
    "b1 = tn.rvs(M).reshape(-1,1) \n",
    "b2 = tn.rvs(K).reshape(-1,1) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "263881e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "Z1 = W1.dot(X.T) + b1 # Hidden x N_samples\n",
    "A1 = tanh(Z1)      # Hidden x N_samples\n",
    "Z2 = W2.dot(A1) + b2  # Output x N_samples\n",
    "A2 = softmax(Z2)      #Output x N_samples\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "84bc22e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81765d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "234067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "# deltas\n",
    "dZ2 = A2 - y_train_cat                                   #Output x N_samples\n",
    "dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "dZ1 = W2.T.dot(dZ2)*dt(Z1)     # Hidden x N_samples\n",
    "dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0bf12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "lr = 0.01\n",
    "W2 -= lr*dW2\n",
    "b2 -= lr*db2\n",
    "W1 -= lr*dW1\n",
    "b1 -= lr*db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5af82b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cross_entropy(y_train_cat, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f964925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23973236540442144"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e387ab",
   "metadata": {},
   "source": [
    "# Testing Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63297da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "t = to_categorical(target)\n",
    "\n",
    "M = 5\n",
    "D = data.shape[1]\n",
    "K = len(set(target))\n",
    "beta1 = 0.9   \n",
    "beta2 = 0.999\n",
    "X_train, X_test, y_train, y_test = train_test_split(data ,target ,test_size=0.25)\n",
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ba98e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vdw1 = np.zeros((M, D ))\n",
    "Vdw2 = np.zeros((K, M))\n",
    "Vdb1 = np.zeros((M, 1 ))\n",
    "Vdb2 = np.zeros((K, 1 ))\n",
    "Sdw1 = np.zeros((M, D))\n",
    "Sdw2 = np.zeros((K, M))\n",
    "Sdb1 = np.zeros((M, 1 ))\n",
    "Sdb2 = np.zeros((K, 1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9147ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    Z1 = W1.dot(X.T) + b1 # Hidden x N_samples\n",
    "    A1 = tanh(Z1)      # Hidden x N_samples\n",
    "    Z2 = W2.dot(A1) + b2  # Output x N_samples\n",
    "    A2 = softmax(Z2)      #Output x N_samples\n",
    "    return A2, Z2, A1, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37beb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropADAM(X, target):\n",
    "    # Forward prop\n",
    "    A2, Z2, A1, Z1 = forward(X)\n",
    "    # Compute cost\n",
    "    cost = cross_entropy(target, A2)\n",
    "    # N samples\n",
    "    m = X.shape[0]\n",
    "    # deltas\n",
    "    dZ2 = A2 - target                                       #Output x N_samples\n",
    "    dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "    dZ1 = W2.T.dot(dZ2)*(1-tanh(Z1)**2)     # Hidden x N_samples\n",
    "    dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "    # Adam updates\n",
    "    beta1 = beta1\n",
    "    beta2 = beta2\n",
    "    # V\n",
    "    Vdw1 = beta1*Vdw1 + (1-beta1)*dW1\n",
    "    Vdw2 = beta1*Vdw2 + (1-beta1)*dW2\n",
    "    Vdb1 = beta1*Vdb1 + (1-beta1)*db1\n",
    "    Vdb2 = beta1*Vdb2 + (1-beta1)*db2\n",
    "    # S\n",
    "    Sdw1 = beta2*Sdw1 + (1-beta2)*dW1**2\n",
    "    Sdw2 = beta2*Sdw2 + (1-beta2)*dW2**2\n",
    "    Sdb1 = beta2*Sdb1 + (1-beta2)*db1**2\n",
    "    Sdb2 = beta2*Sdb2 + (1-beta2)*db2**2    \n",
    "    # Update\n",
    "    lr = learning_rate\n",
    "    W2 -= lr * Vdw2 / (np.sqrt(Sdw2)+1e-8)\n",
    "    b2 -= lr * Vdb2 / (np.sqrt(Sdb2)+1e-8)\n",
    "    W1 -= lr * Vdw1 / (np.sqrt(Sdw1)+1e-8)\n",
    "    b1 -= lr * Vdb1 / (np.sqrt(Sdb1)+1e-8)\n",
    "    return cost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7e339f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,5) (112,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/1684670042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# deltas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m                                       \u001b[0;31m#Output x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdZ2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m                                   \u001b[0;31m#Output x hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m              \u001b[0;31m#Output x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Hidden x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,5) (112,4) "
     ]
    }
   ],
   "source": [
    "tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) \n",
    "# W1 of size hidden x features\n",
    "n = D * M\n",
    "global W1 \n",
    "W1 = tn.rvs(n).reshape((M, D )) # hidden x features\n",
    "# W2 of size output x hidden\n",
    "m = M  * K\n",
    "global W2\n",
    "W2 = tn.rvs(m).reshape((K, M)) # output x hidden\n",
    "global b1\n",
    "b1 = tn.rvs(M).reshape(-1,1) \n",
    "global b2 \n",
    "b2 = tn.rvs(K).reshape(-1,1) \n",
    "\n",
    "epochs = 20\n",
    "for i in range(epochs):\n",
    "    # Bias correction for V\n",
    "    Vdw1 = Vdw1 / (1-beta1**i)\n",
    "    Vdw2 = Vdw2 / (1-beta1**i)\n",
    "    Vdb1 = Vdb1 / (1-beta1**i)\n",
    "    Vdb2 = Vdb2 / (1-beta1**i)\n",
    "    # S\n",
    "    Sdw1 = Sdw1 / (1-beta2**i)\n",
    "    Sdw2 = Sdw2 / (1-beta2**i)\n",
    "    Sdb1 = Sdb1 / (1-beta2**i)\n",
    "    Sdb2 = Sdb2 / (1-beta2**i)\n",
    "    \n",
    "    \n",
    "    # Forward prop\n",
    "    A2, Z2, A1, Z1 = forward(X_train)\n",
    "    # Compute cost\n",
    "    cost = cross_entropy(y_train, A2)\n",
    "    # N samples\n",
    "    m = X_train\n",
    "    # deltas\n",
    "    dZ2 = A2 - y_train                                       #Output x N_samples\n",
    "    dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "    dZ1 = W2.T.dot(dZ2)*(1-tanh(Z1)**2)     # Hidden x N_samples\n",
    "    dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "   \n",
    "    # V\n",
    "    Vdw1 = beta1*Vdw1 + (1-beta1)*dW1\n",
    "    Vdw2 = beta1*Vdw2 + (1-beta1)*dW2\n",
    "    Vdb1 = beta1*Vdb1 + (1-beta1)*db1\n",
    "    Vdb2 = beta1*Vdb2 + (1-beta1)*db2\n",
    "    # S\n",
    "    Sdw1 = beta2*Sdw1 + (1-beta2)*dW1**2\n",
    "    Sdw2 = beta2*Sdw2 + (1-beta2)*dW2**2\n",
    "    Sdb1 = beta2*Sdb1 + (1-beta2)*db1**2\n",
    "    Sdb2 = beta2*Sdb2 + (1-beta2)*db2**2    \n",
    "    # Update\n",
    "    lr = learning_rate\n",
    "    W2 -= lr * Vdw2 / (np.sqrt(Sdw2)+1e-8)\n",
    "    b2 -= lr * Vdb2 / (np.sqrt(Sdb2)+1e-8)\n",
    "    W1 -= lr * Vdw1 / (np.sqrt(Sdw1)+1e-8)\n",
    "    b1 -= lr * Vdb1 / (np.sqrt(Sdb1)+1e-8)\n",
    "    \n",
    "    \n",
    "    print(f'Loss after epoch {i} : {cost}')\n",
    "    costs.append(cost)\n",
    "    if i%100 == 0 and i>0 :\n",
    "        print(f'Loss after epoch {i} : {cost}')\n",
    "    print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "    costs.pop(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a19ae74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2, Z2, A1, Z1 = forward(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cross_entropy(y_train_cat, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd520377",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/2385575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeta1\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "beta1**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdbbd2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.72788095e-01,  4.10790325e-01,  4.57705823e-01,\n",
       "         1.16515290e-01,  1.55025966e-01],\n",
       "       [ 2.73742100e-01, -1.42290534e-01,  4.54997999e-01,\n",
       "         3.48680215e-01,  4.83674311e-01],\n",
       "       [ 7.20732218e-02,  4.60749837e-01,  9.71750370e-03,\n",
       "         4.71877878e-01, -2.05773248e-04]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da705788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
