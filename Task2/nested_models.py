import numpy as npimport mathfrom abc import ABC, abstractmethodfrom numpy.random import default_rngimport matplotlib.pyplot as pltfrom sklearn.utils import shufflefrom sklearn.linear_model import LinearRegressionimport pandas as pdrng = default_rng(seed=0)# LINEAR REGRESSION FUNCTIONdef regression(Xtrain, Xtest, ytrain, ytest) :    reg = LinearRegression().fit(Xtrain, ytrain)        return reg.coef_, reg.intercept_, reg.score(Xtest, ytest)# DATA## We use auto-mpg dataset to work with### Loading datasetmpg = pd.read_csv('./auto-mpg.csv', na_values=['NA','?'])### Pre Processing data, filling na valuescars = mpg['name']mpg.drop('name',1,inplace=True)med = mpg['horsepower'].median()mpg['horsepower'] = mpg['horsepower'].fillna(med) ### Selecting covariates columnscovariates = []for x in mpg.columns:    if x != 'mpg':        covariates.append(x)print(covariates)### Getting data for regression X = mpg[covariates].valuesy = mpg['mpg'].values###split data into testing and trainingfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)# COMPARING MODELS USING MODEL RESULTSfrom sklearn import metrics# build the modelmodel = LinearRegression()  model.fit(X_train, y_train)#calculate the predictions of the linear regression modely_pred = model.predict(X_test)# Mean and RMSEprint('Covariates Used :', covariates)print('Mean:', np.mean(y_test))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))print('R2 Score : ', model.score(X_test, y_test))''' Each time we drop a covariate and look at the regression stats'''n = len(covariates)for i in range(n):        # Dropping covariates[i]    new_covariates = [ covariates[j] for j in range(n) if j!=i ]        # New covariates Input    new_X = np.delete(X, i, 1)    new_X_train = np.delete(X_train, i, 1)    new_X_test = np.delete(X_test, i, 1)        #build model    model = LinearRegression()    model.fit(new_X_train, y_train)        #calculate the predictions of the linear regression model    new_y_pred = model.predict(new_X_test)    # Mean and RMSE    print('Covariates Dropped :', covariates[i])    print('Mean:', np.mean(y_test))    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, new_y_pred)))    print('R2 Score : ', model.score(new_X_test, y_test))    print('---------------------------------------------')        ### Using only weight covariate# New covariates Inputnew_X = X[:,3]new_X_train = X_train[:,3]new_X_test = X_test[:,3]#build modelmodel = LinearRegression()model.fit(new_X_train.reshape(-1,1), y_train)#calculate the predictions of the linear regression modelnew_y_pred = model.predict(new_X_test.reshape(-1,1))print('Covariates Used :', covariates[3])print('Mean:', np.mean(y_test))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, new_y_pred)))print('R2 Score : ', model.score(new_X_test.reshape(-1,1), y_test))### Using only year covariate# New covariates Inputnew_X = X[:,5]new_X_train = X_train[:,5]new_X_test = X_test[:,5]#build modelmodel = LinearRegression()model.fit(new_X_train.reshape(-1,1), y_train)#calculate the predictions of the linear regression modelnew_y_pred = model.predict(new_X_test.reshape(-1,1))print('Covariates Used :', covariates[5])print('Mean:', np.mean(y_test))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, new_y_pred)))print('R2 Score : ', model.score(new_X_test.reshape(-1,1), y_test))### Using only weight and year covariates# New covariates Inputnew_X = X[:,[3,5]]new_X_train = X_train[:,[3,5]]new_X_test = X_test[:,[3,5]]#build modelmodel = LinearRegression()model.fit(new_X_train, y_train)#calculate the predictions of the linear regression modelnew_y_pred = model.predict(new_X_test)print('Covariates Used :', [covariates[i] for i in [3,5]])print('Mean:', np.mean(y_test))print('Root Mean Squared Error Full Model: 3.409213715841733')print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, new_y_pred)))print('R2 Score Full Model : 0.8139447246621095 ')print('R2 Score : ', model.score(new_X_test, y_test))''' ConclusionWe have a better model using only the 'year' and 'weight' covariate. We still need to check that against other training/testing datasets'''# QUANTIFYING THE IMPORTANCE OF A COVARIATE USING T-TEST'''We can use t-test to quantify the importance of a particular covariate in our model.We build on the precedent example we do a t-test on the model by dropping a covariate vs a model with all covariates and see if there is the value is statistically significant.We use a probability threeshold of 95%, and use scipy two sample t-test'''import scipy.stats as statsn = len(covariates)for i in range(n):        # Dropping covariates[i]    new_covariates = [ covariates[j] for j in range(n) if j!=i ]        # New covariates Input    new_X = np.delete(X, i, 1)    new_X_train = np.delete(X_train, i, 1)    new_X_test = np.delete(X_test, i, 1)        #build model    model = LinearRegression()    model.fit(new_X_train, y_train)        #calculate the predictions of the linear regression model    new_y_pred = model.predict(new_X_test)        #Making the t-test    tstat, pvalue = stats.ttest_ind(y_pred, new_y_pred, equal_var=False)        # Mean and RMSE    print('Covariates Dropped :', covariates[i])    print('t-test value:', tstat)    print('p-value:', pvalue)    if(pvalue < 0.95) :        print(f'Including {covariates[i]} in the model is important')    else :         print(f' We fail to confirm the importance of including {covariates[i]}')    print('---------------------------------------------')        ''' ConclusionWe find the same conclusions as before'''