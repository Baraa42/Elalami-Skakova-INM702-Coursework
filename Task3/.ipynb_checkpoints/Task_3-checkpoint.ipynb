{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4484a339",
   "metadata": {},
   "source": [
    "Used resources:\n",
    "- Lab 6 and 7\n",
    "- Sharmal, P. (2019) Simple multi layer neural network implementation. <https://intellipaat.com/community/9507/simple-multi-layer-neural-network-implementation>\n",
    "- https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97227f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "from torchvision.datasets import MNIST\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4432939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Sigmoid activation function with forward pass\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "#Sigmoid activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_sigmoid(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "#ReLU activation function with forward pass\n",
    "@np.vectorize\n",
    "def relu (x):\n",
    "  return max(0,x)\n",
    "\n",
    "#ReLU activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_relu (x):\n",
    "  if x<0:\n",
    "    return 0\n",
    "  if x>0:\n",
    "    return 1\n",
    "#loss method - cross entropy\n",
    "def cross_entropy(output, target):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "# output function - softmax\n",
    "def softmax(x):\n",
    "    a = x - np.max(x, axis=0, keepdims=True)\n",
    "    new_a = np.exp(a)\n",
    "    result = new_a / np.new_a(new_a, axis=0, keepdims=True)\n",
    "\n",
    "activation_function = sigmoid\n",
    "activation_derivative = d_sigmoid\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_nodes,\n",
    "                 learning_rate,\n",
    "                 epochs):\n",
    "        self.no_nodes = no_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    # bring as an output weigths list with weight considering input nodes and output nodes    \n",
    "    def create_weight_matrices(self): \n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        weights = []\n",
    "        for i in range(len(self.no_nodes)-1):\n",
    "            rad = 0.5 \n",
    "            X = truncated_normal(mean=1, sd=1, low=-rad, upp=rad)\n",
    "            weight = X.rvs((self.no_nodes[i], self.no_nodes[i+1]))\n",
    "            weights.append(weight)\n",
    "#         print(len(weights))\n",
    "        return weights  \n",
    "    \n",
    "    #bias\n",
    "    def f_bias (self):\n",
    "        biases = []\n",
    "        for i in range(len(self.no_nodes)-1):\n",
    "            rad = 0.5 \n",
    "            tn = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            bias = tn.rvs(self.no_nodes[i]).reshape(-1,1) \n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, X_train):\n",
    "        weights = self.create_weight_matrices()\n",
    "        output_list = []\n",
    "        for i in range(len(weights)):\n",
    "            #input vector\n",
    "            if i == 0:\n",
    "                output_vector = np.dot(weights[i].T, X_train)\n",
    "                output_vector_in = activation_function(output_vector)\n",
    "                output_list.append(output_vector_in)\n",
    "            else:\n",
    "                output_vector = np.dot (weights[i].T, output_list[i-1])\n",
    "                output_vector_out = activation_function(output_vector)\n",
    "                output_list.append(output_vector_out)\n",
    "        return output_vector_out, output_list\n",
    "    \n",
    "\n",
    "    #training with forward pass and backpropagation\n",
    "    def train(self, X_train, y_train):\n",
    "        weights = self.create_weight_matrices()\n",
    "        \n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        X_train_trans = np.array(X_train, ndmin=2).T\n",
    "        y_train_trans = np.array(y_train, ndmin=2).T\n",
    "    \n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            # forward pass\n",
    "            forward = self.forward(X_train_trans)\n",
    "            output = forward[0]            \n",
    "            output_list = forward[1]\n",
    "            #backprop   \n",
    "            for i in reversed(range(len(weights))):\n",
    "                updated_weights = []\n",
    "                if i == (len(weights)-1):\n",
    "                    # derivative of the loss for the output\n",
    "                    output_errors = (y_train - output.T)\n",
    "                    # derivative of the activation function\n",
    "                    derivative_output = activation_derivative (output)           \n",
    "                    tmp = output_errors * derivative_output.T\n",
    "                    # multiply with the previous activation (output_vector_hidden)\n",
    "                    updated_weight = self.learning_rate * np.dot(tmp.T, output_list[-1].T)\n",
    "                    updated_weights.append(updated_weight)\n",
    "                    \n",
    "                if i == 0:\n",
    "                    input_errors = np.dot(weights[i].T, output_errors * derivative_output.T)\n",
    "                    derivative_input = activation_derivative(output_list[0])    \n",
    "                    tmp = input_errors * derivative_input\n",
    "                    updated_weight = self.learning_rate * np.dot(tmp, X_train.T)\n",
    "                    updated_weights.append(updated_weight)\n",
    "                    \n",
    "                else: # hidden layers\n",
    "                    hidden_errors = np.dot (weights[i].T, output_errors * derivative_output)\n",
    "                    derivative_hidden = activation_derivative(output_list[i])    \n",
    "                    tmp = hidden_errors * derivative_hidden.T\n",
    "                    updated_weight = self.learning_rate * np.dot(tmp.T, output_list[i].T)\n",
    "                    updated_weights.append(updated_weight)\n",
    "                    output_errors= hidden_errors\n",
    "                    derivative_output = derivative_hidden\n",
    "                    \n",
    "            # update the weights:\n",
    "            for i in range(len(updated_weights)):\n",
    "                weights [i] += updated_weights[i]\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def run(self, X_train, y_train, X_test):\n",
    "        weights = train(self, X_train, y_train)\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        for i in range(len(weights)):\n",
    "            if i == 0:\n",
    "                input_vector = np.array(X_test, ndmin=2).T\n",
    "                output_vector = np.dot(weights[0], input_vector)\n",
    "                output_vector = activation_function(output_vector)\n",
    "                \n",
    "            if i == (len(weights)-1):\n",
    "                output_vector = np.dot(weights[-1], input_v)\n",
    "                output_vector = activation_function(output_vector)\n",
    "            \n",
    "            else:\n",
    "                output_vector = np.dot(weights[i], input_v)\n",
    "                output_vector = activation_function(output_vector)                \n",
    "            input_v = output_vector\n",
    "            y_hat = output_vector.T\n",
    "        return y_hat\n",
    "    \n",
    "    #accuracy score\n",
    "    def accuracy (y_hat, y_test):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in rnage(len(y_test)):\n",
    "            y_max = y_hat.argmax()\n",
    "            if y_max == y_test:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "                \n",
    "        accuracy = corrects / ( corrects + wrongs)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e400dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "fashion = fashion_mnist.load_data()\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ec7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27344f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean (X_train, axis = (0,1,2))\n",
    "std = np.std (X_train, axis = (0,1,2))\n",
    "\n",
    "X_train = (X_train- mean)/(std+1e-7)\n",
    "X_test = (X_test- mean)/(std+1e-7)\n",
    "\n",
    "#reshaping datset input\n",
    "X_train = X_train.reshape((X_train.shape[0], 28*28))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28*28))\n",
    "\n",
    "#onehot encoding the output\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93f5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork([784, 32, 10], 0.001, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
