{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3309b98",
   "metadata": {},
   "source": [
    "## Content:\n",
    "- [Part 1](#part1)- **Importing the libraries, packages**\n",
    "- [Part 2](#part2)- **Useful Functions**\n",
    "- [Part 3](#part3) -  **One Hidden Layer Class**\n",
    "- [Part 4](#part4) -  **Two Hidden Layers Class** \n",
    "- [Part 5](#part5) -  **Loading Fashion MNIST** \n",
    "- [Part 6](#part6)-  **Fashion MNIST One Hidden Layer**\n",
    "- [Part 6.1](#part6.1) -  Sigmoid and tanh\n",
    "- [Part 6.2](#part6.2) -  ReLU\n",
    "- [Part 6.3](#part6.3) -  Minibatch\n",
    "- [Part 6.4](#part6.4) -  Adam\n",
    "- [Part 6.5](#part6.5) -  Adam minibatch\n",
    "- [Part 7](#part7)-  **Fashion MNIST Two Hidden Layers**\n",
    "- [Part 7.1](#part7.1) -  Sigmoid and tanh\n",
    "- [Part 7.2](#part7.2) -  ReLU\n",
    "- [Part 7.3](#part7.3) -  More nodes added\n",
    "- [Part 7.4](#part7.4) -  Minibatch\n",
    "- [Part 7.5](#part7.5) -  Adam\n",
    "- [Part 7.6](#part7.6) -  Adam minibatch\n",
    "- [Part 8](#part8) - **Best model confusion analysis**\n",
    "- [Part 9](#part9) -  Results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57025661",
   "metadata": {},
   "source": [
    "Weight initialisation :\n",
    "\n",
    "- https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "- https://www.deeplearning.ai/ai-notes/initialization/\n",
    "- https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b8664",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "### Part 1 -   Importing the libraries, packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3f253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import random \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bac6d",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "### Part 2 -   Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0936fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    e = np.exp(X - np.max(X))\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(target, output):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "def cross_entropy_matrix(output, target):\n",
    "    target = np.array(target)\n",
    "    output = np.array(output)\n",
    "    product = target*np.log(output)\n",
    "    errors = -np.sum(product, axis=1)\n",
    "    m = len(errors)\n",
    "    errors = np.sum(errors) / m\n",
    "    return errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ds(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "\n",
    "def dr(x):\n",
    "    dr = (np.sign(x) + 1) / 2\n",
    "    return dr\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-x)\n",
    "    return (a-b)/(a+b)\n",
    "\n",
    "def dt(x):\n",
    "    return 1-tanh(x)**2\n",
    "    \n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid :\n",
    "        return ds\n",
    "    if f == tanh :\n",
    "        return dt\n",
    "    if f == relu :\n",
    "        return dr\n",
    "    return None\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i][y[i]]=1\n",
    "    return ind\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c7823",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "### Part 3 -   One Hidden Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7220f",
   "metadata": {},
   "source": [
    "# One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5c79",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden x N_features\n",
    "- **b1**    : Hidden\n",
    "- **W2**    : Output x Hidden\n",
    "- **b2**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26933734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenOne:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes,\n",
    "                 activation_hidden,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes = hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.hidden_derivative = derivative(self.activation_hidden)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):       \n",
    "        if self.activation_hidden == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes/2 )  # output x hidden\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes )  # output x hidden\n",
    "                  \n",
    "    \n",
    "    def create_biases(self):    \n",
    "        #tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        #self.b1 = tn.rvs(self.hidden_nodes).reshape(-1,1) \n",
    "        #self.b2 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "        self.b1 =  np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.b2 = np.zeros((self.output_nodes, 1 ))\n",
    "          \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1 # Hidden x N_samples\n",
    "        A1 = self.activation_hidden(Z1)      # Hidden x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2  # Output x N_samples\n",
    "        A2 = softmax(Z2)      #Output x N_samples\n",
    "        return A2, Z2, A1, Z1\n",
    "    \n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A2)/pass_length\n",
    "            # deltas\n",
    "            dZ2 = A2 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   #Output x hidden\n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)              # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              #Hidden x1                                            # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backprop_adam_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)  \n",
    "            t = target_SGD[:,k:k+batch_size].reshape(-1,batch_size)\n",
    "            cost = cost + self.backpropADAM(X, t)/pass_length\n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2    \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "    def predict(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "             \n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 14epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "           \n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 21epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 19epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 44epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 23epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs     \n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 4epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "       \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ef263",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "### Part 4 -   Two Hidden Layers Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1ee59",
   "metadata": {},
   "source": [
    "# Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1383e",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden1 x N_features\n",
    "- **b1**    : Hidden1\n",
    "- **W2**    : Hidden2 x Hidden1\n",
    "- **b2**    : Hidden2\n",
    "- **W3**    : Output x Hidden\n",
    "- **b3**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de86d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenTwo:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes_1,\n",
    "                 hidden_nodes_2,\n",
    "                 activation_hidden_1,\n",
    "                 activation_hidden_2,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                 \n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes_1 = hidden_nodes_1    \n",
    "        self.hidden_nodes_2 = hidden_nodes_2    \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden_1 = activation_hidden_1\n",
    "        self.activation_hidden_2 = activation_hidden_2\n",
    "        self.hidden_derivative_1 = derivative(self.activation_hidden_1)\n",
    "        self.hidden_derivative_2 = derivative(self.activation_hidden_2)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        if self.activation_hidden_1 == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1/2 )  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2/2 )  # output x hidden2\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1)  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2)  # output x hidden2\n",
    "        \n",
    "    def create_biases(self):  \n",
    "        self.b1 =  np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.b2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.b3 = np.zeros((self.output_nodes, 1 ))\n",
    "     \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Vdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Vdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Sdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Sdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1      # Hidden1 x N_samples\n",
    "        A1 = self.activation_hidden_1(Z1)      # Hidden1 x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2      # Hidden2 x N_samples\n",
    "        A2 = self.activation_hidden_2(Z2)      # Hidden2 x N_samples\n",
    "        Z3 = self.W3.dot(A2) + self.b3       # Output x N_samples\n",
    "        A3 = softmax(Z3)                     #Output x N_samples\n",
    "        return A3, Z3, A2, Z2, A1, Z1\n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N_samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "     \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr*dW3\n",
    "        self.b3 -= lr*db3\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A3)/pass_length\n",
    "            # deltas\n",
    "            dZ3 = A3 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW3 = dZ3.dot(A2.T)/batch_size                                   #Output x hidden_2\n",
    "            db3 = np.sum(dZ3, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)            # Hidden2 x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   # Hidden2 x Hidden1 \n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              # Hidden2 x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)            # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              # Hidden x 1                        \n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W3 -= lr*dW3\n",
    "            self.b3 -= lr*db3\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N samples\n",
    "        m = X.shape[0]   \n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdw3 = beta1*self.Vdw3 + (1-beta1)*dW3\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        self.Vdb3 = beta1*self.Vdb3 + (1-beta1)*db3\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdw3 = beta2*self.Sdw3 + (1-beta2)*dW3**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2\n",
    "        self.Sdb3 = beta2*self.Sdb3 + (1-beta2)*db3**2  \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr * self.Vdw3 / (np.sqrt(self.Sdw3)+1e-8)\n",
    "        self.b3 -= lr * self.Vdb3 / (np.sqrt(self.Sdb3)+1e-8)\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "    def backprop_adam_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)  \n",
    "            t = target_SGD[:,k:k+batch_size].reshape(-1,batch_size)\n",
    "            cost = cost + self.backpropADAM(X, t)/pass_length\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def predict(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A3\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    \n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            \n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            \n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 23epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca59cb4",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "### Part 5 -  Loading Fashion MNIST\n",
    "Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b19461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcacf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf5c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f6e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = X_train.shape[1]\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d38a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshaping training and testing set\n",
    "X_train = X_train.reshape(N_train, l*l, 1).squeeze()\n",
    "X_test = X_test.reshape(N_test, l*l, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9846a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming set to categorical\n",
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe45c9",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2735d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX = 255\n",
    "#X_train = X_train/ MAX\n",
    "#X_test =X_test/ MAX\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d8c1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbfae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f303d9",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "### Part 6 -  Fashion MNIST with 1 hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b513e2a",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part6.1'></a>\n",
    "## Sigmoid and tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa228f",
   "metadata": {},
   "source": [
    "### Sigmoid 5 hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "1e4d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sigmoid = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "e552d3e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.22878686917662514\n",
      "Loss after epoch 20 : 0.22324330296231312\n",
      "Loss after epoch 30 : 0.21951122002251572\n",
      "Loss after epoch 40 : 0.21682870435876497\n",
      "Loss after epoch 50 : 0.21474906125226936\n",
      "Loss after epoch 60 : 0.21304089540720955\n",
      "Loss after epoch 70 : 0.2115789143401526\n",
      "Loss after epoch 80 : 0.21028976361325366\n",
      "Loss after epoch 90 : 0.20912740855936438\n",
      "Loss after epoch 100 : 0.20806138458023749\n",
      "Loss after epoch 110 : 0.20707074711843926\n",
      "Loss after epoch 120 : 0.2061407151166888\n",
      "Loss after epoch 130 : 0.2052606698925594\n",
      "Loss after epoch 140 : 0.2044228751755826\n",
      "Loss after epoch 150 : 0.2036216050299323\n",
      "Loss after epoch 160 : 0.2028525234367993\n",
      "Loss after epoch 170 : 0.2021122377120528\n",
      "Loss after epoch 180 : 0.20139798405825327\n",
      "Loss after epoch 190 : 0.20070741692970584\n",
      "Loss after epoch 200 : 0.20003847739535777\n",
      "Loss after epoch 210 : 0.19938931727843567\n",
      "Loss after epoch 220 : 0.1987582589087303\n",
      "Loss after epoch 230 : 0.19814377497191293\n",
      "Loss after epoch 240 : 0.19754447797836727\n",
      "Loss after epoch 250 : 0.19695911318580325\n",
      "Loss after epoch 260 : 0.1963865518947187\n",
      "Loss after epoch 270 : 0.19582578392751285\n",
      "Loss after epoch 280 : 0.19527590909458323\n",
      "Loss after epoch 290 : 0.1947361278762872\n",
      "Loss after epoch 300 : 0.19420573167081157\n",
      "Loss after epoch 310 : 0.1936840929442396\n",
      "Loss after epoch 320 : 0.19317065556277058\n",
      "Loss after epoch 330 : 0.1926649255293884\n",
      "Loss after epoch 340 : 0.19216646230059609\n",
      "Loss after epoch 350 : 0.19167487082183382\n",
      "Loss after epoch 360 : 0.19118979438717382\n",
      "Loss after epoch 370 : 0.19071090839466995\n",
      "Loss after epoch 380 : 0.19023791503092616\n",
      "Loss after epoch 390 : 0.18977053887783957\n",
      "Loss after epoch 400 : 0.18930852339431395\n",
      "Loss after epoch 410 : 0.1888516281904275\n",
      "Loss after epoch 420 : 0.18839962698511284\n",
      "Loss after epoch 430 : 0.18795230612344965\n",
      "Loss after epoch 440 : 0.1875094635267082\n",
      "Loss after epoch 450 : 0.18707090795591302\n",
      "Loss after epoch 460 : 0.1866364584851357\n",
      "Loss after epoch 470 : 0.18620594410061728\n",
      "Loss after epoch 480 : 0.18577920336294113\n",
      "Loss after epoch 490 : 0.18535608408928922\n",
      "Loss after epoch 501 : 0.18497825451950434\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "2ab15b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.4723\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b341e5",
   "metadata": {},
   "source": [
    " More epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "c00b5555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1845201455241048\n",
      "Loss after epoch 20 : 0.1841070651372288\n",
      "Loss after epoch 30 : 0.18369708327252177\n",
      "Loss after epoch 40 : 0.1832900887719804\n",
      "Loss after epoch 50 : 0.1828859775081273\n",
      "Loss after epoch 60 : 0.1824846519753738\n",
      "Loss after epoch 70 : 0.18208602088695125\n",
      "Loss after epoch 80 : 0.18168999878248254\n",
      "Loss after epoch 90 : 0.1812965056500274\n",
      "Loss after epoch 100 : 0.1809054665652534\n",
      "Loss after epoch 110 : 0.18051681134937325\n",
      "Loss after epoch 120 : 0.18013047424665227\n",
      "Loss after epoch 130 : 0.17974639362165884\n",
      "Loss after epoch 140 : 0.17936451167595469\n",
      "Loss after epoch 150 : 0.17898477418360056\n",
      "Loss after epoch 160 : 0.17860713024464278\n",
      "Loss after epoch 170 : 0.17823153205562078\n",
      "Loss after epoch 180 : 0.177857934696077\n",
      "Loss after epoch 190 : 0.17748629593004103\n",
      "Loss after epoch 200 : 0.1771165760214742\n",
      "Loss after epoch 210 : 0.17674873756269946\n",
      "Loss after epoch 220 : 0.17638274531489245\n",
      "Loss after epoch 230 : 0.17601856605976515\n",
      "Loss after epoch 240 : 0.17565616846162854\n",
      "Loss after epoch 250 : 0.1752955229390832\n",
      "Loss after epoch 260 : 0.17493660154563698\n",
      "Loss after epoch 270 : 0.17457937785860544\n",
      "Loss after epoch 280 : 0.1742238268756978\n",
      "Loss after epoch 290 : 0.1738699249187364\n",
      "Loss after epoch 300 : 0.17351764954400156\n",
      "Loss after epoch 310 : 0.1731669794587302\n",
      "Loss after epoch 320 : 0.1728178944433324\n",
      "Loss after epoch 330 : 0.17247037527892525\n",
      "Loss after epoch 340 : 0.17212440367980875\n",
      "Loss after epoch 350 : 0.17177996223053899\n",
      "Loss after epoch 360 : 0.1714370343272809\n",
      "Loss after epoch 370 : 0.17109560412314\n",
      "Loss after epoch 380 : 0.17075565647720134\n",
      "Loss after epoch 390 : 0.17041717690701896\n",
      "Loss after epoch 400 : 0.17008015154431919\n",
      "Loss after epoch 410 : 0.16974456709370028\n",
      "Loss after epoch 420 : 0.16941041079412034\n",
      "Loss after epoch 430 : 0.16907767038298946\n",
      "Loss after epoch 440 : 0.16874633406268594\n",
      "Loss after epoch 450 : 0.16841639046933876\n",
      "Loss after epoch 460 : 0.16808782864371816\n",
      "Loss after epoch 470 : 0.1677606380041009\n",
      "Loss after epoch 480 : 0.1674348083209737\n",
      "Loss after epoch 490 : 0.16711032969345718\n",
      "Loss after epoch 500 : 0.16678719252733423\n",
      "Loss after epoch 510 : 0.16646538751458004\n",
      "Loss after epoch 520 : 0.16614490561429412\n",
      "Loss after epoch 530 : 0.16582573803494255\n",
      "Loss after epoch 540 : 0.16550787621782714\n",
      "Loss after epoch 550 : 0.16519131182169822\n",
      "Loss after epoch 560 : 0.1648760367084417\n",
      "Loss after epoch 570 : 0.1645620429297646\n",
      "Loss after epoch 580 : 0.16424932271481943\n",
      "Loss after epoch 590 : 0.16393786845870337\n",
      "Loss after epoch 600 : 0.16362767271177506\n",
      "Loss after epoch 610 : 0.16331872816973722\n",
      "Loss after epoch 620 : 0.16301102766443315\n",
      "Loss after epoch 630 : 0.16270456415531068\n",
      "Loss after epoch 640 : 0.16239933072150955\n",
      "Loss after epoch 650 : 0.16209532055453002\n",
      "Loss after epoch 660 : 0.16179252695144455\n",
      "Loss after epoch 670 : 0.16149094330861555\n",
      "Loss after epoch 680 : 0.16119056311588437\n",
      "Loss after epoch 690 : 0.16089137995120106\n",
      "Loss after epoch 700 : 0.16059338747566201\n",
      "Loss after epoch 710 : 0.16029657942892966\n",
      "Loss after epoch 720 : 0.16000094962500563\n",
      "Loss after epoch 730 : 0.15970649194833414\n",
      "Loss after epoch 740 : 0.15941320035021028\n",
      "Loss after epoch 750 : 0.1591210688454731\n",
      "Loss after epoch 760 : 0.15883009150946104\n",
      "Loss after epoch 770 : 0.1585402624752126\n",
      "Loss after epoch 780 : 0.15825157593089184\n",
      "Loss after epoch 790 : 0.15796402611742344\n",
      "Loss after epoch 800 : 0.1576776073263214\n",
      "Loss after epoch 810 : 0.15739231389769542\n",
      "Loss after epoch 820 : 0.15710814021842187\n",
      "Loss after epoch 830 : 0.15682508072046716\n",
      "Loss after epoch 840 : 0.15654312987934943\n",
      "Loss after epoch 850 : 0.15626228221272961\n",
      "Loss after epoch 860 : 0.15598253227911915\n",
      "Loss after epoch 870 : 0.15570387467669636\n",
      "Loss after epoch 880 : 0.15542630404222058\n",
      "Loss after epoch 890 : 0.15514981505003708\n",
      "Loss after epoch 900 : 0.15487440241116374\n",
      "Loss after epoch 910 : 0.15460006087245198\n",
      "Loss after epoch 920 : 0.15432678521581664\n",
      "Loss after epoch 930 : 0.1540545702575255\n",
      "Loss after epoch 940 : 0.15378341084754563\n",
      "Loss after epoch 950 : 0.15351330186893872\n",
      "Loss after epoch 960 : 0.15324423823730202\n",
      "Loss after epoch 970 : 0.15297621490024776\n",
      "Loss after epoch 980 : 0.15270922683692012\n",
      "Loss after epoch 990 : 0.15244326905754274\n",
      "Loss after epoch 1001 : 0.1522047838495397\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "667842f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6505\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6158f",
   "metadata": {},
   "source": [
    " More epochs again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "41d2ee19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.15191442454440818\n",
      "Loss after epoch 20 : 0.15165152798279682\n",
      "Loss after epoch 30 : 0.15138964204869057\n",
      "Loss after epoch 40 : 0.1511287619017964\n",
      "Loss after epoch 50 : 0.1508688827306692\n",
      "Loss after epoch 60 : 0.15060999975239645\n",
      "Loss after epoch 70 : 0.15035210821229322\n",
      "Loss after epoch 80 : 0.1500952033836065\n",
      "Loss after epoch 90 : 0.14983928056722853\n",
      "Loss after epoch 100 : 0.14958433509141605\n",
      "Loss after epoch 110 : 0.14933036231151722\n",
      "Loss after epoch 120 : 0.14907735760970434\n",
      "Loss after epoch 130 : 0.14882531639471183\n",
      "Loss after epoch 140 : 0.1485742341015798\n",
      "Loss after epoch 150 : 0.14832410619140332\n",
      "Loss after epoch 160 : 0.14807492815108647\n",
      "Loss after epoch 170 : 0.14782669549310223\n",
      "Loss after epoch 180 : 0.14757940375525846\n",
      "Loss after epoch 190 : 0.14733304850046886\n",
      "Loss after epoch 200 : 0.1470876253165313\n",
      "Loss after epoch 210 : 0.14684312981591177\n",
      "Loss after epoch 220 : 0.14659955763553573\n",
      "Loss after epoch 230 : 0.14635690443658492\n",
      "Loss after epoch 240 : 0.146115165904303\n",
      "Loss after epoch 250 : 0.14587433774780564\n",
      "Loss after epoch 260 : 0.1456344156998984\n",
      "Loss after epoch 270 : 0.1453953955168999\n",
      "Loss after epoch 280 : 0.14515727297847017\n",
      "Loss after epoch 290 : 0.14492004388744295\n",
      "Loss after epoch 300 : 0.1446837040696614\n",
      "Loss after epoch 310 : 0.144448249373815\n",
      "Loss after epoch 320 : 0.1442136756712761\n",
      "Loss after epoch 330 : 0.14397997885593486\n",
      "Loss after epoch 340 : 0.14374715484402967\n",
      "Loss after epoch 350 : 0.14351519957397132\n",
      "Loss after epoch 360 : 0.1432841090061584\n",
      "Loss after epoch 370 : 0.1430538791227812\n",
      "Loss after epoch 380 : 0.14282450592761275\n",
      "Loss after epoch 390 : 0.1425959854457836\n",
      "Loss after epoch 400 : 0.14236831372354025\n",
      "Loss after epoch 410 : 0.1421414868279829\n",
      "Loss after epoch 420 : 0.1419155008467852\n",
      "Loss after epoch 430 : 0.14169035188789156\n",
      "Loss after epoch 440 : 0.14146603607919578\n",
      "Loss after epoch 450 : 0.14124254956819876\n",
      "Loss after epoch 460 : 0.14101988852165015\n",
      "Loss after epoch 470 : 0.14079804912517294\n",
      "Loss after epoch 480 : 0.14057702758287732\n",
      "Loss after epoch 490 : 0.14035682011696535\n",
      "Loss after epoch 500 : 0.14013742296733192\n",
      "Loss after epoch 510 : 0.13991883239116717\n",
      "Loss after epoch 520 : 0.1397010446625651\n",
      "Loss after epoch 530 : 0.13948405607214248\n",
      "Loss after epoch 540 : 0.1392678629266762\n",
      "Loss after epoch 550 : 0.13905246154876125\n",
      "Loss after epoch 560 : 0.13883784827649506\n",
      "Loss after epoch 570 : 0.1386240194631925\n",
      "Loss after epoch 580 : 0.13841097147713394\n",
      "Loss after epoch 590 : 0.1381987007013488\n",
      "Loss after epoch 600 : 0.13798720353343788\n",
      "Loss after epoch 610 : 0.13777647638543242\n",
      "Loss after epoch 620 : 0.13756651568369205\n",
      "Loss after epoch 630 : 0.13735731786883976\n",
      "Loss after epoch 640 : 0.13714887939573192\n",
      "Loss after epoch 650 : 0.1369411967334619\n",
      "Loss after epoch 660 : 0.13673426636539351\n",
      "Loss after epoch 670 : 0.13652808478922226\n",
      "Loss after epoch 680 : 0.13632264851706072\n",
      "Loss after epoch 690 : 0.13611795407554217\n",
      "Loss after epoch 700 : 0.13591399800594406\n",
      "Loss after epoch 710 : 0.13571077686432223\n",
      "Loss after epoch 720 : 0.13550828722165606\n",
      "Loss after epoch 730 : 0.13530652566400087\n",
      "Loss after epoch 740 : 0.13510548879264314\n",
      "Loss after epoch 750 : 0.13490517322425805\n",
      "Loss after epoch 760 : 0.13470557559106613\n",
      "Loss after epoch 770 : 0.13450669254098635\n",
      "Loss after epoch 780 : 0.13430852073778593\n",
      "Loss after epoch 790 : 0.13411105686122288\n",
      "Loss after epoch 800 : 0.13391429760718315\n",
      "Loss after epoch 810 : 0.13371823968780905\n",
      "Loss after epoch 820 : 0.13352287983161934\n",
      "Loss after epoch 830 : 0.13332821478362156\n",
      "Loss after epoch 840 : 0.13313424130541393\n",
      "Loss after epoch 850 : 0.13294095617527887\n",
      "Loss after epoch 860 : 0.1327483561882678\n",
      "Loss after epoch 870 : 0.13255643815627574\n",
      "Loss after epoch 880 : 0.13236519890810794\n",
      "Loss after epoch 890 : 0.1321746352895378\n",
      "Loss after epoch 900 : 0.1319847441633561\n",
      "Loss after epoch 910 : 0.13179552240941222\n",
      "Loss after epoch 920 : 0.13160696692464785\n",
      "Loss after epoch 930 : 0.13141907462312316\n",
      "Loss after epoch 940 : 0.13123184243603547\n",
      "Loss after epoch 950 : 0.13104526731173194\n",
      "Loss after epoch 960 : 0.13085934621571527\n",
      "Loss after epoch 970 : 0.13067407613064302\n",
      "Loss after epoch 980 : 0.1304894540563218\n",
      "Loss after epoch 990 : 0.13030547700969522\n",
      "Loss after epoch 1001 : 0.13014044671471006\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "6a3fa5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7001\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8e785",
   "metadata": {},
   "source": [
    "Conclusion : Sigmoid is slow to learn lets try tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79022e2f",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "573686e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.19901271791427455\n",
      "Loss after epoch 20 : 0.1839460437641077\n",
      "Loss after epoch 30 : 0.1760564874681165\n",
      "Loss after epoch 40 : 0.17084134159373232\n",
      "Loss after epoch 50 : 0.16702597105090627\n",
      "Loss after epoch 60 : 0.1640225265694221\n",
      "Loss after epoch 70 : 0.16153178938422919\n",
      "Loss after epoch 80 : 0.1593877113964945\n",
      "Loss after epoch 90 : 0.1574894483359804\n",
      "Loss after epoch 100 : 0.15577189774246541\n",
      "Loss after epoch 110 : 0.15419132016498646\n",
      "Loss after epoch 120 : 0.15271742154501597\n",
      "Loss after epoch 130 : 0.1513287025573449\n",
      "Loss after epoch 140 : 0.15000958318016647\n",
      "Loss after epoch 150 : 0.14874855503497456\n",
      "Loss after epoch 160 : 0.1475369643038639\n",
      "Loss after epoch 170 : 0.14636819285711322\n",
      "Loss after epoch 180 : 0.14523709684316952\n",
      "Loss after epoch 190 : 0.1441396165799856\n",
      "Loss after epoch 200 : 0.1430725024526592\n",
      "Loss after epoch 210 : 0.1420331191153498\n",
      "Loss after epoch 220 : 0.14101930220177158\n",
      "Loss after epoch 230 : 0.14002925120224552\n",
      "Loss after epoch 240 : 0.13906144899884254\n",
      "Loss after epoch 250 : 0.13811460190333721\n",
      "Loss after epoch 260 : 0.13718759497602115\n",
      "Loss after epoch 270 : 0.13627945807231895\n",
      "Loss after epoch 280 : 0.13538933926303398\n",
      "Loss after epoch 290 : 0.13451648352834858\n",
      "Loss after epoch 300 : 0.13366021550933285\n",
      "Loss after epoch 310 : 0.13281992557566638\n",
      "Loss after epoch 320 : 0.1319950586608364\n",
      "Loss after epoch 330 : 0.13118510537276504\n",
      "Loss after epoch 340 : 0.13038959492067734\n",
      "Loss after epoch 350 : 0.12960808945769625\n",
      "Loss after epoch 360 : 0.1288401795190163\n",
      "Loss after epoch 370 : 0.12808548031431238\n",
      "Loss after epoch 380 : 0.12734362869478438\n",
      "Loss after epoch 390 : 0.12661428065800878\n",
      "Loss after epoch 400 : 0.12589710928317568\n",
      "Loss after epoch 410 : 0.12519180301180968\n",
      "Loss after epoch 420 : 0.1244980642085437\n",
      "Loss after epoch 430 : 0.12381560795335711\n",
      "Loss after epoch 440 : 0.12314416102921195\n",
      "Loss after epoch 450 : 0.12248346107576023\n",
      "Loss after epoch 460 : 0.12183325588134578\n",
      "Loss after epoch 470 : 0.1211933027845933\n",
      "Loss after epoch 480 : 0.12056336815664195\n",
      "Loss after epoch 490 : 0.11994322693750929\n",
      "Loss after epoch 500 : 0.11933266220522033\n",
      "Loss after epoch 510 : 0.11873146476295646\n",
      "Loss after epoch 520 : 0.11813943273604033\n",
      "Loss after epoch 530 : 0.11755637117597859\n",
      "Loss after epoch 540 : 0.11698209167260012\n",
      "Loss after epoch 550 : 0.1164164119775913\n",
      "Loss after epoch 560 : 0.11585915564370361\n",
      "Loss after epoch 570 : 0.11531015168390535\n",
      "Loss after epoch 580 : 0.11476923425402688\n",
      "Loss after epoch 590 : 0.1142362423612483\n",
      "Loss after epoch 600 : 0.11371101959930832\n",
      "Loss after epoch 610 : 0.11319341390979751\n",
      "Loss after epoch 620 : 0.11268327736753264\n",
      "Loss after epoch 630 : 0.11218046598698429\n",
      "Loss after epoch 640 : 0.11168483954614632\n",
      "Loss after epoch 650 : 0.11119626142415438\n",
      "Loss after epoch 660 : 0.11071459844934113\n",
      "Loss after epoch 670 : 0.11023972075517974\n",
      "Loss after epoch 680 : 0.10977150164257858\n",
      "Loss after epoch 690 : 0.10930981744808868\n",
      "Loss after epoch 700 : 0.10885454741860477\n",
      "Loss after epoch 710 : 0.1084055735938848\n",
      "Loss after epoch 720 : 0.10796278069852984\n",
      "Loss after epoch 730 : 0.10752605604480434\n",
      "Loss after epoch 740 : 0.10709528944676204\n",
      "Loss after epoch 750 : 0.10667037314459427\n",
      "Loss after epoch 760 : 0.10625120173610335\n",
      "Loss after epoch 770 : 0.10583767211015492\n",
      "Loss after epoch 780 : 0.10542968337561727\n",
      "Loss after epoch 790 : 0.10502713677958037\n",
      "Loss after epoch 800 : 0.10462993561126337\n",
      "Loss after epoch 810 : 0.10423798509273807\n",
      "Loss after epoch 820 : 0.10385119226285923\n",
      "Loss after epoch 830 : 0.10346946586420637\n",
      "Loss after epoch 840 : 0.10309271624260861\n",
      "Loss after epoch 850 : 0.10272085526509564\n",
      "Loss after epoch 860 : 0.10235379625692859\n",
      "Loss after epoch 870 : 0.10199145395421008\n",
      "Loss after epoch 880 : 0.10163374446683003\n",
      "Loss after epoch 890 : 0.1012805852470244\n",
      "Loss after epoch 900 : 0.1009318950605125\n",
      "Loss after epoch 910 : 0.10058759395895411\n",
      "Loss after epoch 920 : 0.10024760325369379\n",
      "Loss after epoch 930 : 0.09991184549131861\n",
      "Loss after epoch 940 : 0.09958024443161306\n",
      "Loss after epoch 950 : 0.09925272502829627\n",
      "Loss after epoch 960 : 0.09892921341267177\n",
      "Loss after epoch 970 : 0.0986096368801239\n",
      "Loss after epoch 980 : 0.09829392387928726\n",
      "Loss after epoch 990 : 0.0979820040036948\n",
      "Loss after epoch 1001 : 0.09770446196459562\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "77111639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7345\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927d822",
   "metadata": {},
   "source": [
    "tanh faster to learn lets try more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "6a8beb5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.09736926769271226\n",
      "Loss after epoch 20 : 0.09706831612501637\n",
      "Loss after epoch 30 : 0.09677088741593402\n",
      "Loss after epoch 40 : 0.09647691683306854\n",
      "Loss after epoch 50 : 0.0961863407807747\n",
      "Loss after epoch 60 : 0.09589909680308277\n",
      "Loss after epoch 70 : 0.09561512358641353\n",
      "Loss after epoch 80 : 0.09533436096136601\n",
      "Loss after epoch 90 : 0.09505674990296167\n",
      "Loss after epoch 100 : 0.0947822325289809\n",
      "Loss after epoch 110 : 0.09451075209640591\n",
      "Loss after epoch 120 : 0.09424225299642072\n",
      "Loss after epoch 130 : 0.09397668074879285\n",
      "Loss after epoch 140 : 0.09371398199667001\n",
      "Loss after epoch 150 : 0.0934541045027858\n",
      "Loss after epoch 160 : 0.0931969971477858\n",
      "Loss after epoch 170 : 0.09294260993093617\n",
      "Loss after epoch 180 : 0.09269089397298262\n",
      "Loss after epoch 190 : 0.09244180152051566\n",
      "Loss after epoch 200 : 0.09219528595094599\n",
      "Loss after epoch 210 : 0.09195130177713623\n",
      "Loss after epoch 220 : 0.09170980465083806\n",
      "Loss after epoch 230 : 0.09147075136430176\n",
      "Loss after epoch 240 : 0.09123409984969526\n",
      "Loss after epoch 250 : 0.09099980917624488\n",
      "Loss after epoch 260 : 0.0907678395452582\n",
      "Loss after epoch 270 : 0.0905381522834008\n",
      "Loss after epoch 280 : 0.09031070983476454\n",
      "Loss after epoch 290 : 0.0900854757523992\n",
      "Loss after epoch 300 : 0.08986241469008072\n",
      "Loss after epoch 310 : 0.08964149239516844\n",
      "Loss after epoch 320 : 0.08942267570345218\n",
      "Loss after epoch 330 : 0.08920593253689439\n",
      "Loss after epoch 340 : 0.08899123190509263\n",
      "Loss after epoch 350 : 0.08877854391105816\n",
      "Loss after epoch 360 : 0.0885678397614346\n",
      "Loss after epoch 370 : 0.08835909178044499\n",
      "Loss after epoch 380 : 0.08815227342556252\n",
      "Loss after epoch 390 : 0.0879473593011377\n",
      "Loss after epoch 400 : 0.08774432516420193\n",
      "Loss after epoch 410 : 0.0875431479149531\n",
      "Loss after epoch 420 : 0.08734380556394156\n",
      "Loss after epoch 430 : 0.08714627716974316\n",
      "Loss after epoch 440 : 0.08695054274550859\n",
      "Loss after epoch 450 : 0.08675658313955904\n",
      "Loss after epoch 460 : 0.08656437990204534\n",
      "Loss after epoch 470 : 0.08637391515369741\n",
      "Loss after epoch 480 : 0.08618517147185174\n",
      "Loss after epoch 490 : 0.08599813180348567\n",
      "Loss after epoch 500 : 0.08581277940735768\n",
      "Loss after epoch 510 : 0.08562909782069208\n",
      "Loss after epoch 520 : 0.08544707084221455\n",
      "Loss after epoch 530 : 0.08526668252298802\n",
      "Loss after epoch 540 : 0.08508791715836174\n",
      "Loss after epoch 550 : 0.08491075927700245\n",
      "Loss after epoch 560 : 0.08473519362538796\n",
      "Loss after epoch 570 : 0.08456120514783586\n",
      "Loss after epoch 580 : 0.08438877896306993\n",
      "Loss after epoch 590 : 0.08421790033868302\n",
      "Loss after epoch 600 : 0.08404855466484418\n",
      "Loss after epoch 610 : 0.08388072742841426\n",
      "Loss after epoch 620 : 0.08371440418838545\n",
      "Loss after epoch 630 : 0.08354957055331472\n",
      "Loss after epoch 640 : 0.08338621216120794\n",
      "Loss after epoch 650 : 0.08322431466213442\n",
      "Loss after epoch 660 : 0.08306386370371671\n",
      "Loss after epoch 670 : 0.08290484491952846\n",
      "Loss after epoch 680 : 0.08274724392035453\n",
      "Loss after epoch 690 : 0.0825910462882024\n",
      "Loss after epoch 700 : 0.08243623757290602\n",
      "Loss after epoch 710 : 0.08228280329112923\n",
      "Loss after epoch 720 : 0.08213072892754765\n",
      "Loss after epoch 730 : 0.0819799999379744\n",
      "Loss after epoch 740 : 0.08183060175418216\n",
      "Loss after epoch 750 : 0.0816825197901732\n",
      "Loss after epoch 760 : 0.08153573944964948\n",
      "Loss after epoch 770 : 0.08139024613444407\n",
      "Loss after epoch 780 : 0.08124602525368335\n",
      "Loss after epoch 790 : 0.08110306223346747\n",
      "Loss after epoch 800 : 0.0809613425268691\n",
      "Loss after epoch 810 : 0.08082085162407393\n",
      "Loss after epoch 820 : 0.08068157506250224\n",
      "Loss after epoch 830 : 0.08054349843677461\n",
      "Loss after epoch 840 : 0.08040660740840387\n",
      "Loss after epoch 850 : 0.0802708877151166\n",
      "Loss after epoch 860 : 0.08013632517972694\n",
      "Loss after epoch 870 : 0.08000290571850426\n",
      "Loss after epoch 880 : 0.0798706153489934\n",
      "Loss after epoch 890 : 0.07973944019726295\n",
      "Loss after epoch 900 : 0.07960936650456993\n",
      "Loss after epoch 910 : 0.0794803806334436\n",
      "Loss after epoch 920 : 0.07935246907320018\n",
      "Loss after epoch 930 : 0.07922561844491167\n",
      "Loss after epoch 940 : 0.07909981550585782\n",
      "Loss after epoch 950 : 0.07897504715349585\n",
      "Loss after epoch 960 : 0.07885130042898834\n",
      "Loss after epoch 970 : 0.07872856252032946\n",
      "Loss after epoch 980 : 0.07860682076511134\n",
      "Loss after epoch 990 : 0.07848606265297105\n",
      "Loss after epoch 1001 : 0.07837821115129574\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "ab868f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7616\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0568bf",
   "metadata": {},
   "source": [
    "Accuracy increases, lets try more hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "095a199a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.18243071300010974\n",
      "Loss after epoch 20 : 0.16443330947833454\n",
      "Loss after epoch 30 : 0.15405631998614444\n",
      "Loss after epoch 40 : 0.14675462757163799\n",
      "Loss after epoch 50 : 0.14134486145826794\n",
      "Loss after epoch 60 : 0.13708421482458347\n",
      "Loss after epoch 70 : 0.13355617250438886\n",
      "Loss after epoch 80 : 0.13052072819131544\n",
      "Loss after epoch 90 : 0.12783552961703754\n",
      "Loss after epoch 100 : 0.12541230826896504\n",
      "Loss after epoch 110 : 0.12319353806869811\n",
      "Loss after epoch 120 : 0.12113998922543047\n",
      "Loss after epoch 130 : 0.11922385426614797\n",
      "Loss after epoch 140 : 0.11742474232179387\n",
      "Loss after epoch 150 : 0.11572721248276718\n",
      "Loss after epoch 160 : 0.1141192226790189\n",
      "Loss after epoch 170 : 0.11259115084606686\n",
      "Loss after epoch 180 : 0.11113515371283451\n",
      "Loss after epoch 190 : 0.10974472511765353\n",
      "Loss after epoch 200 : 0.10841438165369169\n",
      "Loss after epoch 210 : 0.10713943371617227\n",
      "Loss after epoch 220 : 0.10591581517898954\n",
      "Loss after epoch 230 : 0.10473995524159851\n",
      "Loss after epoch 240 : 0.10360868274578552\n",
      "Loss after epoch 250 : 0.10251915581817454\n",
      "Loss after epoch 260 : 0.10146880987948456\n",
      "Loss after epoch 270 : 0.1004553178284084\n",
      "Loss after epoch 280 : 0.09947655821088398\n",
      "Loss after epoch 290 : 0.09853058916697592\n",
      "Loss after epoch 300 : 0.09761562702851867\n",
      "Loss after epoch 310 : 0.09673002873837748\n",
      "Loss after epoch 320 : 0.09587227724697223\n",
      "Loss after epoch 330 : 0.09504096904747437\n",
      "Loss after epoch 340 : 0.09423480316576148\n",
      "Loss after epoch 350 : 0.09345257121324714\n",
      "Loss after epoch 360 : 0.0926931484241993\n",
      "Loss after epoch 370 : 0.09195548577031135\n",
      "Loss after epoch 380 : 0.09123860319958146\n",
      "Loss after epoch 390 : 0.09054158388209181\n",
      "Loss after epoch 400 : 0.08986356922435441\n",
      "Loss after epoch 410 : 0.0892037544015228\n",
      "Loss after epoch 420 : 0.08856138421148663\n",
      "Loss after epoch 430 : 0.08793574912586402\n",
      "Loss after epoch 440 : 0.08732618148661202\n",
      "Loss after epoch 450 : 0.08673205187009163\n",
      "Loss after epoch 460 : 0.08615276569403126\n",
      "Loss after epoch 470 : 0.08558776015444854\n",
      "Loss after epoch 480 : 0.08503650154795368\n",
      "Loss after epoch 490 : 0.08449848298288262\n",
      "Loss after epoch 500 : 0.08397322243751476\n",
      "Loss after epoch 510 : 0.08346026109871575\n",
      "Loss after epoch 520 : 0.08295916190906946\n",
      "Loss after epoch 530 : 0.08246950825887998\n",
      "Loss after epoch 540 : 0.08199090277553896\n",
      "Loss after epoch 550 : 0.08152296618171387\n",
      "Loss after epoch 560 : 0.08106533621111202\n",
      "Loss after epoch 570 : 0.08061766658282632\n",
      "Loss after epoch 580 : 0.08017962604105242\n",
      "Loss after epoch 590 : 0.07975089746707718\n",
      "Loss after epoch 600 : 0.07933117706729095\n",
      "Loss after epoch 610 : 0.07892017363770326\n",
      "Loss after epoch 620 : 0.0785176079049622\n",
      "Loss after epoch 630 : 0.07812321194825357\n",
      "Loss after epoch 640 : 0.07773672871621853\n",
      "Loss after epoch 650 : 0.07735791166561128\n",
      "Loss after epoch 660 : 0.07698652455431988\n",
      "Loss after epoch 670 : 0.076622341399117\n",
      "Loss after epoch 680 : 0.0762651465282589\n",
      "Loss after epoch 690 : 0.07591473451837219\n",
      "Loss after epoch 700 : 0.07557090969945086\n",
      "Loss after epoch 710 : 0.07523348503251255\n",
      "Loss after epoch 720 : 0.07490228056586658\n",
      "Loss after epoch 730 : 0.07457712203269914\n",
      "Loss after epoch 740 : 0.0742578400658888\n",
      "Loss after epoch 750 : 0.07394427007420604\n",
      "Loss after epoch 760 : 0.0736362524984771\n",
      "Loss after epoch 770 : 0.073333633144231\n",
      "Loss after epoch 780 : 0.07303626341974251\n",
      "Loss after epoch 790 : 0.07274400041835886\n",
      "Loss after epoch 800 : 0.07245670682782633\n",
      "Loss after epoch 810 : 0.07217425066611263\n",
      "Loss after epoch 820 : 0.07189650487311183\n",
      "Loss after epoch 830 : 0.0716233468312592\n",
      "Loss after epoch 840 : 0.0713546579124356\n",
      "Loss after epoch 850 : 0.07109032312734942\n",
      "Loss after epoch 860 : 0.07083023089948491\n",
      "Loss after epoch 870 : 0.07057427293628568\n",
      "Loss after epoch 880 : 0.07032234415054814\n",
      "Loss after epoch 890 : 0.07007434259286474\n",
      "Loss after epoch 900 : 0.06983016937603914\n",
      "Loss after epoch 910 : 0.06958972859312886\n",
      "Loss after epoch 920 : 0.06935292724935438\n",
      "Loss after epoch 930 : 0.06911967524666263\n",
      "Loss after epoch 940 : 0.06888988547922771\n",
      "Loss after epoch 950 : 0.06866347411184075\n",
      "Loss after epoch 960 : 0.06844036109979748\n",
      "Loss after epoch 970 : 0.06822047093281342\n",
      "Loss after epoch 980 : 0.06800373341960982\n",
      "Loss after epoch 990 : 0.06779008411321616\n",
      "Loss after epoch 1001 : 0.06760039118329622\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 2*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "e98dc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7867\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319143f1",
   "metadata": {},
   "source": [
    "### More hidden nodes => Faster learning\n",
    "2X hidden nodes = 2X learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "39e2bd0d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1768686421578603\n",
      "Loss after epoch 20 : 0.1496912437462031\n",
      "Loss after epoch 30 : 0.13583368568823373\n",
      "Loss after epoch 40 : 0.12685158291716592\n",
      "Loss after epoch 50 : 0.12034212894110974\n",
      "Loss after epoch 60 : 0.11529615154308333\n",
      "Loss after epoch 70 : 0.11119942488119336\n",
      "Loss after epoch 80 : 0.10776107435545887\n",
      "Loss after epoch 90 : 0.10480530305644188\n",
      "Loss after epoch 100 : 0.10221926425090824\n",
      "Loss after epoch 110 : 0.09992576509578766\n",
      "Loss after epoch 120 : 0.09786903565317893\n",
      "Loss after epoch 130 : 0.09600721632390159\n",
      "Loss after epoch 140 : 0.0943081059081259\n",
      "Loss after epoch 150 : 0.09274651087502041\n",
      "Loss after epoch 160 : 0.09130245744484108\n",
      "Loss after epoch 170 : 0.08995992549071942\n",
      "Loss after epoch 180 : 0.0887059252038117\n",
      "Loss after epoch 190 : 0.08752981111536305\n",
      "Loss after epoch 200 : 0.08642276695041502\n",
      "Loss after epoch 210 : 0.0853774163385686\n",
      "Loss after epoch 220 : 0.08438752708130536\n",
      "Loss after epoch 230 : 0.08344778510653884\n",
      "Loss after epoch 240 : 0.08255362041599741\n",
      "Loss after epoch 250 : 0.08170107199693039\n",
      "Loss after epoch 260 : 0.08088668220027127\n",
      "Loss after epoch 270 : 0.08010741374302419\n",
      "Loss after epoch 280 : 0.0793605843871309\n",
      "Loss after epoch 290 : 0.07864381534983494\n",
      "Loss after epoch 300 : 0.07795498947450029\n",
      "Loss after epoch 310 : 0.07729221504440527\n",
      "Loss after epoch 320 : 0.07665379299857376\n",
      "Loss after epoch 330 : 0.07603818871341356\n",
      "Loss after epoch 340 : 0.07544400979805235\n",
      "Loss after epoch 350 : 0.07486998807751853\n",
      "Loss after epoch 360 : 0.07431496289768724\n",
      "Loss after epoch 370 : 0.07377786581063026\n",
      "Loss after epoch 380 : 0.07325770878491944\n",
      "Loss after epoch 390 : 0.0727535764786948\n",
      "Loss after epoch 400 : 0.07226462087531235\n",
      "Loss after epoch 410 : 0.07179005650762076\n",
      "Loss after epoch 420 : 0.07132915560127354\n",
      "Loss after epoch 430 : 0.07088124320724278\n",
      "Loss after epoch 440 : 0.07044569255343733\n",
      "Loss after epoch 450 : 0.07002192076392247\n",
      "Loss after epoch 460 : 0.06960938499367465\n",
      "Loss after epoch 470 : 0.06920757896510785\n",
      "Loss after epoch 480 : 0.0688160298645122\n",
      "Loss after epoch 490 : 0.06843429554786572\n",
      "Loss after epoch 500 : 0.06806196200698532\n",
      "Loss after epoch 510 : 0.06769864105423638\n",
      "Loss after epoch 520 : 0.06734396819471258\n",
      "Loss after epoch 530 : 0.0669976006668523\n",
      "Loss after epoch 540 : 0.06665921564343667\n",
      "Loss after epoch 550 : 0.06632850859250598\n",
      "Loss after epoch 560 : 0.06600519180041493\n",
      "Loss after epoch 570 : 0.06568899305672103\n",
      "Loss after epoch 580 : 0.0653796544937743\n",
      "Loss after epoch 590 : 0.0650769315648214\n",
      "Loss after epoch 600 : 0.0647805921361383\n",
      "Loss after epoch 610 : 0.06449041566449845\n",
      "Loss after epoch 620 : 0.06420619243367187\n",
      "Loss after epoch 630 : 0.06392772283285465\n",
      "Loss after epoch 640 : 0.06365481667295705\n",
      "Loss after epoch 650 : 0.06338729254812124\n",
      "Loss after epoch 660 : 0.06312497725467398\n",
      "Loss after epoch 670 : 0.06286770527643322\n",
      "Loss after epoch 680 : 0.06261531833673059\n",
      "Loss after epoch 690 : 0.06236766500870075\n",
      "Loss after epoch 700 : 0.06212460037018027\n",
      "Loss after epoch 710 : 0.06188598568893604\n",
      "Loss after epoch 720 : 0.06165168812657223\n",
      "Loss after epoch 730 : 0.06142158045331721\n",
      "Loss after epoch 740 : 0.06119554076948147\n",
      "Loss after epoch 750 : 0.06097345223209669\n",
      "Loss after epoch 760 : 0.06075520278711186\n",
      "Loss after epoch 770 : 0.060540684908753015\n",
      "Loss after epoch 780 : 0.0603297953484085\n",
      "Loss after epoch 790 : 0.06012243489570101\n",
      "Loss after epoch 800 : 0.05991850815420956\n",
      "Loss after epoch 810 : 0.05971792333361913\n",
      "Loss after epoch 820 : 0.05952059205904803\n",
      "Loss after epoch 830 : 0.05932642919717323\n",
      "Loss after epoch 840 : 0.05913535269781354\n",
      "Loss after epoch 850 : 0.058947283449013045\n",
      "Loss after epoch 860 : 0.058762145143454035\n",
      "Loss after epoch 870 : 0.05857986415414876\n",
      "Loss after epoch 880 : 0.05840036941769183\n",
      "Loss after epoch 890 : 0.05822359232376828\n",
      "Loss after epoch 900 : 0.058049466610005\n",
      "Loss after epoch 910 : 0.05787792826157432\n",
      "Loss after epoch 920 : 0.05770891541518556\n",
      "Loss after epoch 930 : 0.05754236826724766\n",
      "Loss after epoch 940 : 0.05737822898606184\n",
      "Loss after epoch 950 : 0.05721644162794217\n",
      "Loss after epoch 960 : 0.057056952057164295\n",
      "Loss after epoch 970 : 0.05689970786964134\n",
      "Loss after epoch 980 : 0.05674465832020976\n",
      "Loss after epoch 990 : 0.056591754253400385\n",
      "Loss after epoch 1001 : 0.05645593558288547\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "b8ac008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8087\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53f325",
   "metadata": {},
   "source": [
    " Slightly better results, learning not that fast let's try relu before adding optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e6778",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part6.2'></a>\n",
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "710cd770",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.22090266196295236\n",
      "Loss after epoch 20 : 0.19523942278124612\n",
      "Loss after epoch 30 : 0.1841315866303666\n",
      "Loss after epoch 40 : 0.17746053111685306\n",
      "Loss after epoch 50 : 0.17263425961832415\n",
      "Loss after epoch 60 : 0.1688167680664316\n",
      "Loss after epoch 70 : 0.16562200664513124\n",
      "Loss after epoch 80 : 0.1628434081903318\n",
      "Loss after epoch 90 : 0.1603743470318354\n",
      "Loss after epoch 100 : 0.15813575456560658\n",
      "Loss after epoch 110 : 0.15606843318793037\n",
      "Loss after epoch 120 : 0.15413539092196019\n",
      "Loss after epoch 130 : 0.15230972958807631\n",
      "Loss after epoch 140 : 0.15056102913785943\n",
      "Loss after epoch 150 : 0.1488740647590752\n",
      "Loss after epoch 160 : 0.14724008855485587\n",
      "Loss after epoch 170 : 0.14565090588175914\n",
      "Loss after epoch 180 : 0.1441013395402311\n",
      "Loss after epoch 190 : 0.14259182558012873\n",
      "Loss after epoch 200 : 0.14112181406831384\n",
      "Loss after epoch 210 : 0.13969009529060336\n",
      "Loss after epoch 220 : 0.1382972423931227\n",
      "Loss after epoch 230 : 0.13693682308501795\n",
      "Loss after epoch 240 : 0.13560593262598894\n",
      "Loss after epoch 250 : 0.1343074739056616\n",
      "Loss after epoch 260 : 0.13304257964005772\n",
      "Loss after epoch 270 : 0.1318083590433974\n",
      "Loss after epoch 280 : 0.13060566788899935\n",
      "Loss after epoch 290 : 0.12943215881898\n",
      "Loss after epoch 300 : 0.12828724693310586\n",
      "Loss after epoch 310 : 0.1271668502243643\n",
      "Loss after epoch 320 : 0.12607113438212403\n",
      "Loss after epoch 330 : 0.12499906808479451\n",
      "Loss after epoch 340 : 0.12394900512585555\n",
      "Loss after epoch 350 : 0.1229218048363989\n",
      "Loss after epoch 360 : 0.1219158257531586\n",
      "Loss after epoch 370 : 0.12093082771135237\n",
      "Loss after epoch 380 : 0.1199654904309692\n",
      "Loss after epoch 390 : 0.11902062814518584\n",
      "Loss after epoch 400 : 0.11809673499453953\n",
      "Loss after epoch 410 : 0.11719255187128488\n",
      "Loss after epoch 420 : 0.11630847574105911\n",
      "Loss after epoch 430 : 0.11544481986030876\n",
      "Loss after epoch 440 : 0.11459952334427274\n",
      "Loss after epoch 450 : 0.11377308618055734\n",
      "Loss after epoch 460 : 0.11296476863206907\n",
      "Loss after epoch 470 : 0.1121742627492154\n",
      "Loss after epoch 480 : 0.11140136539681254\n",
      "Loss after epoch 490 : 0.11064599184118143\n",
      "Loss after epoch 500 : 0.10990767610601543\n",
      "Loss after epoch 510 : 0.10918603065782718\n",
      "Loss after epoch 520 : 0.1084809332560679\n",
      "Loss after epoch 530 : 0.10778959867113146\n",
      "Loss after epoch 540 : 0.10711239662358271\n",
      "Loss after epoch 550 : 0.10644816243444187\n",
      "Loss after epoch 560 : 0.10579684849336374\n",
      "Loss after epoch 570 : 0.10515782477055073\n",
      "Loss after epoch 580 : 0.10453184738864911\n",
      "Loss after epoch 590 : 0.10391695336583587\n",
      "Loss after epoch 600 : 0.10331365523236627\n",
      "Loss after epoch 610 : 0.10272241627533037\n",
      "Loss after epoch 620 : 0.10214387429825714\n",
      "Loss after epoch 630 : 0.10157635352361732\n",
      "Loss after epoch 640 : 0.10101907372341187\n",
      "Loss after epoch 650 : 0.10046980633621398\n",
      "Loss after epoch 660 : 0.09992936025631811\n",
      "Loss after epoch 670 : 0.09940003914041569\n",
      "Loss after epoch 680 : 0.09888061085298097\n",
      "Loss after epoch 690 : 0.09837032182634885\n",
      "Loss after epoch 700 : 0.0978691249453477\n",
      "Loss after epoch 710 : 0.09737629569475924\n",
      "Loss after epoch 720 : 0.09689087075800547\n",
      "Loss after epoch 730 : 0.09641199661227567\n",
      "Loss after epoch 740 : 0.09593995500034988\n",
      "Loss after epoch 750 : 0.09547572443319192\n",
      "Loss after epoch 760 : 0.09501847929733992\n",
      "Loss after epoch 770 : 0.0945676446587438\n",
      "Loss after epoch 780 : 0.09412284238906087\n",
      "Loss after epoch 790 : 0.09368431935983894\n",
      "Loss after epoch 800 : 0.09325086382752236\n",
      "Loss after epoch 810 : 0.0928229163103153\n",
      "Loss after epoch 820 : 0.09239998651350811\n",
      "Loss after epoch 830 : 0.0919819137256425\n",
      "Loss after epoch 840 : 0.09156826798012073\n",
      "Loss after epoch 850 : 0.09115813132094475\n",
      "Loss after epoch 860 : 0.09075122373045519\n",
      "Loss after epoch 870 : 0.09034767535736261\n",
      "Loss after epoch 880 : 0.08994688061205991\n",
      "Loss after epoch 890 : 0.08954777340816472\n",
      "Loss after epoch 900 : 0.08915115623244153\n",
      "Loss after epoch 910 : 0.08875655250427175\n",
      "Loss after epoch 920 : 0.08836392367614222\n",
      "Loss after epoch 930 : 0.08797342287458562\n",
      "Loss after epoch 940 : 0.08758453665585181\n",
      "Loss after epoch 950 : 0.08719752387125895\n",
      "Loss after epoch 960 : 0.08681289993906087\n",
      "Loss after epoch 970 : 0.0864305695535124\n",
      "Loss after epoch 980 : 0.08604973156698663\n",
      "Loss after epoch 990 : 0.08567111706052381\n",
      "Loss after epoch 1001 : 0.08533235345436108\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "c3a85552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7503\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829b393",
   "metadata": {},
   "source": [
    "Similar results to tanh, let's add hidden nodes and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "f07f4346",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.20999322813225482\n",
      "Loss after epoch 20 : 0.16621525188621342\n",
      "Loss after epoch 30 : 0.14329902067824313\n",
      "Loss after epoch 40 : 0.12870198180130324\n",
      "Loss after epoch 50 : 0.11832672042598538\n",
      "Loss after epoch 60 : 0.11044672544622539\n",
      "Loss after epoch 70 : 0.10420482189985736\n",
      "Loss after epoch 80 : 0.09912683063309624\n",
      "Loss after epoch 90 : 0.0949092299038056\n",
      "Loss after epoch 100 : 0.09133997121026016\n",
      "Loss after epoch 110 : 0.0882733975150196\n",
      "Loss after epoch 120 : 0.08560483915792877\n",
      "Loss after epoch 130 : 0.0832561327578845\n",
      "Loss after epoch 140 : 0.08116321210382554\n",
      "Loss after epoch 150 : 0.07928060696679286\n",
      "Loss after epoch 160 : 0.07757213931440257\n",
      "Loss after epoch 170 : 0.07601432521725095\n",
      "Loss after epoch 180 : 0.07458399309461819\n",
      "Loss after epoch 190 : 0.07326676932193549\n",
      "Loss after epoch 200 : 0.07205356699734393\n",
      "Loss after epoch 210 : 0.07092939149681263\n",
      "Loss after epoch 220 : 0.06988344304179005\n",
      "Loss after epoch 230 : 0.06891068379429008\n",
      "Loss after epoch 240 : 0.06800243629821691\n",
      "Loss after epoch 250 : 0.06715108665229196\n",
      "Loss after epoch 260 : 0.06634963788226204\n",
      "Loss after epoch 270 : 0.0655948169535663\n",
      "Loss after epoch 280 : 0.06488370225047552\n",
      "Loss after epoch 290 : 0.06421095629140897\n",
      "Loss after epoch 300 : 0.06357443154442967\n",
      "Loss after epoch 310 : 0.06297106078136225\n",
      "Loss after epoch 320 : 0.06239747716589233\n",
      "Loss after epoch 330 : 0.061850512296591414\n",
      "Loss after epoch 340 : 0.06132917719220682\n",
      "Loss after epoch 350 : 0.060832247089748985\n",
      "Loss after epoch 360 : 0.06035797951688438\n",
      "Loss after epoch 370 : 0.059904377975945354\n",
      "Loss after epoch 380 : 0.05946997005591431\n",
      "Loss after epoch 390 : 0.059053386717776046\n",
      "Loss after epoch 400 : 0.0586535185565746\n",
      "Loss after epoch 410 : 0.05826918040014603\n",
      "Loss after epoch 420 : 0.05789976512312447\n",
      "Loss after epoch 430 : 0.057544078729395924\n",
      "Loss after epoch 440 : 0.05720073388144488\n",
      "Loss after epoch 450 : 0.05686991656099373\n",
      "Loss after epoch 460 : 0.05655051525850738\n",
      "Loss after epoch 470 : 0.056241685470699646\n",
      "Loss after epoch 480 : 0.05594236735908761\n",
      "Loss after epoch 490 : 0.055651924587826265\n",
      "Loss after epoch 500 : 0.05537055716453745\n",
      "Loss after epoch 510 : 0.05509810976072097\n",
      "Loss after epoch 520 : 0.05483350125873447\n",
      "Loss after epoch 530 : 0.05457637127205065\n",
      "Loss after epoch 540 : 0.054326293887637316\n",
      "Loss after epoch 550 : 0.05408312987713889\n",
      "Loss after epoch 560 : 0.053846773139575595\n",
      "Loss after epoch 570 : 0.053616841849937404\n",
      "Loss after epoch 580 : 0.05339360018966934\n",
      "Loss after epoch 590 : 0.053177062608005275\n",
      "Loss after epoch 600 : 0.052966981938444194\n",
      "Loss after epoch 610 : 0.05276287330456872\n",
      "Loss after epoch 620 : 0.05256484461168967\n",
      "Loss after epoch 630 : 0.0523723726609366\n",
      "Loss after epoch 640 : 0.05218537004655258\n",
      "Loss after epoch 650 : 0.05200338506065297\n",
      "Loss after epoch 660 : 0.0518256781174227\n",
      "Loss after epoch 670 : 0.051652594929976405\n",
      "Loss after epoch 680 : 0.05148405488554234\n",
      "Loss after epoch 690 : 0.05132035052602539\n",
      "Loss after epoch 700 : 0.05116064765409251\n",
      "Loss after epoch 710 : 0.05100466649885098\n",
      "Loss after epoch 720 : 0.05085217187112786\n",
      "Loss after epoch 730 : 0.05070339986586884\n",
      "Loss after epoch 740 : 0.050558312120043684\n",
      "Loss after epoch 750 : 0.05041691814812104\n",
      "Loss after epoch 760 : 0.050278655192809586\n",
      "Loss after epoch 770 : 0.05014336014099392\n",
      "Loss after epoch 780 : 0.050010927996006684\n",
      "Loss after epoch 790 : 0.049881326215012525\n",
      "Loss after epoch 800 : 0.04975439706104884\n",
      "Loss after epoch 810 : 0.049629631598041714\n",
      "Loss after epoch 820 : 0.04950695631566017\n",
      "Loss after epoch 830 : 0.049386383560165184\n",
      "Loss after epoch 840 : 0.04926780838196524\n",
      "Loss after epoch 850 : 0.049151337647006424\n",
      "Loss after epoch 860 : 0.04903683155284055\n",
      "Loss after epoch 870 : 0.048924233965957735\n",
      "Loss after epoch 880 : 0.04881334123915737\n",
      "Loss after epoch 890 : 0.0487044144000415\n",
      "Loss after epoch 900 : 0.04859755936177264\n",
      "Loss after epoch 910 : 0.04849251747187844\n",
      "Loss after epoch 920 : 0.048389381313359424\n",
      "Loss after epoch 930 : 0.048288074870867746\n",
      "Loss after epoch 940 : 0.04818852291781872\n",
      "Loss after epoch 950 : 0.04809051338672313\n",
      "Loss after epoch 960 : 0.04799421212090101\n",
      "Loss after epoch 970 : 0.04789954688590069\n",
      "Loss after epoch 980 : 0.047806509477423605\n",
      "Loss after epoch 990 : 0.047714849873844216\n",
      "Loss after epoch 1001 : 0.04763351629522698\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "61774254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8209\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651c2a6",
   "metadata": {},
   "source": [
    "Better accuracy than tanh, lets try optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99bf01",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part6.3'></a>\n",
    "## Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda32dbb",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "Dropping sigmoid as tanh is almost always better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "f9090b6a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.05185703938220312\n",
      "Loss after epoch 20 : 0.04471078186022689\n",
      "Loss after epoch 30 : 0.041428303200184785\n",
      "Loss after epoch 40 : 0.03939626268286578\n",
      "Loss after epoch 50 : 0.037954493371327516\n",
      "Loss after epoch 60 : 0.0368417009344809\n",
      "Loss after epoch 70 : 0.03592997290066909\n",
      "Loss after epoch 80 : 0.03515323775305513\n",
      "Loss after epoch 90 : 0.03447133335275145\n",
      "Loss after epoch 100 : 0.03385885670744845\n",
      "Loss after epoch 110 : 0.033305375640224975\n",
      "Loss after epoch 120 : 0.03279715846604191\n",
      "Loss after epoch 130 : 0.0323252801148695\n",
      "Loss after epoch 140 : 0.03188310428444356\n",
      "Loss after epoch 150 : 0.03147076768105533\n",
      "Loss after epoch 160 : 0.031083000505919573\n",
      "Loss after epoch 170 : 0.030716833912471483\n",
      "Loss after epoch 180 : 0.030369887815975154\n",
      "Loss after epoch 190 : 0.03004014176796332\n",
      "Loss after epoch 200 : 0.029725845380103078\n",
      "Loss after epoch 210 : 0.029425524437904842\n",
      "Loss after epoch 220 : 0.02913772080378199\n",
      "Loss after epoch 230 : 0.02886090643554675\n",
      "Loss after epoch 240 : 0.028595676854613946\n",
      "Loss after epoch 250 : 0.028340918159051348\n",
      "Loss after epoch 260 : 0.028095682817168705\n",
      "Loss after epoch 270 : 0.027859205329726182\n",
      "Loss after epoch 280 : 0.027630789903802844\n",
      "Loss after epoch 290 : 0.027409784455094958\n",
      "Loss after epoch 300 : 0.027195288842105696\n",
      "Loss after epoch 310 : 0.026987026173189873\n",
      "Loss after epoch 320 : 0.026785816930689647\n",
      "Loss after epoch 330 : 0.026590455838755172\n",
      "Loss after epoch 340 : 0.026400444708405777\n",
      "Loss after epoch 350 : 0.026215664857775173\n",
      "Loss after epoch 360 : 0.026036046867623727\n",
      "Loss after epoch 370 : 0.02586227644589221\n",
      "Loss after epoch 380 : 0.025693431175843946\n",
      "Loss after epoch 390 : 0.02552912382153214\n",
      "Loss after epoch 400 : 0.02536918984764279\n",
      "Loss after epoch 410 : 0.025213421996434733\n",
      "Loss after epoch 420 : 0.02506123661288793\n",
      "Loss after epoch 430 : 0.024913184791523987\n",
      "Loss after epoch 440 : 0.02476888837793714\n",
      "Loss after epoch 450 : 0.02462801060952916\n",
      "Loss after epoch 460 : 0.0244903547874982\n",
      "Loss after epoch 470 : 0.02435581418757769\n",
      "Loss after epoch 480 : 0.024224289995290128\n",
      "Loss after epoch 490 : 0.024095670676648053\n",
      "Loss after epoch 500 : 0.023969818914531666\n",
      "Loss after epoch 510 : 0.02384659123683394\n",
      "Loss after epoch 520 : 0.023725851536277175\n",
      "Loss after epoch 530 : 0.023607469078975363\n",
      "Loss after epoch 540 : 0.02349131531737613\n",
      "Loss after epoch 550 : 0.02337726218048874\n",
      "Loss after epoch 560 : 0.02326518421541372\n",
      "Loss after epoch 570 : 0.02315498119681816\n",
      "Loss after epoch 580 : 0.02304661198483635\n",
      "Loss after epoch 590 : 0.02294004610562487\n",
      "Loss after epoch 600 : 0.022835221123441923\n",
      "Loss after epoch 610 : 0.022732064948268824\n",
      "Loss after epoch 620 : 0.022630508028907856\n",
      "Loss after epoch 630 : 0.022530486879416514\n",
      "Loss after epoch 640 : 0.022431944107153563\n",
      "Loss after epoch 650 : 0.022334818899268107\n",
      "Loss after epoch 660 : 0.022239024652626362\n",
      "Loss after epoch 670 : 0.022144473856762915\n",
      "Loss after epoch 680 : 0.022051139631608877\n",
      "Loss after epoch 690 : 0.02195874152065588\n",
      "Loss after epoch 700 : 0.021867349375400934\n",
      "Loss after epoch 710 : 0.02177766099627336\n",
      "Loss after epoch 720 : 0.02168951529855521\n",
      "Loss after epoch 730 : 0.02160273509532613\n",
      "Loss after epoch 740 : 0.021517222695643046\n",
      "Loss after epoch 750 : 0.021432904806031237\n",
      "Loss after epoch 760 : 0.02134971965514246\n",
      "Loss after epoch 770 : 0.021267619294870244\n",
      "Loss after epoch 780 : 0.021186572653434762\n",
      "Loss after epoch 790 : 0.021106560036128454\n",
      "Loss after epoch 800 : 0.02102756733853354\n",
      "Loss after epoch 810 : 0.020949581634172328\n",
      "Loss after epoch 820 : 0.020872583253774488\n",
      "Loss after epoch 830 : 0.020796544334284975\n",
      "Loss after epoch 840 : 0.02072143358150254\n",
      "Loss after epoch 850 : 0.020647220010783444\n",
      "Loss after epoch 860 : 0.02057387475768823\n",
      "Loss after epoch 870 : 0.02050137224791996\n",
      "Loss after epoch 880 : 0.020429690997907972\n",
      "Loss after epoch 890 : 0.0203588137524467\n",
      "Loss after epoch 900 : 0.020288726670880948\n",
      "Loss after epoch 910 : 0.020219417648234657\n",
      "Loss after epoch 920 : 0.020150874596947504\n",
      "Loss after epoch 930 : 0.020083084942646726\n",
      "Loss after epoch 940 : 0.020016036374617394\n",
      "Loss after epoch 950 : 0.01994971618880282\n",
      "Loss after epoch 960 : 0.019884111525953638\n",
      "Loss after epoch 970 : 0.019819219277674858\n",
      "Loss after epoch 980 : 0.01975503421987976\n",
      "Loss after epoch 990 : 0.0196915436902402\n",
      "Loss after epoch 1001 : 0.01963498524387584\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "11e669e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8606\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca28eba",
   "metadata": {},
   "source": [
    "Better accuracy than normal tanh ANN but slow to train, let's do the same with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "3d560331",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.04551065167444922\n",
      "Loss after epoch 20 : 0.0407957885722934\n",
      "Loss after epoch 30 : 0.03838111070194218\n",
      "Loss after epoch 40 : 0.036775252491078436\n",
      "Loss after epoch 50 : 0.03557719780359323\n",
      "Loss after epoch 60 : 0.034622886847097856\n",
      "Loss after epoch 70 : 0.03382450247829172\n",
      "Loss after epoch 80 : 0.033132708439367664\n",
      "Loss after epoch 90 : 0.032526286488784556\n",
      "Loss after epoch 100 : 0.0319863336788978\n",
      "Loss after epoch 110 : 0.03149484703424328\n",
      "Loss after epoch 120 : 0.03105192804294003\n",
      "Loss after epoch 130 : 0.030643495854840812\n",
      "Loss after epoch 140 : 0.03026251351529908\n",
      "Loss after epoch 150 : 0.029904785164562703\n",
      "Loss after epoch 160 : 0.029571393926833313\n",
      "Loss after epoch 170 : 0.029257107141883266\n",
      "Loss after epoch 180 : 0.02896193196497473\n",
      "Loss after epoch 190 : 0.02868209253011767\n",
      "Loss after epoch 200 : 0.028415440003546524\n",
      "Loss after epoch 210 : 0.028158692600169257\n",
      "Loss after epoch 220 : 0.027913776948015622\n",
      "Loss after epoch 230 : 0.02767899804118269\n",
      "Loss after epoch 240 : 0.027454146087506556\n",
      "Loss after epoch 250 : 0.027236032889716783\n",
      "Loss after epoch 260 : 0.027026077748524075\n",
      "Loss after epoch 270 : 0.02682130128315319\n",
      "Loss after epoch 280 : 0.026624301034773618\n",
      "Loss after epoch 290 : 0.026434763688538344\n",
      "Loss after epoch 300 : 0.026251224883640543\n",
      "Loss after epoch 310 : 0.026073894724575625\n",
      "Loss after epoch 320 : 0.025899799241725384\n",
      "Loss after epoch 330 : 0.025729862647582806\n",
      "Loss after epoch 340 : 0.02556590635652637\n",
      "Loss after epoch 350 : 0.025405535989997657\n",
      "Loss after epoch 360 : 0.02525086667422398\n",
      "Loss after epoch 370 : 0.025098162061172137\n",
      "Loss after epoch 380 : 0.02495148190019649\n",
      "Loss after epoch 390 : 0.02480879222202942\n",
      "Loss after epoch 400 : 0.024668939228146607\n",
      "Loss after epoch 410 : 0.024532396748846278\n",
      "Loss after epoch 420 : 0.024400271523632857\n",
      "Loss after epoch 430 : 0.0242725057368106\n",
      "Loss after epoch 440 : 0.024147512869711115\n",
      "Loss after epoch 450 : 0.024024285352522604\n",
      "Loss after epoch 460 : 0.023902984126855792\n",
      "Loss after epoch 470 : 0.02378486854249039\n",
      "Loss after epoch 480 : 0.023669207406098885\n",
      "Loss after epoch 490 : 0.02355509165822318\n",
      "Loss after epoch 500 : 0.023441401615503916\n",
      "Loss after epoch 510 : 0.02333152564413214\n",
      "Loss after epoch 520 : 0.023223221357183483\n",
      "Loss after epoch 530 : 0.02311755524858084\n",
      "Loss after epoch 540 : 0.02301459583313574\n",
      "Loss after epoch 550 : 0.022911083996843638\n",
      "Loss after epoch 560 : 0.022809858774939292\n",
      "Loss after epoch 570 : 0.022710329340468932\n",
      "Loss after epoch 580 : 0.02261094676564882\n",
      "Loss after epoch 590 : 0.022512717435238982\n",
      "Loss after epoch 600 : 0.02241675982552449\n",
      "Loss after epoch 610 : 0.0223220742504408\n",
      "Loss after epoch 620 : 0.022230131313682677\n",
      "Loss after epoch 630 : 0.022138249033676796\n",
      "Loss after epoch 640 : 0.022048393659476893\n",
      "Loss after epoch 650 : 0.021960213072873474\n",
      "Loss after epoch 660 : 0.02187303261438038\n",
      "Loss after epoch 670 : 0.021787804208156965\n",
      "Loss after epoch 680 : 0.02170213822951107\n",
      "Loss after epoch 690 : 0.02161792418390765\n",
      "Loss after epoch 700 : 0.02153638244320312\n",
      "Loss after epoch 710 : 0.02145449350409452\n",
      "Loss after epoch 720 : 0.02137465334888805\n",
      "Loss after epoch 730 : 0.02129599878195374\n",
      "Loss after epoch 740 : 0.021218077749096367\n",
      "Loss after epoch 750 : 0.021141587941134722\n",
      "Loss after epoch 760 : 0.021064943944654455\n",
      "Loss after epoch 770 : 0.020987981828182543\n",
      "Loss after epoch 780 : 0.020913132214916436\n",
      "Loss after epoch 790 : 0.020840350952735818\n",
      "Loss after epoch 800 : 0.02076649394403128\n",
      "Loss after epoch 810 : 0.02069556337245528\n",
      "Loss after epoch 820 : 0.020621711880931287\n",
      "Loss after epoch 830 : 0.020550683603560223\n",
      "Loss after epoch 840 : 0.020480765539506698\n",
      "Loss after epoch 850 : 0.020410822395975668\n",
      "Loss after epoch 860 : 0.020342327818281266\n",
      "Loss after epoch 870 : 0.020275347520219745\n",
      "Loss after epoch 880 : 0.020208366708695857\n",
      "Loss after epoch 890 : 0.020142946787226446\n",
      "Loss after epoch 900 : 0.020077075910310223\n",
      "Loss after epoch 910 : 0.020014008288114733\n",
      "Loss after epoch 920 : 0.019950128858518587\n",
      "Loss after epoch 930 : 0.019887101935436354\n",
      "Loss after epoch 940 : 0.01982468708537441\n",
      "Loss after epoch 950 : 0.019763603913067158\n",
      "Loss after epoch 960 : 0.019703965211797486\n",
      "Loss after epoch 970 : 0.019644761000242938\n",
      "Loss after epoch 980 : 0.01958537095089791\n",
      "Loss after epoch 990 : 0.019527885636146207\n",
      "Loss after epoch 1001 : 0.01947754674898345\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "bffaa79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8633\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8a096",
   "metadata": {},
   "source": [
    "Similar to tanh with mini batch, lets try Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0148d",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part6.4'></a>\n",
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36331021",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "c4f8768e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.07540961550600114\n",
      "Loss after epoch 20 : 0.06379622132602146\n",
      "Loss after epoch 30 : 0.05743046067665448\n",
      "Loss after epoch 40 : 0.05362365589356266\n",
      "Loss after epoch 50 : 0.051076798614497244\n",
      "Loss after epoch 60 : 0.04932787259640259\n",
      "Loss after epoch 70 : 0.04797701415674386\n",
      "Loss after epoch 80 : 0.04695245753975253\n",
      "Loss after epoch 90 : 0.046065043356619\n",
      "Loss after epoch 100 : 0.04532642482701896\n",
      "Loss after epoch 110 : 0.04465664309412634\n",
      "Loss after epoch 120 : 0.04404162733662966\n",
      "Loss after epoch 130 : 0.0434748116273695\n",
      "Loss after epoch 140 : 0.04295821380851785\n",
      "Loss after epoch 150 : 0.04251021472014239\n",
      "Loss after epoch 160 : 0.04208423235509456\n",
      "Loss after epoch 170 : 0.04169605805435618\n",
      "Loss after epoch 180 : 0.041331207668652475\n",
      "Loss after epoch 190 : 0.04098103546277455\n",
      "Loss after epoch 200 : 0.04065304416061686\n",
      "Loss after epoch 210 : 0.04034359088412875\n",
      "Loss after epoch 220 : 0.04005377867437489\n",
      "Loss after epoch 230 : 0.03977123106613451\n",
      "Loss after epoch 240 : 0.039499904568372116\n",
      "Loss after epoch 250 : 0.03924996385952121\n",
      "Loss after epoch 260 : 0.03902828066600092\n",
      "Loss after epoch 270 : 0.03880960303050625\n",
      "Loss after epoch 280 : 0.03860933114302857\n",
      "Loss after epoch 290 : 0.03840454206810971\n",
      "Loss after epoch 301 : 0.038223659247277025\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "d51c7dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8294\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb22e7",
   "metadata": {},
   "source": [
    " Faster to train, lets see relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8fa7b",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "b7cdc8c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1833568634839161\n",
      "Loss after epoch 20 : 0.10212137958706662\n",
      "Loss after epoch 30 : 0.06857828516646716\n",
      "Loss after epoch 40 : 0.05480903830273192\n",
      "Loss after epoch 50 : 0.04886009451627954\n",
      "Loss after epoch 60 : 0.04534392041862924\n",
      "Loss after epoch 70 : 0.042973871754454086\n",
      "Loss after epoch 80 : 0.041385505327800685\n",
      "Loss after epoch 90 : 0.04019432291581741\n",
      "Loss after epoch 100 : 0.039235811596445576\n",
      "Loss after epoch 110 : 0.03843753377871331\n",
      "Loss after epoch 120 : 0.03775150697198504\n",
      "Loss after epoch 130 : 0.037152268159316826\n",
      "Loss after epoch 140 : 0.03661478933500159\n",
      "Loss after epoch 150 : 0.03612215125704253\n",
      "Loss after epoch 160 : 0.03566968200889436\n",
      "Loss after epoch 170 : 0.03524993006066657\n",
      "Loss after epoch 180 : 0.03485311856834381\n",
      "Loss after epoch 190 : 0.03447427974880276\n",
      "Loss after epoch 200 : 0.034115324281767216\n",
      "Loss after epoch 210 : 0.03377870478865493\n",
      "Loss after epoch 220 : 0.03346214536007906\n",
      "Loss after epoch 230 : 0.0331675649158025\n",
      "Loss after epoch 240 : 0.03289133896151734\n",
      "Loss after epoch 250 : 0.032633904725833364\n",
      "Loss after epoch 260 : 0.03239306258032296\n",
      "Loss after epoch 270 : 0.032164263399240806\n",
      "Loss after epoch 280 : 0.03194279605134301\n",
      "Loss after epoch 290 : 0.031729895309850036\n",
      "Loss after epoch 301 : 0.03154625930612474\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "69ad19c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8574\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09459ecc",
   "metadata": {},
   "source": [
    "Better accuracy with fewer epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce33896",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part6.5'></a>\n",
    "## Adam minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94019eb",
   "metadata": {},
   "source": [
    "### relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "e0a896b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.04755057027296001\n",
      "Loss after epoch 20 : 0.04327890265109075\n",
      "Loss after epoch 30 : 0.04231382606405085\n",
      "Loss after epoch 40 : 0.04181135906903604\n",
      "Loss after epoch 50 : 0.04164756719636596\n",
      "Loss after epoch 60 : 0.04155841421945309\n",
      "Loss after epoch 70 : 0.04132807760480975\n",
      "Loss after epoch 80 : 0.04106785263355706\n",
      "Loss after epoch 90 : 0.04089326358160141\n",
      "Early stop at epoch 99, the cost is : 0.04085803353321501\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='mini_adam',\n",
    "                delta_stop = 1e-3,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "46157077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.8245\n",
      "-------------------\n",
      "Train set accuracy\n",
      "Accuracy : 0.8541166666666666\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_relu.evaluate(X_test, y_test)\n",
    "print('-------------------')\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_relu.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "a9970e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.04092390210424985\n",
      "Loss after epoch 20 : 0.04074135475133304\n",
      "Loss after epoch 30 : 0.0405719678707483\n",
      "Loss after epoch 40 : 0.040418121244498936\n",
      "Loss after epoch 51 : 0.04029139688548351\n"
     ]
    }
   ],
   "source": [
    "c = nn_relu.run(X_train, y_train_cat, epochs=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "0f034ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.822\n",
      "-------------------\n",
      "Train set accuracy\n",
      "Accuracy : 0.855\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_relu.evaluate(X_test, y_test)\n",
    "print('-------------------')\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_relu.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b502bb",
   "metadata": {},
   "source": [
    " More layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "42210667",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.03472696961272601\n",
      "Loss after epoch 20 : 0.032796321736945994\n",
      "Loss after epoch 30 : 0.031645961504183084\n",
      "Loss after epoch 40 : 0.03120391811263395\n",
      "Loss after epoch 50 : 0.030212387438725192\n",
      "Loss after epoch 60 : 0.03045856186203996\n",
      "Loss after epoch 70 : 0.02973332469236334\n",
      "Loss after epoch 80 : 0.029881910540594494\n",
      "Loss after epoch 90 : 0.02977689718864881\n",
      "Loss after epoch 100 : 0.02922987930669251\n",
      "Loss after epoch 110 : 0.02936987049656084\n",
      "Loss after epoch 120 : 0.029876535298550767\n",
      "Loss after epoch 130 : 0.029766534097635168\n",
      "Loss after epoch 140 : 0.028186863173783074\n",
      "Loss after epoch 150 : 0.028334491063287365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baraa/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/baraa/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in multiply\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 160 : nan\n",
      "Loss after epoch 170 : 0.02859773052166804\n",
      "Loss after epoch 180 : 0.02850909790568509\n",
      "Loss after epoch 190 : 0.028806619402263575\n",
      "Loss after epoch 201 : nan\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 2*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='mini_adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "fdc71293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.8403\n",
      "-------------------\n",
      "Train set accuracy\n",
      "Accuracy : 0.8896833333333334\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_relu.evaluate(X_test, y_test)\n",
    "print('-------------------')\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_relu.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee0ac8",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part7'></a>\n",
    "\n",
    "## Part 7 -  Fashion MNIST with 2 hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ddf74",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part7.1'></a>\n",
    "## Sigmoid and tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0a493",
   "metadata": {},
   "source": [
    "### sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "d05d2872",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.23575231820360967\n",
      "Loss after epoch 20 : 0.23514197222950578\n",
      "Loss after epoch 30 : 0.23456277549713792\n",
      "Loss after epoch 40 : 0.2340131676594147\n",
      "Loss after epoch 50 : 0.2334918126458501\n",
      "Loss after epoch 60 : 0.2329975243132221\n",
      "Loss after epoch 70 : 0.23252920893963105\n",
      "Loss after epoch 80 : 0.23208582158981517\n",
      "Loss after epoch 90 : 0.23166633403034356\n",
      "Loss after epoch 100 : 0.23126971259301615\n",
      "Loss after epoch 110 : 0.23089490485990805\n",
      "Loss after epoch 120 : 0.23054083418572854\n",
      "Loss after epoch 130 : 0.23020640093471084\n",
      "Loss after epoch 140 : 0.22989048903320752\n",
      "Loss after epoch 150 : 0.22959197619910057\n",
      "Loss after epoch 160 : 0.2293097461365385\n",
      "Loss after epoch 170 : 0.22904270112374725\n",
      "Loss after epoch 180 : 0.22878977373351114\n",
      "Loss after epoch 190 : 0.22854993682568195\n",
      "Loss after epoch 200 : 0.22832221135125588\n",
      "Loss after epoch 210 : 0.22810567184538882\n",
      "Loss after epoch 220 : 0.22789944973233006\n",
      "Loss after epoch 230 : 0.22770273471628039\n",
      "Loss after epoch 240 : 0.22751477460319675\n",
      "Loss after epoch 250 : 0.2273348739109998\n",
      "Loss after epoch 260 : 0.22716239160075047\n",
      "Loss after epoch 270 : 0.22699673821660735\n",
      "Loss after epoch 280 : 0.22683737267034357\n",
      "Loss after epoch 290 : 0.2266837988548725\n",
      "Loss after epoch 300 : 0.22653556222507473\n",
      "Loss after epoch 310 : 0.22639224644515585\n",
      "Loss after epoch 320 : 0.2262534701701767\n",
      "Loss after epoch 330 : 0.226118884004774\n",
      "Loss after epoch 340 : 0.22598816766349888\n",
      "Loss after epoch 350 : 0.22586102734359723\n",
      "Loss after epoch 360 : 0.22573719331141867\n",
      "Loss after epoch 370 : 0.2256164176970759\n",
      "Loss after epoch 380 : 0.22549847248772115\n",
      "Loss after epoch 390 : 0.22538314770726195\n",
      "Loss after epoch 400 : 0.22527024976901633\n",
      "Loss after epoch 410 : 0.22515959998734814\n",
      "Loss after epoch 420 : 0.22505103323444908\n",
      "Loss after epoch 430 : 0.22494439672894015\n",
      "Loss after epoch 440 : 0.22483954894370428\n",
      "Loss after epoch 450 : 0.22473635862123958\n",
      "Loss after epoch 460 : 0.22463470388574158\n",
      "Loss after epoch 470 : 0.2245344714420667\n",
      "Loss after epoch 480 : 0.22443555585263375\n",
      "Loss after epoch 490 : 0.22433785888419108\n",
      "Loss after epoch 501 : 0.22425089768208767\n"
     ]
    }
   ],
   "source": [
    "nn_sigmoid = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "                hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = sigmoid,\n",
    "                activation_hidden_2 = sigmoid\n",
    "                      )\n",
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "ea1dd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.2486\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1736cda",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "f8399be3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.2097446025613806\n",
      "Loss after epoch 20 : 0.19628579733291193\n",
      "Loss after epoch 30 : 0.18826626908880031\n",
      "Loss after epoch 40 : 0.18288840321194091\n",
      "Loss after epoch 50 : 0.17900719511921628\n",
      "Loss after epoch 60 : 0.17599469038893353\n",
      "Loss after epoch 70 : 0.17351518638051577\n",
      "Loss after epoch 80 : 0.17138786142751758\n",
      "Loss after epoch 90 : 0.16950811119412187\n",
      "Loss after epoch 100 : 0.16780958082653108\n",
      "Loss after epoch 110 : 0.1662472097930416\n",
      "Loss after epoch 120 : 0.16478933107526678\n",
      "Loss after epoch 130 : 0.16341334590672812\n",
      "Loss after epoch 140 : 0.16210300511707235\n",
      "Loss after epoch 150 : 0.16084648142351224\n",
      "Loss after epoch 160 : 0.1596349674149653\n",
      "Loss after epoch 170 : 0.15846174050379933\n",
      "Loss after epoch 180 : 0.15732155119681066\n",
      "Loss after epoch 190 : 0.15621021986263198\n",
      "Loss after epoch 200 : 0.15512437259359727\n",
      "Loss after epoch 210 : 0.15406126228511083\n",
      "Loss after epoch 220 : 0.15301864236426618\n",
      "Loss after epoch 230 : 0.1519946745464421\n",
      "Loss after epoch 240 : 0.1509878554910877\n",
      "Loss after epoch 250 : 0.14999694889113\n",
      "Loss after epoch 260 : 0.14902092007958873\n",
      "Loss after epoch 270 : 0.14805888600453393\n",
      "Loss after epoch 280 : 0.14711009037155146\n",
      "Loss after epoch 290 : 0.14617389052285007\n",
      "Loss after epoch 300 : 0.14524973781722267\n",
      "Loss after epoch 310 : 0.1443371563777443\n",
      "Loss after epoch 320 : 0.143435734084261\n",
      "Loss after epoch 330 : 0.14254512454612206\n",
      "Loss after epoch 340 : 0.14166504865702728\n",
      "Loss after epoch 350 : 0.1407952879786743\n",
      "Loss after epoch 360 : 0.13993566950896985\n",
      "Loss after epoch 370 : 0.13908604657191487\n",
      "Loss after epoch 380 : 0.1382462827503779\n",
      "Loss after epoch 390 : 0.1374162440991626\n",
      "Loss after epoch 400 : 0.13659579966984814\n",
      "Loss after epoch 410 : 0.13578482603620604\n",
      "Loss after epoch 420 : 0.13498321152074233\n",
      "Loss after epoch 430 : 0.13419085826592367\n",
      "Loss after epoch 440 : 0.13340768214826626\n",
      "Loss after epoch 450 : 0.1326336110958362\n",
      "Loss after epoch 460 : 0.13186858220656728\n",
      "Loss after epoch 470 : 0.13111253795857425\n",
      "Loss after epoch 480 : 0.1303654223041493\n",
      "Loss after epoch 490 : 0.12962717800920115\n",
      "Loss after epoch 501 : 0.12897029441421912\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "                hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = tanh,\n",
    "                activation_hidden_2 = tanh,\n",
    "                    delta_stop = 1e-3,\n",
    "                    patience=5\n",
    "                      )\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "3377b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6764\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256f333",
   "metadata": {},
   "source": [
    "Adding more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "d30cf290",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.12817706662670458\n",
      "Loss after epoch 20 : 0.12746507978690036\n",
      "Loss after epoch 30 : 0.12676172508034014\n",
      "Loss after epoch 40 : 0.12606693937640573\n",
      "Loss after epoch 50 : 0.1253806546377349\n",
      "Loss after epoch 60 : 0.12470279709450079\n",
      "Loss after epoch 70 : 0.12403328820854714\n",
      "Loss after epoch 80 : 0.12337204651940757\n",
      "Loss after epoch 90 : 0.12271898928252241\n",
      "Loss after epoch 100 : 0.12207403338627215\n",
      "Loss after epoch 110 : 0.12143709560084581\n",
      "Loss after epoch 120 : 0.12080809245533594\n",
      "Loss after epoch 130 : 0.12018694002359905\n",
      "Loss after epoch 140 : 0.11957355377715695\n",
      "Loss after epoch 150 : 0.11896784854726272\n",
      "Loss after epoch 160 : 0.11836973857523649\n",
      "Loss after epoch 170 : 0.1177791376164201\n",
      "Loss after epoch 180 : 0.1171959590734589\n",
      "Loss after epoch 190 : 0.11662011614701022\n",
      "Loss after epoch 200 : 0.11605152199610866\n",
      "Loss after epoch 210 : 0.11549008989527584\n",
      "Loss after epoch 220 : 0.11493573336393527\n",
      "Loss after epoch 230 : 0.1143883662302365\n",
      "Loss after epoch 240 : 0.1138479025833347\n",
      "Loss after epoch 250 : 0.11331425657719078\n",
      "Loss after epoch 260 : 0.11278734208617444\n",
      "Loss after epoch 270 : 0.11226707227539079\n",
      "Loss after epoch 280 : 0.11175335920996826\n",
      "Loss after epoch 290 : 0.1112461136496342\n",
      "Loss after epoch 300 : 0.11074524514247593\n",
      "Loss after epoch 310 : 0.11025066247053289\n",
      "Loss after epoch 320 : 0.10976227445067095\n",
      "Loss after epoch 330 : 0.10927999106734484\n",
      "Loss after epoch 340 : 0.10880372487099607\n",
      "Loss after epoch 350 : 0.10833339245953985\n",
      "Loss after epoch 360 : 0.10786891566236118\n",
      "Loss after epoch 370 : 0.10741022188047258\n",
      "Loss after epoch 380 : 0.1069572431239537\n",
      "Loss after epoch 390 : 0.10650991375687771\n",
      "Loss after epoch 400 : 0.10606816760189118\n",
      "Loss after epoch 410 : 0.10563193540851397\n",
      "Loss after epoch 420 : 0.10520114347590993\n",
      "Loss after epoch 430 : 0.1047757136262037\n",
      "Loss after epoch 440 : 0.10435556417463476\n",
      "Loss after epoch 450 : 0.10394061132166324\n",
      "Loss after epoch 460 : 0.1035307704880993\n",
      "Loss after epoch 470 : 0.1031259573437714\n",
      "Loss after epoch 480 : 0.10272608847921333\n",
      "Loss after epoch 490 : 0.10233108177797924\n",
      "Loss after epoch 501 : 0.10197966619980064\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "c3d299c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7535\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "b0a744b7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.10155533371680012\n",
      "Loss after epoch 20 : 0.10117443548803452\n",
      "Loss after epoch 30 : 0.10079808561175832\n",
      "Loss after epoch 40 : 0.10042620918601315\n",
      "Loss after epoch 50 : 0.10005873266907345\n",
      "Loss after epoch 60 : 0.09969558388284779\n",
      "Loss after epoch 70 : 0.09933669203071784\n",
      "Loss after epoch 80 : 0.09898198771177855\n",
      "Loss after epoch 90 : 0.0986314029076284\n",
      "Loss after epoch 100 : 0.0982848709203265\n",
      "Loss after epoch 110 : 0.09794232625419463\n",
      "Loss after epoch 120 : 0.09760370445708054\n",
      "Loss after epoch 130 : 0.09726894195758445\n",
      "Loss after epoch 140 : 0.09693797594018932\n",
      "Loss after epoch 150 : 0.09661074428552537\n",
      "Loss after epoch 160 : 0.09628718557722836\n",
      "Loss after epoch 170 : 0.09596723915539175\n",
      "Loss after epoch 180 : 0.0956508451888287\n",
      "Loss after epoch 190 : 0.09533794474308711\n",
      "Loss after epoch 200 : 0.0950284798313884\n",
      "Loss after epoch 210 : 0.09472239344509387\n",
      "Loss after epoch 220 : 0.09441962956619673\n",
      "Loss after epoch 230 : 0.09412013316678564\n",
      "Loss after epoch 240 : 0.09382385020064524\n",
      "Loss after epoch 250 : 0.09353072759133525\n",
      "Loss after epoch 260 : 0.09324071321997877\n",
      "Loss after epoch 270 : 0.09295375591493939\n",
      "Loss after epoch 280 : 0.09266980544468076\n",
      "Loss after epoch 290 : 0.09238881251435455\n",
      "Loss after epoch 300 : 0.0921107287659969\n",
      "Loss after epoch 310 : 0.09183550678157186\n",
      "Loss after epoch 320 : 0.09156310008746643\n",
      "Loss after epoch 330 : 0.09129346315843066\n",
      "Loss after epoch 340 : 0.09102655141846193\n",
      "Loss after epoch 350 : 0.09076232123588615\n",
      "Loss after epoch 360 : 0.09050072991005541\n",
      "Loss after epoch 370 : 0.09024173564780191\n",
      "Loss after epoch 380 : 0.0899852975290852\n",
      "Loss after epoch 390 : 0.08973137546300601\n",
      "Loss after epoch 400 : 0.08947993013721452\n",
      "Loss after epoch 410 : 0.08923092296527568\n",
      "Loss after epoch 420 : 0.08898431603734573\n",
      "Loss after epoch 430 : 0.08874007207931964\n",
      "Loss after epoch 440 : 0.08849815442443214\n",
      "Loss after epoch 450 : 0.08825852699944355\n",
      "Loss after epoch 460 : 0.08802115432546441\n",
      "Loss after epoch 470 : 0.08778600153167691\n",
      "Loss after epoch 480 : 0.08755303437902191\n",
      "Loss after epoch 490 : 0.08732221929045685\n",
      "Loss after epoch 501 : 0.08711629853409968\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "2fb1920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7746\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "94b5f735",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.08686691450950483\n",
      "Loss after epoch 20 : 0.08664236127581182\n",
      "Loss after epoch 30 : 0.08641983308500771\n",
      "Loss after epoch 40 : 0.08619930015355429\n",
      "Loss after epoch 50 : 0.0859807335296627\n",
      "Loss after epoch 60 : 0.08576410510129447\n",
      "Loss after epoch 70 : 0.08554938759341038\n",
      "Loss after epoch 80 : 0.0853365545529178\n",
      "Loss after epoch 90 : 0.08512558032065423\n",
      "Loss after epoch 100 : 0.08491643999114007\n",
      "Loss after epoch 110 : 0.0847091093625251\n",
      "Loss after epoch 120 : 0.08450356488073694\n",
      "Loss after epoch 130 : 0.08429978358286162\n",
      "Loss after epoch 140 : 0.08409774304490975\n",
      "Loss after epoch 150 : 0.08389742133829059\n",
      "Loss after epoch 160 : 0.08369879699776568\n",
      "Loss after epoch 170 : 0.0835018490018095\n",
      "Loss after epoch 180 : 0.08330655676458275\n",
      "Loss after epoch 190 : 0.08311290013738973\n",
      "Loss after epoch 200 : 0.08292085941658157\n",
      "Loss after epoch 210 : 0.08273041535427393\n",
      "Loss after epoch 220 : 0.08254154916784301\n",
      "Loss after epoch 230 : 0.08235424254394484\n",
      "Loss after epoch 240 : 0.0821684776329628\n",
      "Loss after epoch 250 : 0.08198423703070178\n",
      "Loss after epoch 260 : 0.08180150374612072\n",
      "Loss after epoch 270 : 0.08162026115689104\n",
      "Loss after epoch 280 : 0.08144049295792141\n",
      "Loss after epoch 290 : 0.08126218311051182\n",
      "Loss after epoch 300 : 0.0810853158003001\n",
      "Loss after epoch 310 : 0.08090987541018145\n",
      "Loss after epoch 320 : 0.0807358465105734\n",
      "Loss after epoch 330 : 0.08056321386521267\n",
      "Loss after epoch 340 : 0.08039196244757188\n",
      "Loss after epoch 350 : 0.08022207746173905\n",
      "Loss after epoch 360 : 0.08005354436210971\n",
      "Loss after epoch 370 : 0.07988634886785359\n",
      "Loss after epoch 380 : 0.07972047697008314\n",
      "Loss after epoch 390 : 0.07955591493142035\n",
      "Loss after epoch 400 : 0.07939264927894381\n",
      "Loss after epoch 410 : 0.0792306667922458\n",
      "Loss after epoch 420 : 0.07906995448857981\n",
      "Loss after epoch 430 : 0.07891049960695687\n",
      "Loss after epoch 440 : 0.078752289592672\n",
      "Loss after epoch 450 : 0.07859531208325049\n",
      "Loss after epoch 460 : 0.07843955489630461\n",
      "Loss after epoch 470 : 0.07828500601937488\n",
      "Loss after epoch 480 : 0.0781316536015347\n",
      "Loss after epoch 490 : 0.0779794859463786\n",
      "Loss after epoch 500 : 0.07782849150596402\n",
      "Loss after epoch 510 : 0.07767865887530685\n",
      "Loss after epoch 520 : 0.07752997678709955\n",
      "Loss after epoch 530 : 0.077382434106408\n",
      "Loss after epoch 540 : 0.0772360198251815\n",
      "Loss after epoch 550 : 0.07709072305648498\n",
      "Loss after epoch 560 : 0.07694653302842233\n",
      "Loss after epoch 570 : 0.07680343907777647\n",
      "Loss after epoch 580 : 0.07666143064344408\n",
      "Loss after epoch 590 : 0.07652049725979164\n",
      "Loss after epoch 600 : 0.07638062855010457\n",
      "Loss after epoch 610 : 0.07624181422033521\n",
      "Loss after epoch 620 : 0.07610404405337277\n",
      "Loss after epoch 630 : 0.07596730790405683\n",
      "Loss after epoch 640 : 0.07583159569513194\n",
      "Loss after epoch 650 : 0.07569689741429277\n",
      "Loss after epoch 660 : 0.07556320311241083\n",
      "Loss after epoch 670 : 0.07543050290296101\n",
      "Loss after epoch 680 : 0.07529878696259652\n",
      "Loss after epoch 690 : 0.07516804553276281\n",
      "Loss after epoch 700 : 0.07503826892218728\n",
      "Loss after epoch 710 : 0.07490944751005775\n",
      "Loss after epoch 720 : 0.07478157174968728\n",
      "Loss after epoch 730 : 0.07465463217246832\n",
      "Loss after epoch 740 : 0.07452861939193635\n",
      "Loss after epoch 750 : 0.07440352410778835\n",
      "Loss after epoch 760 : 0.07427933710973067\n",
      "Loss after epoch 770 : 0.07415604928106374\n",
      "Loss after epoch 780 : 0.07403365160193602\n",
      "Loss after epoch 790 : 0.07391213515222934\n",
      "Loss after epoch 800 : 0.07379149111405522\n",
      "Loss after epoch 810 : 0.07367171077385853\n",
      "Loss after epoch 820 : 0.07355278552413835\n",
      "Loss after epoch 830 : 0.0734347068648008\n",
      "Loss after epoch 840 : 0.07331746640416545\n",
      "Loss after epoch 850 : 0.07320105585964831\n",
      "Loss after epoch 860 : 0.07308546705814518\n",
      "Loss after epoch 870 : 0.07297069193613769\n",
      "Loss after epoch 880 : 0.07285672253954478\n",
      "Loss after epoch 890 : 0.07274355102333936\n",
      "Loss after epoch 900 : 0.07263116965094875\n",
      "Loss after epoch 910 : 0.07251957079345779\n",
      "Loss after epoch 920 : 0.072408746928631\n",
      "Loss after epoch 930 : 0.07229869063977076\n",
      "Loss after epoch 940 : 0.07218939461442923\n",
      "Loss after epoch 950 : 0.0720808516429906\n",
      "Loss after epoch 960 : 0.07197305461714333\n",
      "Loss after epoch 970 : 0.07186599652825972\n",
      "Loss after epoch 980 : 0.07175967046570388\n",
      "Loss after epoch 990 : 0.0716540696150866\n",
      "Loss after epoch 1001 : 0.07155964334980063\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "4aed2ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7959\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526586e",
   "metadata": {},
   "source": [
    "We move to relu now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a873247",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part7.2'></a>\n",
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "b8ba27da",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.24167439085594922\n",
      "Loss after epoch 20 : 0.2301354358418106\n",
      "Loss after epoch 30 : 0.2231941374024515\n",
      "Loss after epoch 40 : 0.2181364816182128\n",
      "Loss after epoch 50 : 0.21407890048662379\n",
      "Loss after epoch 60 : 0.21063887457444103\n",
      "Loss after epoch 70 : 0.20756028948226565\n",
      "Loss after epoch 80 : 0.2047302958459747\n",
      "Loss after epoch 90 : 0.20209503192081704\n",
      "Loss after epoch 100 : 0.19963181532084245\n",
      "Loss after epoch 110 : 0.19733904737870214\n",
      "Loss after epoch 120 : 0.19521989615178895\n",
      "Loss after epoch 130 : 0.19327934835869282\n",
      "Loss after epoch 140 : 0.19149530060427308\n",
      "Loss after epoch 150 : 0.1898319892475479\n",
      "Loss after epoch 160 : 0.1882707655189316\n",
      "Loss after epoch 170 : 0.18677398938943696\n",
      "Loss after epoch 180 : 0.18532906762595672\n",
      "Loss after epoch 190 : 0.18393535164166083\n",
      "Loss after epoch 200 : 0.1825765770434035\n",
      "Loss after epoch 210 : 0.1812515497641525\n",
      "Loss after epoch 220 : 0.17995546645588936\n",
      "Loss after epoch 230 : 0.17868959056787262\n",
      "Loss after epoch 240 : 0.1774534242485284\n",
      "Loss after epoch 250 : 0.17623677355675524\n",
      "Loss after epoch 260 : 0.17504305141759172\n",
      "Loss after epoch 270 : 0.1738777836706515\n",
      "Loss after epoch 280 : 0.1727368358343282\n",
      "Loss after epoch 290 : 0.1716160534137798\n",
      "Loss after epoch 300 : 0.17051205416311788\n",
      "Loss after epoch 310 : 0.16942305086612502\n",
      "Loss after epoch 320 : 0.16834535515966728\n",
      "Loss after epoch 330 : 0.16727966742871286\n",
      "Loss after epoch 340 : 0.16622597351413126\n",
      "Loss after epoch 350 : 0.16518245430728934\n",
      "Loss after epoch 360 : 0.16414368652834233\n",
      "Loss after epoch 370 : 0.1631105484057043\n",
      "Loss after epoch 380 : 0.16208400641502044\n",
      "Loss after epoch 390 : 0.16106274032080786\n",
      "Loss after epoch 400 : 0.16004461304462128\n",
      "Loss after epoch 410 : 0.15902963273351434\n",
      "Loss after epoch 420 : 0.15802003345623883\n",
      "Loss after epoch 430 : 0.15701784314749695\n",
      "Loss after epoch 440 : 0.15602599069842954\n",
      "Loss after epoch 450 : 0.15504666831999356\n",
      "Loss after epoch 460 : 0.15408016674861658\n",
      "Loss after epoch 470 : 0.1531320580619653\n",
      "Loss after epoch 480 : 0.15220290209435208\n",
      "Loss after epoch 490 : 0.15129659811536683\n",
      "Loss after epoch 500 : 0.1504126590212205\n",
      "Loss after epoch 510 : 0.14955251861713809\n",
      "Loss after epoch 520 : 0.14871594807053157\n",
      "Loss after epoch 530 : 0.14790236814346946\n",
      "Loss after epoch 540 : 0.14711214702085287\n",
      "Loss after epoch 550 : 0.14634389562684874\n",
      "Loss after epoch 560 : 0.1455984997987982\n",
      "Loss after epoch 570 : 0.14487361656160974\n",
      "Loss after epoch 580 : 0.14416510952680928\n",
      "Loss after epoch 590 : 0.14347243982122632\n",
      "Loss after epoch 600 : 0.14279478913715116\n",
      "Loss after epoch 610 : 0.1421304273281232\n",
      "Loss after epoch 620 : 0.14147867057111785\n",
      "Loss after epoch 630 : 0.14083911396728446\n",
      "Loss after epoch 640 : 0.14020920789070557\n",
      "Loss after epoch 650 : 0.13958782544846135\n",
      "Loss after epoch 660 : 0.13897380747172042\n",
      "Loss after epoch 670 : 0.1383690273232242\n",
      "Loss after epoch 680 : 0.13777176153914641\n",
      "Loss after epoch 690 : 0.13717941942317796\n",
      "Loss after epoch 700 : 0.13659440763991773\n",
      "Loss after epoch 710 : 0.13601662816921734\n",
      "Loss after epoch 720 : 0.1354460800775863\n",
      "Loss after epoch 730 : 0.13488207040007924\n",
      "Loss after epoch 740 : 0.1343242095155965\n",
      "Loss after epoch 750 : 0.13377269070773598\n",
      "Loss after epoch 760 : 0.13322820220618203\n",
      "Loss after epoch 770 : 0.13269087175707797\n",
      "Loss after epoch 780 : 0.13215932401438757\n",
      "Loss after epoch 790 : 0.1316337666296302\n",
      "Loss after epoch 800 : 0.13111436723143427\n",
      "Loss after epoch 810 : 0.13060073731378422\n",
      "Loss after epoch 820 : 0.13009435196575453\n",
      "Loss after epoch 830 : 0.12959425433872143\n",
      "Loss after epoch 840 : 0.12910030905226194\n",
      "Loss after epoch 850 : 0.12861286098131977\n",
      "Loss after epoch 860 : 0.12813116392431545\n",
      "Loss after epoch 870 : 0.12765505658067752\n",
      "Loss after epoch 880 : 0.12718354901829176\n",
      "Loss after epoch 890 : 0.12671844669599722\n",
      "Loss after epoch 900 : 0.1262585719935512\n",
      "Loss after epoch 910 : 0.12580366401621107\n",
      "Loss after epoch 920 : 0.12535434432963663\n",
      "Loss after epoch 930 : 0.12491006382597067\n",
      "Loss after epoch 940 : 0.12447142890324042\n",
      "Loss after epoch 950 : 0.12403766145722628\n",
      "Loss after epoch 960 : 0.12360869661628793\n",
      "Loss after epoch 970 : 0.12318406034189393\n",
      "Loss after epoch 980 : 0.12276438733722617\n",
      "Loss after epoch 990 : 0.12234852339989266\n",
      "Loss after epoch 1000 : 0.12193650925770519\n",
      "Loss after epoch 1010 : 0.12152814082245189\n",
      "Loss after epoch 1020 : 0.12112282153193354\n",
      "Loss after epoch 1030 : 0.12072200959264771\n",
      "Loss after epoch 1040 : 0.12032433974923015\n",
      "Loss after epoch 1050 : 0.11992845856885834\n",
      "Loss after epoch 1060 : 0.11953460164726835\n",
      "Loss after epoch 1070 : 0.11914357356500291\n",
      "Loss after epoch 1080 : 0.1187536475653621\n",
      "Loss after epoch 1090 : 0.11836492476339684\n",
      "Loss after epoch 1100 : 0.11797700157239842\n",
      "Loss after epoch 1110 : 0.11758804089715975\n",
      "Loss after epoch 1120 : 0.11719866554845479\n",
      "Loss after epoch 1130 : 0.11680816035329937\n",
      "Loss after epoch 1140 : 0.11641588817457071\n",
      "Loss after epoch 1150 : 0.1160205621610649\n",
      "Loss after epoch 1160 : 0.1156204435767237\n",
      "Loss after epoch 1170 : 0.11521188060058965\n",
      "Loss after epoch 1180 : 0.11479458770702482\n",
      "Loss after epoch 1190 : 0.11436907724431049\n",
      "Loss after epoch 1200 : 0.11393314660771342\n",
      "Loss after epoch 1210 : 0.11348579735462573\n",
      "Loss after epoch 1220 : 0.11302334936623074\n",
      "Loss after epoch 1230 : 0.1125409574207871\n",
      "Loss after epoch 1240 : 0.11203793619266042\n",
      "Loss after epoch 1250 : 0.11151768760275932\n",
      "Loss after epoch 1260 : 0.11097786536914163\n",
      "Loss after epoch 1270 : 0.11042006106513143\n",
      "Loss after epoch 1280 : 0.10983761982630628\n",
      "Loss after epoch 1290 : 0.10923079524896524\n",
      "Loss after epoch 1300 : 0.10861590925881068\n",
      "Loss after epoch 1310 : 0.10799503358029588\n",
      "Loss after epoch 1320 : 0.10736901098974197\n",
      "Loss after epoch 1330 : 0.10674115265778375\n",
      "Loss after epoch 1340 : 0.10612712807358372\n",
      "Loss after epoch 1350 : 0.10553164446333677\n",
      "Loss after epoch 1360 : 0.10496644797814621\n",
      "Loss after epoch 1370 : 0.10443239983795265\n",
      "Loss after epoch 1380 : 0.10393402531255677\n",
      "Loss after epoch 1390 : 0.10346890118043209\n",
      "Loss after epoch 1400 : 0.10303208853008422\n",
      "Loss after epoch 1410 : 0.10262385500368788\n",
      "Loss after epoch 1420 : 0.1022377804779111\n",
      "Loss after epoch 1430 : 0.10186987206289998\n",
      "Loss after epoch 1440 : 0.10151775926743054\n",
      "Loss after epoch 1450 : 0.10117672350839693\n",
      "Loss after epoch 1460 : 0.10084862203874148\n",
      "Loss after epoch 1470 : 0.10052920792822856\n",
      "Loss after epoch 1480 : 0.10021614663093119\n",
      "Loss after epoch 1490 : 0.09990895452609866\n",
      "Loss after epoch 1501 : 0.09963841335144132\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "                hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "                activation_hidden_2 = relu,\n",
    "                    delta_stop = 1e-3,\n",
    "                    patience=5\n",
    "                      )\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "a72681d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6534\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76fbd6",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part7.3'></a>\n",
    "## Adding more nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995b6fb",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "e7ea63c7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.19727871077340983\n",
      "Loss after epoch 20 : 0.1797146037204876\n",
      "Loss after epoch 30 : 0.16856613201922382\n",
      "Loss after epoch 40 : 0.16029112921480648\n",
      "Loss after epoch 50 : 0.1536812175620203\n",
      "Loss after epoch 60 : 0.14818258250796118\n",
      "Loss after epoch 70 : 0.14348228381080677\n",
      "Loss after epoch 80 : 0.13938248496531006\n",
      "Loss after epoch 90 : 0.13574671120546764\n",
      "Loss after epoch 100 : 0.1324754658130446\n",
      "Loss after epoch 110 : 0.12949541743173512\n",
      "Loss after epoch 120 : 0.1267528318420853\n",
      "Loss after epoch 130 : 0.12420830936102865\n",
      "Loss after epoch 140 : 0.12183258884755373\n",
      "Loss after epoch 150 : 0.11960342523714225\n",
      "Loss after epoch 160 : 0.11750340316487647\n",
      "Loss after epoch 170 : 0.11551848423374372\n",
      "Loss after epoch 180 : 0.11363707626773631\n",
      "Loss after epoch 190 : 0.11184943732532013\n",
      "Loss after epoch 200 : 0.1101472764369285\n",
      "Loss after epoch 210 : 0.10852346725660145\n",
      "Loss after epoch 220 : 0.10697183070884979\n",
      "Loss after epoch 230 : 0.10548696461488655\n",
      "Loss after epoch 240 : 0.10406410832895131\n",
      "Loss after epoch 250 : 0.1026990347372214\n",
      "Loss after epoch 260 : 0.10138796397723668\n",
      "Loss after epoch 270 : 0.1001274943934813\n",
      "Loss after epoch 280 : 0.09891454709101585\n",
      "Loss after epoch 290 : 0.09774632115014426\n",
      "Loss after epoch 300 : 0.09662025716299244\n",
      "Loss after epoch 310 : 0.09553400725688907\n",
      "Loss after epoch 320 : 0.0944854101848892\n",
      "Loss after epoch 330 : 0.09347247039826362\n",
      "Loss after epoch 340 : 0.0924933402790364\n",
      "Loss after epoch 350 : 0.09154630491328117\n",
      "Loss after epoch 360 : 0.09062976893831169\n",
      "Loss after epoch 370 : 0.08974224510891711\n",
      "Loss after epoch 380 : 0.08888234430834874\n",
      "Loss after epoch 390 : 0.08804876678685493\n",
      "Loss after epoch 400 : 0.08724029445099475\n",
      "Loss after epoch 410 : 0.0864557840562135\n",
      "Loss after epoch 420 : 0.08569416117724887\n",
      "Loss after epoch 430 : 0.08495441484856238\n",
      "Loss after epoch 440 : 0.08423559278177258\n",
      "Loss after epoch 450 : 0.08353679707990294\n",
      "Loss after epoch 460 : 0.08285718037957517\n",
      "Loss after epoch 470 : 0.08219594236230418\n",
      "Loss after epoch 480 : 0.081552326584884\n",
      "Loss after epoch 490 : 0.0809256175865787\n",
      "Loss after epoch 501 : 0.0803754743843051\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 32,\n",
    "                hidden_nodes_2 = 16,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = tanh,\n",
    "                activation_hidden_2 = tanh,\n",
    "                    delta_stop = 1e-3,\n",
    "                    patience=5\n",
    "                      )\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "61d3ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7672\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866b68c",
   "metadata": {},
   "source": [
    "More epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "e243a194",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.07972024729853144\n",
      "Loss after epoch 20 : 0.07914033716728282\n",
      "Loss after epoch 30 : 0.0785748317902998\n",
      "Loss after epoch 40 : 0.07802318472328003\n",
      "Loss after epoch 50 : 0.07748487732553021\n",
      "Loss after epoch 60 : 0.07695941707647497\n",
      "Loss after epoch 70 : 0.0764463360041399\n",
      "Loss after epoch 80 : 0.07594518921697044\n",
      "Loss after epoch 90 : 0.0754555535314702\n",
      "Loss after epoch 100 : 0.07497702618900004\n",
      "Loss after epoch 110 : 0.07450922365574028\n",
      "Loss after epoch 120 : 0.0740517805003496\n",
      "Loss after epoch 130 : 0.07360434834429265\n",
      "Loss after epoch 140 : 0.07316659488017282\n",
      "Loss after epoch 150 : 0.07273820295371744\n",
      "Loss after epoch 160 : 0.07231886970530925\n",
      "Loss after epoch 170 : 0.07190830576715841\n",
      "Loss after epoch 180 : 0.07150623451236672\n",
      "Loss after epoch 190 : 0.07111239135226927\n",
      "Loss after epoch 200 : 0.07072652307857277\n",
      "Loss after epoch 210 : 0.07034838724695786\n",
      "Loss after epoch 220 : 0.06997775159899425\n",
      "Loss after epoch 230 : 0.06961439351943416\n",
      "Loss after epoch 240 : 0.06925809952619538\n",
      "Loss after epoch 250 : 0.06890866479060916\n",
      "Loss after epoch 260 : 0.06856589268577959\n",
      "Loss after epoch 270 : 0.06822959436114877\n",
      "Loss after epoch 280 : 0.06789958834159683\n",
      "Loss after epoch 290 : 0.06757570014959575\n",
      "Loss after epoch 300 : 0.06725776194909976\n",
      "Loss after epoch 310 : 0.06694561220997444\n",
      "Loss after epoch 320 : 0.06663909539186541\n",
      "Loss after epoch 330 : 0.06633806164647163\n",
      "Loss after epoch 340 : 0.06604236653724155\n",
      "Loss after epoch 350 : 0.06575187077554522\n",
      "Loss after epoch 360 : 0.06546643997240524\n",
      "Loss after epoch 370 : 0.06518594440489382\n",
      "Loss after epoch 380 : 0.0649102587963298\n",
      "Loss after epoch 390 : 0.06463926210943523\n",
      "Loss after epoch 400 : 0.06437283735164236\n",
      "Loss after epoch 410 : 0.06411087139177954\n",
      "Loss after epoch 420 : 0.0638532547874039\n",
      "Loss after epoch 430 : 0.0635998816220987\n",
      "Loss after epoch 440 : 0.06335064935210483\n",
      "Loss after epoch 450 : 0.06310545866171625\n",
      "Loss after epoch 460 : 0.06286421332693166\n",
      "Loss after epoch 470 : 0.06262682008692333\n",
      "Loss after epoch 480 : 0.06239318852295077\n",
      "Loss after epoch 490 : 0.06216323094441633\n",
      "Loss after epoch 501 : 0.06195934001881603\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "29c754fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7982\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b586f",
   "metadata": {},
   "source": [
    "### ReLu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "93939a2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.2106830215888743\n",
      "Loss after epoch 20 : 0.17939198596363085\n",
      "Loss after epoch 30 : 0.15904410987336132\n",
      "Loss after epoch 40 : 0.14316936117323034\n",
      "Loss after epoch 50 : 0.13025364102713088\n",
      "Loss after epoch 60 : 0.11966906419487837\n",
      "Loss after epoch 70 : 0.1110705723881656\n",
      "Loss after epoch 80 : 0.10415404708274295\n",
      "Loss after epoch 90 : 0.09858374167463138\n",
      "Loss after epoch 100 : 0.09405979552849661\n",
      "Loss after epoch 110 : 0.09031930973043228\n",
      "Loss after epoch 120 : 0.08715535766310481\n",
      "Loss after epoch 130 : 0.08443868350913732\n",
      "Loss after epoch 140 : 0.08206814873060732\n",
      "Loss after epoch 150 : 0.07997979211816411\n",
      "Loss after epoch 160 : 0.0781192351456278\n",
      "Loss after epoch 170 : 0.07644836265194775\n",
      "Loss after epoch 180 : 0.07494261862426149\n",
      "Loss after epoch 190 : 0.07357579064578323\n",
      "Loss after epoch 200 : 0.07232781203258346\n",
      "Loss after epoch 210 : 0.07117991156245465\n",
      "Loss after epoch 220 : 0.07012207188077503\n",
      "Loss after epoch 230 : 0.06914208954140709\n",
      "Loss after epoch 240 : 0.06823222936571284\n",
      "Loss after epoch 250 : 0.06738658087131486\n",
      "Loss after epoch 260 : 0.06659458944712232\n",
      "Loss after epoch 270 : 0.06584831329842694\n",
      "Loss after epoch 280 : 0.0651451706960015\n",
      "Loss after epoch 290 : 0.0644801331703118\n",
      "Loss after epoch 300 : 0.06385138043293813\n",
      "Loss after epoch 310 : 0.06325626208851245\n",
      "Loss after epoch 320 : 0.06268918311084234\n",
      "Loss after epoch 330 : 0.062149004199015034\n",
      "Loss after epoch 340 : 0.061634551278931235\n",
      "Loss after epoch 350 : 0.06114284735709517\n",
      "Loss after epoch 360 : 0.060672165160510834\n",
      "Loss after epoch 370 : 0.06022078407086831\n",
      "Loss after epoch 380 : 0.05978740441479742\n",
      "Loss after epoch 390 : 0.05937189014770397\n",
      "Loss after epoch 400 : 0.05897266608556544\n",
      "Loss after epoch 410 : 0.05858965855094963\n",
      "Loss after epoch 420 : 0.05822126608022793\n",
      "Loss after epoch 430 : 0.05786720734118241\n",
      "Loss after epoch 440 : 0.05752647812813427\n",
      "Loss after epoch 450 : 0.05719744594238514\n",
      "Loss after epoch 460 : 0.05687910385426926\n",
      "Loss after epoch 470 : 0.0565701327488833\n",
      "Loss after epoch 480 : 0.05626966500006787\n",
      "Loss after epoch 490 : 0.05597875981000565\n",
      "Loss after epoch 501 : 0.05572426643845835\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 32,\n",
    "                hidden_nodes_2 = 16,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "                activation_hidden_2 = relu,\n",
    "                    delta_stop = 1e-3,\n",
    "                    patience=5\n",
    "                      )\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "11222523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7979\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357e36a",
   "metadata": {},
   "source": [
    "More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "5ef52a13",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.055422786293961024\n",
      "Loss after epoch 20 : 0.05515729367773253\n",
      "Loss after epoch 30 : 0.054900262954473075\n",
      "Loss after epoch 40 : 0.05465048183184583\n",
      "Loss after epoch 50 : 0.054407603165185704\n",
      "Loss after epoch 60 : 0.054171674753544626\n",
      "Loss after epoch 70 : 0.05394203902933873\n",
      "Loss after epoch 80 : 0.053718045068192825\n",
      "Loss after epoch 90 : 0.05349988162712563\n",
      "Loss after epoch 100 : 0.053286662726130536\n",
      "Loss after epoch 110 : 0.05307847480777914\n",
      "Loss after epoch 120 : 0.05287516463631913\n",
      "Loss after epoch 130 : 0.052676492296314416\n",
      "Loss after epoch 140 : 0.052482191776562166\n",
      "Loss after epoch 150 : 0.05229215379555113\n",
      "Loss after epoch 160 : 0.052106124072423744\n",
      "Loss after epoch 170 : 0.0519239065455751\n",
      "Loss after epoch 180 : 0.05174525901096353\n",
      "Loss after epoch 190 : 0.05156978492996211\n",
      "Loss after epoch 200 : 0.05139765975075329\n",
      "Loss after epoch 210 : 0.0512287842835003\n",
      "Loss after epoch 220 : 0.051063037806767735\n",
      "Loss after epoch 230 : 0.05089990918652707\n",
      "Loss after epoch 240 : 0.05073982973867831\n",
      "Loss after epoch 250 : 0.05058272995841043\n",
      "Loss after epoch 260 : 0.050428328585945564\n",
      "Loss after epoch 270 : 0.050276347589369115\n",
      "Loss after epoch 280 : 0.05012654315595134\n",
      "Loss after epoch 290 : 0.049979136735495235\n",
      "Loss after epoch 300 : 0.04983435238241838\n",
      "Loss after epoch 310 : 0.049692038213585914\n",
      "Loss after epoch 320 : 0.04955220235800068\n",
      "Loss after epoch 330 : 0.049414264774957625\n",
      "Loss after epoch 340 : 0.04927864386537633\n",
      "Loss after epoch 350 : 0.049145059882659624\n",
      "Loss after epoch 360 : 0.04901350327759602\n",
      "Loss after epoch 370 : 0.04888418512025403\n",
      "Loss after epoch 380 : 0.048756795716295\n",
      "Loss after epoch 390 : 0.048631229049970345\n",
      "Loss after epoch 400 : 0.04850779956002367\n",
      "Loss after epoch 410 : 0.048386703053848915\n",
      "Loss after epoch 420 : 0.048267660021214645\n",
      "Loss after epoch 430 : 0.04815029972399212\n",
      "Loss after epoch 440 : 0.048034232915656026\n",
      "Loss after epoch 450 : 0.04791967960152409\n",
      "Loss after epoch 460 : 0.047806664584186316\n",
      "Loss after epoch 470 : 0.047695032794992015\n",
      "Loss after epoch 480 : 0.04758500548508066\n",
      "Loss after epoch 490 : 0.047476570443688915\n",
      "Loss after epoch 501 : 0.04738000888903126\n"
     ]
    }
   ],
   "source": [
    "c = nn_relu.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "id": "f6b3a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8198\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc3c93",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part7.4'></a>\n",
    "## Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0489f3",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "0a2b0b59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.06920988859221543\n",
      "Loss after epoch 20 : 0.05379002979821404\n",
      "Loss after epoch 30 : 0.04722713615234914\n",
      "Loss after epoch 40 : 0.043376704728365384\n",
      "Loss after epoch 50 : 0.040770202367678185\n",
      "Loss after epoch 60 : 0.038860294305149805\n",
      "Loss after epoch 70 : 0.03736125593125599\n",
      "Loss after epoch 80 : 0.03613341806534686\n",
      "Loss after epoch 90 : 0.03510132105354712\n",
      "Loss after epoch 100 : 0.03421288749902329\n",
      "Loss after epoch 110 : 0.0334364043505347\n",
      "Loss after epoch 120 : 0.03274701076478729\n",
      "Loss after epoch 130 : 0.03213047805485106\n",
      "Loss after epoch 140 : 0.031574790306967415\n",
      "Loss after epoch 150 : 0.03107057886725876\n",
      "Loss after epoch 160 : 0.030608844724938876\n",
      "Loss after epoch 170 : 0.030181366074885786\n",
      "Loss after epoch 180 : 0.029782403871707996\n",
      "Loss after epoch 190 : 0.02940811703233811\n",
      "Loss after epoch 200 : 0.029055481797094735\n",
      "Loss after epoch 210 : 0.02872251128396035\n",
      "Loss after epoch 220 : 0.02840742951480179\n",
      "Loss after epoch 230 : 0.02810823500054036\n",
      "Loss after epoch 240 : 0.027822498336231828\n",
      "Loss after epoch 250 : 0.027547921064919875\n",
      "Loss after epoch 260 : 0.02728408598470449\n",
      "Loss after epoch 270 : 0.0270319454909777\n",
      "Loss after epoch 280 : 0.026790297080087625\n",
      "Loss after epoch 290 : 0.026558598693507338\n",
      "Loss after epoch 300 : 0.026336135891730575\n",
      "Loss after epoch 310 : 0.026122311841784825\n",
      "Loss after epoch 320 : 0.025916757349047324\n",
      "Loss after epoch 330 : 0.025718832865064004\n",
      "Loss after epoch 340 : 0.025527957977671555\n",
      "Loss after epoch 350 : 0.025343609269908113\n",
      "Loss after epoch 360 : 0.02516556145308517\n",
      "Loss after epoch 370 : 0.024993318773666173\n",
      "Loss after epoch 380 : 0.024826402665911598\n",
      "Loss after epoch 390 : 0.024664486485752864\n",
      "Loss after epoch 400 : 0.02450743499618944\n",
      "Loss after epoch 410 : 0.0243549764122658\n",
      "Loss after epoch 420 : 0.02420659735885951\n",
      "Loss after epoch 430 : 0.024062606042533657\n",
      "Loss after epoch 440 : 0.02392255641859415\n",
      "Loss after epoch 450 : 0.023786148214667055\n",
      "Loss after epoch 460 : 0.023653132353075333\n",
      "Loss after epoch 470 : 0.023523277998586545\n",
      "Loss after epoch 480 : 0.023396395900259225\n",
      "Loss after epoch 490 : 0.02327228169091458\n",
      "Loss after epoch 500 : 0.023150698879368745\n",
      "Loss after epoch 510 : 0.023031472216067307\n",
      "Loss after epoch 520 : 0.022914534081148314\n",
      "Loss after epoch 530 : 0.02279985006236651\n",
      "Loss after epoch 540 : 0.02268746247365272\n",
      "Loss after epoch 550 : 0.022577621689161716\n",
      "Loss after epoch 560 : 0.022470114531830107\n",
      "Loss after epoch 570 : 0.02236435260987024\n",
      "Loss after epoch 580 : 0.022260191449189896\n",
      "Loss after epoch 590 : 0.0221582922814011\n",
      "Loss after epoch 600 : 0.022058452669635756\n",
      "Loss after epoch 610 : 0.02195828459043226\n",
      "Loss after epoch 620 : 0.02185896639151099\n",
      "Loss after epoch 630 : 0.02176331282808727\n",
      "Loss after epoch 640 : 0.02166943502634556\n",
      "Loss after epoch 650 : 0.021577080907686286\n",
      "Loss after epoch 660 : 0.021486181443750214\n",
      "Loss after epoch 670 : 0.02139651373282275\n",
      "Loss after epoch 680 : 0.021306640155871776\n",
      "Loss after epoch 690 : 0.021219160128331794\n",
      "Loss after epoch 700 : 0.021133125490976085\n",
      "Loss after epoch 710 : 0.021048123327179074\n",
      "Loss after epoch 720 : 0.020964363137545308\n",
      "Loss after epoch 730 : 0.020882072746933395\n",
      "Loss after epoch 740 : 0.020801448605574795\n",
      "Loss after epoch 750 : 0.02072245939680985\n",
      "Loss after epoch 760 : 0.02064493235462\n",
      "Loss after epoch 770 : 0.020568708351473657\n",
      "Loss after epoch 780 : 0.02049368257826361\n",
      "Loss after epoch 790 : 0.02041979874164603\n",
      "Loss after epoch 800 : 0.02034699244341458\n",
      "Loss after epoch 810 : 0.020274876431827217\n",
      "Loss after epoch 820 : 0.020203417736157473\n",
      "Loss after epoch 830 : 0.02013328730984936\n",
      "Loss after epoch 840 : 0.020064618073368772\n",
      "Loss after epoch 850 : 0.019997118193703818\n",
      "Loss after epoch 860 : 0.019930691688995066\n",
      "Loss after epoch 870 : 0.019865268753546318\n",
      "Loss after epoch 880 : 0.019800789839740967\n",
      "Loss after epoch 890 : 0.019737201480312265\n",
      "Loss after epoch 900 : 0.019674452900575436\n",
      "Loss after epoch 910 : 0.019612492435589077\n",
      "Loss after epoch 920 : 0.019551263765382382\n",
      "Loss after epoch 930 : 0.019490703089469957\n",
      "Loss after epoch 940 : 0.019430739725229164\n",
      "Loss after epoch 950 : 0.019371301265029\n",
      "Loss after epoch 960 : 0.01931231391575637\n",
      "Loss after epoch 970 : 0.019253684220067506\n",
      "Loss after epoch 980 : 0.019195300009701426\n",
      "Loss after epoch 990 : 0.019137085542481586\n",
      "Loss after epoch 1001 : 0.01908478694892165\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 16,\n",
    "                hidden_nodes_2 = 8,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = tanh,\n",
    "                activation_hidden_2 = tanh,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "78f00dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8468\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b039c3",
   "metadata": {},
   "source": [
    "More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "42e4c49a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.019020891934576473\n",
      "Loss after epoch 20 : 0.018963615729445486\n",
      "Loss after epoch 30 : 0.018907686094280562\n",
      "Loss after epoch 40 : 0.01885279847465595\n",
      "Loss after epoch 50 : 0.01879883065217914\n",
      "Loss after epoch 60 : 0.018745665338184725\n",
      "Loss after epoch 70 : 0.018693212279887867\n",
      "Loss after epoch 80 : 0.018641389862810345\n",
      "Loss after epoch 90 : 0.018590013001725147\n",
      "Loss after epoch 100 : 0.01853744231361625\n",
      "Loss after epoch 110 : 0.018487038401415332\n",
      "Loss after epoch 120 : 0.018437234868751907\n",
      "Loss after epoch 130 : 0.018387972154104665\n",
      "Loss after epoch 140 : 0.018338980674610126\n",
      "Loss after epoch 150 : 0.018289820247467416\n",
      "Loss after epoch 160 : 0.018242177694314313\n",
      "Loss after epoch 170 : 0.018195403510155816\n",
      "Loss after epoch 180 : 0.018149204368835017\n",
      "Loss after epoch 190 : 0.01810349797833168\n",
      "Loss after epoch 200 : 0.018058245299570546\n",
      "Loss after epoch 210 : 0.018013413575097257\n",
      "Loss after epoch 220 : 0.017968975369529617\n",
      "Loss after epoch 230 : 0.01792493066522521\n",
      "Loss after epoch 240 : 0.017881303897141605\n",
      "Loss after epoch 250 : 0.01783810929291968\n",
      "Loss after epoch 260 : 0.017795340626464533\n",
      "Loss after epoch 270 : 0.01775298440488827\n",
      "Loss after epoch 280 : 0.017711026133094495\n",
      "Loss after epoch 290 : 0.01766944749445524\n",
      "Loss after epoch 300 : 0.01762822413670755\n",
      "Loss after epoch 310 : 0.017587323413662898\n",
      "Loss after epoch 320 : 0.01754671919508892\n",
      "Loss after epoch 330 : 0.017506464030356456\n",
      "Loss after epoch 340 : 0.017466585809318997\n",
      "Loss after epoch 350 : 0.0174270829471636\n",
      "Loss after epoch 360 : 0.017387965547507947\n",
      "Loss after epoch 370 : 0.01734925375739945\n",
      "Loss after epoch 380 : 0.017310955427098186\n",
      "Loss after epoch 390 : 0.01727305884845099\n",
      "Loss after epoch 400 : 0.01723554115667969\n",
      "Loss after epoch 410 : 0.017198376439000868\n",
      "Loss after epoch 420 : 0.01716153857221955\n",
      "Loss after epoch 430 : 0.017125000404754724\n",
      "Loss after epoch 440 : 0.017088730036229443\n",
      "Loss after epoch 450 : 0.017052681667654473\n",
      "Loss after epoch 460 : 0.017016772662695537\n",
      "Loss after epoch 470 : 0.016980839253041426\n",
      "Loss after epoch 480 : 0.01694471242634897\n",
      "Loss after epoch 490 : 0.016908713444914212\n",
      "Loss after epoch 500 : 0.01687321055174958\n",
      "Loss after epoch 510 : 0.016838120095582113\n",
      "Loss after epoch 520 : 0.016803335538513455\n",
      "Loss after epoch 530 : 0.016768793717820467\n",
      "Loss after epoch 540 : 0.016734453089391896\n",
      "Loss after epoch 550 : 0.016700286280168123\n",
      "Loss after epoch 560 : 0.01666628309798161\n",
      "Loss after epoch 570 : 0.01663247212015825\n",
      "Loss after epoch 580 : 0.0165989297300485\n",
      "Loss after epoch 590 : 0.016565714476029916\n",
      "Loss after epoch 600 : 0.016532825099597206\n",
      "Loss after epoch 610 : 0.016500241837721873\n",
      "Loss after epoch 620 : 0.01646794972449185\n",
      "Loss after epoch 630 : 0.016435935247458636\n",
      "Loss after epoch 640 : 0.016404186677956487\n",
      "Loss after epoch 650 : 0.016372693978172734\n",
      "Loss after epoch 660 : 0.01634144619813658\n",
      "Loss after epoch 670 : 0.016310431634715353\n",
      "Loss after epoch 680 : 0.01627963925122533\n",
      "Loss after epoch 690 : 0.01624905983781696\n",
      "Loss after epoch 700 : 0.01621868677960569\n",
      "Loss after epoch 710 : 0.016188516243447606\n",
      "Loss after epoch 720 : 0.01615854661387393\n",
      "Loss after epoch 730 : 0.016128777614104365\n",
      "Loss after epoch 740 : 0.016099210109281552\n",
      "Loss after epoch 750 : 0.016069846693282003\n",
      "Loss after epoch 760 : 0.016040691003909473\n",
      "Loss after epoch 770 : 0.01601174424167165\n",
      "Loss after epoch 780 : 0.015983001385510065\n",
      "Loss after epoch 790 : 0.015954448977690803\n",
      "Loss after epoch 800 : 0.015926058909786107\n",
      "Loss after epoch 810 : 0.015897761275310243\n",
      "Loss after epoch 820 : 0.0158694549163686\n",
      "Loss after epoch 830 : 0.01584126279110996\n",
      "Loss after epoch 840 : 0.015813312222986854\n",
      "Loss after epoch 850 : 0.01578557923412807\n",
      "Loss after epoch 860 : 0.015758038278235017\n",
      "Loss after epoch 870 : 0.01573066766435764\n",
      "Loss after epoch 880 : 0.015703416714273714\n",
      "Loss after epoch 890 : 0.015676072049238245\n",
      "Loss after epoch 900 : 0.015648927690522268\n",
      "Loss after epoch 910 : 0.015622135274876553\n",
      "Loss after epoch 920 : 0.015595529601902965\n",
      "Loss after epoch 930 : 0.01556908103853759\n",
      "Loss after epoch 940 : 0.015542778731499016\n",
      "Loss after epoch 950 : 0.015516615742845257\n",
      "Loss after epoch 960 : 0.015490585773572521\n",
      "Loss after epoch 970 : 0.015464682032229733\n",
      "Loss after epoch 980 : 0.01543889916287928\n",
      "Loss after epoch 990 : 0.015413240251982997\n",
      "Loss after epoch 1001 : 0.015390263209062484\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "6215503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8372\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "cd57b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9524833333333333\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58215bd4",
   "metadata": {},
   "source": [
    "Accuracy went down, we are overfitting. Back at  ReLu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d76d15",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "c0f8e4ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.05177909625908198\n",
      "Loss after epoch 20 : 0.0443501633672035\n",
      "Loss after epoch 30 : 0.040950448805973774\n",
      "Loss after epoch 40 : 0.03882887086675694\n",
      "Loss after epoch 50 : 0.03731279051214495\n",
      "Loss after epoch 60 : 0.03613102220038927\n",
      "Loss after epoch 70 : 0.035163296250973954\n",
      "Loss after epoch 80 : 0.034336701624126865\n",
      "Loss after epoch 90 : 0.03361460882884629\n",
      "Loss after epoch 100 : 0.03298027113491394\n",
      "Loss after epoch 110 : 0.032423884056834024\n",
      "Loss after epoch 120 : 0.031917016030691255\n",
      "Loss after epoch 130 : 0.03145603189726815\n",
      "Loss after epoch 140 : 0.03103099124736117\n",
      "Loss after epoch 150 : 0.030635345030684435\n",
      "Loss after epoch 160 : 0.030269651365932508\n",
      "Loss after epoch 170 : 0.029929497945016637\n",
      "Loss after epoch 180 : 0.02960954288919598\n",
      "Loss after epoch 190 : 0.029305847646692133\n",
      "Loss after epoch 200 : 0.029014195590523033\n",
      "Loss after epoch 210 : 0.02873472185941939\n",
      "Loss after epoch 220 : 0.028472668363352237\n",
      "Loss after epoch 230 : 0.028219930800997953\n",
      "Loss after epoch 240 : 0.02797863313902341\n",
      "Loss after epoch 250 : 0.027746388832011505\n",
      "Loss after epoch 260 : 0.02752445972755519\n",
      "Loss after epoch 270 : 0.027307810188620937\n",
      "Loss after epoch 280 : 0.027097815480367417\n",
      "Loss after epoch 290 : 0.02689731304841613\n",
      "Loss after epoch 300 : 0.026702125623781214\n",
      "Loss after epoch 310 : 0.02651614229160302\n",
      "Loss after epoch 320 : 0.026333236131902617\n",
      "Loss after epoch 330 : 0.02615700615182702\n",
      "Loss after epoch 340 : 0.02598733944694935\n",
      "Loss after epoch 350 : 0.025821277008370152\n",
      "Loss after epoch 360 : 0.025660536004121076\n",
      "Loss after epoch 370 : 0.025501684776679235\n",
      "Loss after epoch 380 : 0.025349536925562063\n",
      "Loss after epoch 390 : 0.025201639242394422\n",
      "Loss after epoch 400 : 0.025059795216847567\n",
      "Loss after epoch 410 : 0.024921197727843483\n",
      "Loss after epoch 420 : 0.02478688284877135\n",
      "Loss after epoch 430 : 0.024654684936070932\n",
      "Loss after epoch 440 : 0.02452780425869729\n",
      "Loss after epoch 450 : 0.024403293724217543\n",
      "Loss after epoch 460 : 0.02428296278302772\n",
      "Loss after epoch 470 : 0.02416443110632063\n",
      "Loss after epoch 480 : 0.024048439598670764\n",
      "Loss after epoch 490 : 0.02393789592145964\n",
      "Loss after epoch 500 : 0.02382853478689078\n",
      "Loss after epoch 510 : 0.023718493310296435\n",
      "Loss after epoch 520 : 0.023608685276579264\n",
      "Loss after epoch 530 : 0.023508771670395626\n",
      "Loss after epoch 540 : 0.023408359723844207\n",
      "Loss after epoch 550 : 0.02330587864421602\n",
      "Loss after epoch 560 : 0.023207062526305274\n",
      "Loss after epoch 570 : 0.02310692661292129\n",
      "Loss after epoch 580 : 0.023010593964935037\n",
      "Loss after epoch 590 : 0.02291801158590605\n",
      "Loss after epoch 600 : 0.022826626163286293\n",
      "Loss after epoch 610 : 0.022734561882280974\n",
      "Loss after epoch 620 : 0.022646959453213572\n",
      "Loss after epoch 630 : 0.022559721204591772\n",
      "Loss after epoch 640 : 0.022474919307488645\n",
      "Loss after epoch 650 : 0.0223944075224972\n",
      "Loss after epoch 660 : 0.022310488606961106\n",
      "Loss after epoch 670 : 0.022232422897332602\n",
      "Loss after epoch 680 : 0.022153269670757342\n",
      "Loss after epoch 690 : 0.022076232232900984\n",
      "Loss after epoch 700 : 0.022000332729454074\n",
      "Loss after epoch 710 : 0.021928147146322362\n",
      "Loss after epoch 720 : 0.021855857043901444\n",
      "Loss after epoch 730 : 0.0217846089091813\n",
      "Loss after epoch 740 : 0.0217159463158957\n",
      "Loss after epoch 750 : 0.02164717673643766\n",
      "Loss after epoch 760 : 0.021581130752717927\n",
      "Loss after epoch 770 : 0.021517143289665974\n",
      "Loss after epoch 780 : 0.021454830598581193\n",
      "Loss after epoch 790 : 0.021394453613003\n",
      "Loss after epoch 800 : 0.021332668556308526\n",
      "Loss after epoch 810 : 0.021270105128018208\n",
      "Loss after epoch 820 : 0.021210098814236966\n",
      "Loss after epoch 830 : 0.02115054620303972\n",
      "Loss after epoch 840 : 0.021091756727530178\n",
      "Loss after epoch 850 : 0.021036931519593786\n",
      "Loss after epoch 860 : 0.020977724982041233\n",
      "Loss after epoch 870 : 0.02092022662314935\n",
      "Loss after epoch 880 : 0.02086678944640413\n",
      "Loss after epoch 890 : 0.020811846154970864\n",
      "Loss after epoch 900 : 0.02075883616448094\n",
      "Loss after epoch 910 : 0.020704768519889685\n",
      "Loss after epoch 920 : 0.020651331643774713\n",
      "Loss after epoch 930 : 0.020600594199135534\n",
      "Loss after epoch 940 : 0.020546319385516806\n",
      "Loss after epoch 950 : 0.02049774576046974\n",
      "Loss after epoch 960 : 0.020445659395478103\n",
      "Loss after epoch 970 : 0.02039561874718478\n",
      "Loss after epoch 980 : 0.020347507128544648\n",
      "Loss after epoch 990 : 0.020295697199217543\n",
      "Loss after epoch 1001 : 0.02025170204729033\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 16,\n",
    "                hidden_nodes_2 = 8,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "                activation_hidden_2 = relu,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "571a8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8505\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "545ac0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9524833333333333\n"
     ]
    }
   ],
   "source": [
    "#train set\n",
    "acc = nn_tanh.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed950a5",
   "metadata": {},
   "source": [
    "85% accuracy nice, adding more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "8d5230de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.020201194120543095\n",
      "Loss after epoch 20 : 0.020153698093367266\n",
      "Loss after epoch 30 : 0.020110659825287557\n",
      "Loss after epoch 40 : 0.02006570669663285\n",
      "Loss after epoch 50 : 0.020018380289486003\n",
      "Loss after epoch 60 : 0.019972653073159208\n",
      "Loss after epoch 70 : 0.019926610809173777\n",
      "Loss after epoch 80 : 0.019881981325148596\n",
      "Loss after epoch 90 : 0.019838914086045815\n",
      "Loss after epoch 100 : 0.019791553490276243\n",
      "Loss after epoch 110 : 0.019751618456815823\n",
      "Loss after epoch 120 : 0.019703599056252515\n",
      "Loss after epoch 130 : 0.01966425787919436\n",
      "Loss after epoch 140 : 0.0196270150978737\n",
      "Loss after epoch 150 : 0.019577903076698225\n",
      "Loss after epoch 160 : 0.019540034121426266\n",
      "Loss after epoch 170 : 0.019496932350959065\n",
      "Loss after epoch 180 : 0.01945953465356411\n",
      "Loss after epoch 190 : 0.01941672499985153\n",
      "Loss after epoch 200 : 0.019379156806976796\n",
      "Loss after epoch 210 : 0.01933688913898402\n",
      "Loss after epoch 220 : 0.019301787878080172\n",
      "Loss after epoch 230 : 0.01926035335628501\n",
      "Loss after epoch 240 : 0.0192205563590515\n",
      "Loss after epoch 250 : 0.019182522238094902\n",
      "Loss after epoch 260 : 0.019142968143717975\n",
      "Loss after epoch 270 : 0.01910485641785248\n",
      "Loss after epoch 280 : 0.01906891052130147\n",
      "Loss after epoch 290 : 0.01903485234319574\n",
      "Loss after epoch 301 : 0.018997299238857582\n"
     ]
    }
   ],
   "source": [
    "c = nn_relu.run(X_train, y_train_cat, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "a76aadaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8473\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "acc = nn_relu.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "7e669236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9323833333333333\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596142e",
   "metadata": {},
   "source": [
    "We are overfitting, we move to adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cc27e",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "<a id='part7.5'></a>\n",
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd43d29",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "aa2332fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.15120156129746481\n",
      "Loss after epoch 20 : 0.12344157871765024\n",
      "Loss after epoch 30 : 0.10796038945177726\n",
      "Loss after epoch 40 : 0.09860091744414928\n",
      "Loss after epoch 50 : 0.09214218800928618\n",
      "Loss after epoch 60 : 0.0876846245683957\n",
      "Loss after epoch 70 : 0.0845057619473858\n",
      "Loss after epoch 80 : 0.08206823272193402\n",
      "Loss after epoch 90 : 0.0802398833226977\n",
      "Loss after epoch 100 : 0.07876693344015889\n",
      "Loss after epoch 110 : 0.07731599822976618\n",
      "Loss after epoch 120 : 0.07603273756910556\n",
      "Loss after epoch 130 : 0.07475703292586805\n",
      "Loss after epoch 140 : 0.07362804411815349\n",
      "Loss after epoch 150 : 0.07267933569826293\n",
      "Loss after epoch 160 : 0.07177440931183142\n",
      "Loss after epoch 170 : 0.07094275737968961\n",
      "Loss after epoch 180 : 0.07020051175366927\n",
      "Loss after epoch 190 : 0.06960172860887255\n",
      "Loss after epoch 200 : 0.06906587035414596\n",
      "Loss after epoch 210 : 0.06858409176668022\n",
      "Loss after epoch 220 : 0.06819064273062508\n",
      "Loss after epoch 230 : 0.06781998329213623\n",
      "Loss after epoch 240 : 0.06742264693193722\n",
      "Loss after epoch 250 : 0.06706909343744893\n",
      "Loss after epoch 260 : 0.06676726051034398\n",
      "Loss after epoch 270 : 0.0664853492706584\n",
      "Loss after epoch 280 : 0.0662045890092282\n",
      "Loss after epoch 290 : 0.0659433371095684\n",
      "Loss after epoch 300 : 0.06568053265532081\n",
      "Loss after epoch 310 : 0.06543796337581882\n",
      "Loss after epoch 320 : 0.06517075021740067\n",
      "Loss after epoch 330 : 0.06493448498483448\n",
      "Loss after epoch 340 : 0.06475435819880612\n",
      "Loss after epoch 350 : 0.06445845237492374\n",
      "Loss after epoch 360 : 0.06417247101414786\n",
      "Loss after epoch 370 : 0.06391813881019247\n",
      "Loss after epoch 380 : 0.06365504052088958\n",
      "Loss after epoch 390 : 0.06339440831867085\n",
      "Loss after epoch 400 : 0.06317242923188032\n",
      "Loss after epoch 410 : 0.06297390699424728\n",
      "Loss after epoch 420 : 0.06276851279805681\n",
      "Loss after epoch 430 : 0.0625604779811044\n",
      "Loss after epoch 440 : 0.06237653426888516\n",
      "Loss after epoch 450 : 0.06216818975556446\n",
      "Loss after epoch 460 : 0.06194606633588441\n",
      "Loss after epoch 470 : 0.06174743077824929\n",
      "Loss after epoch 480 : 0.06168562438452624\n",
      "Loss after epoch 490 : 0.06138408940609974\n",
      "Loss after epoch 501 : 0.06118095395042031\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 8,\n",
    "                hidden_nodes_2 = 4,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = tanh,\n",
    "                activation_hidden_2 = tanh,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "fdc94f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.7535\n",
      "Train set accuracy\n",
      "Accuracy : 0.7739666666666667\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_tanh.evaluate(X_test, y_test)\n",
    "\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_tanh.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c26bf",
   "metadata": {},
   "source": [
    "### ReLu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "562d26be",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.19714237393683834\n",
      "Loss after epoch 20 : 0.15054508633394986\n",
      "Loss after epoch 30 : 0.12726420409545686\n",
      "Loss after epoch 40 : 0.11243340295745523\n",
      "Loss after epoch 50 : 0.10173779282827461\n",
      "Loss after epoch 60 : 0.09488255307254606\n",
      "Loss after epoch 70 : 0.09004251326594795\n",
      "Loss after epoch 80 : 0.08609865198351713\n",
      "Loss after epoch 90 : 0.08160952241434824\n",
      "Loss after epoch 100 : 0.07578751257633969\n",
      "Loss after epoch 110 : 0.0713064447708047\n",
      "Loss after epoch 120 : 0.0679493672736777\n",
      "Loss after epoch 130 : 0.06535787262023415\n",
      "Loss after epoch 140 : 0.0633568245949392\n",
      "Loss after epoch 150 : 0.06176235885235458\n",
      "Loss after epoch 160 : 0.06046926306047273\n",
      "Loss after epoch 170 : 0.05939327669326018\n",
      "Loss after epoch 180 : 0.058484989552817966\n",
      "Loss after epoch 190 : 0.05771902040051211\n",
      "Loss after epoch 200 : 0.057052894244009306\n",
      "Loss after epoch 210 : 0.05647270813844721\n",
      "Loss after epoch 220 : 0.055939599818752433\n",
      "Loss after epoch 230 : 0.055453993580473156\n",
      "Loss after epoch 240 : 0.05500002204819892\n",
      "Loss after epoch 250 : 0.0545675606978347\n",
      "Loss after epoch 260 : 0.05415552009562481\n",
      "Loss after epoch 270 : 0.05376345688132617\n",
      "Loss after epoch 280 : 0.05337990424560896\n",
      "Loss after epoch 290 : 0.052996953890159994\n",
      "Loss after epoch 300 : 0.05260610372013598\n",
      "Loss after epoch 310 : 0.05218132547384565\n",
      "Loss after epoch 320 : 0.05169493762732227\n",
      "Loss after epoch 330 : 0.05113635311196272\n",
      "Loss after epoch 340 : 0.05049426361478579\n",
      "Loss after epoch 350 : 0.049765324495872\n",
      "Loss after epoch 360 : 0.04905380312614529\n",
      "Loss after epoch 370 : 0.04836988661372606\n",
      "Loss after epoch 380 : 0.047776483981234075\n",
      "Loss after epoch 390 : 0.04725236084612903\n",
      "Loss after epoch 400 : 0.04672515171761335\n",
      "Loss after epoch 410 : 0.046143136221872307\n",
      "Loss after epoch 420 : 0.04535029088460384\n",
      "Loss after epoch 430 : 0.04420139099694923\n",
      "Loss after epoch 440 : 0.0434206329122453\n",
      "Loss after epoch 450 : 0.04272551524537215\n",
      "Loss after epoch 460 : 0.04218589521133186\n",
      "Loss after epoch 470 : 0.041731780226700196\n",
      "Loss after epoch 480 : 0.04133974482999701\n",
      "Loss after epoch 490 : 0.04098869279679084\n",
      "Loss after epoch 500 : 0.0406682783346371\n",
      "Loss after epoch 510 : 0.040372158615132735\n",
      "Loss after epoch 520 : 0.04008878239330165\n",
      "Loss after epoch 530 : 0.039823371223366676\n",
      "Loss after epoch 540 : 0.03961350498444202\n",
      "Loss after epoch 550 : 0.03931413905756967\n",
      "Loss after epoch 560 : 0.039069456797447444\n",
      "Loss after epoch 570 : 0.03881295483328292\n",
      "Loss after epoch 580 : 0.03856441451143808\n",
      "Loss after epoch 590 : 0.038325825782494466\n",
      "Loss after epoch 600 : 0.03810123443016678\n",
      "Loss after epoch 610 : 0.03788336586023297\n",
      "Loss after epoch 620 : 0.03764635489104002\n",
      "Loss after epoch 630 : 0.037403752446753154\n",
      "Loss after epoch 640 : 0.037167172676853245\n",
      "Loss after epoch 650 : 0.036945702286463274\n",
      "Loss after epoch 660 : 0.036734523836916354\n",
      "Loss after epoch 670 : 0.03654460969143972\n",
      "Loss after epoch 680 : 0.036356534726581215\n",
      "Loss after epoch 690 : 0.03617748523209881\n",
      "Loss after epoch 700 : 0.036008010095350484\n",
      "Loss after epoch 710 : 0.03584201235412106\n",
      "Loss after epoch 720 : 0.03567889535367039\n",
      "Loss after epoch 730 : 0.035521376294835524\n",
      "Loss after epoch 740 : 0.03537906622277603\n",
      "Loss after epoch 750 : 0.03521909106778782\n",
      "Loss after epoch 760 : 0.035096560671898126\n",
      "Loss after epoch 770 : 0.03493027463450514\n",
      "Loss after epoch 780 : 0.03479672403247411\n",
      "Loss after epoch 790 : 0.03465285447646678\n",
      "Loss after epoch 800 : 0.034505167187255915\n",
      "Loss after epoch 810 : 0.03437210477306326\n",
      "Loss after epoch 820 : 0.034226086840411944\n",
      "Loss after epoch 830 : 0.03408793406072687\n",
      "Loss after epoch 840 : 0.03397855876674817\n",
      "Loss after epoch 850 : 0.033821323040079676\n",
      "Loss after epoch 860 : 0.0336775492621845\n",
      "Loss after epoch 870 : 0.033595749634858486\n",
      "Loss after epoch 880 : 0.033418613495649556\n",
      "Loss after epoch 890 : 0.03329152891800035\n",
      "Loss after epoch 900 : 0.0331922947001859\n",
      "Loss after epoch 910 : 0.03310613850773969\n",
      "Loss after epoch 920 : 0.03295762532705547\n",
      "Loss after epoch 930 : 0.03283117253053964\n",
      "Loss after epoch 940 : 0.03282393481471606\n",
      "Loss after epoch 950 : 0.032683252237196\n",
      "Loss after epoch 960 : 0.03252966975476138\n",
      "Loss after epoch 970 : 0.03238233838717525\n",
      "Loss after epoch 980 : 0.03240545905721773\n",
      "Loss after epoch 990 : 0.03224164487504021\n",
      "Loss after epoch 1001 : 0.03209374878950175\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 16,\n",
    "                hidden_nodes_2 = 8,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "                activation_hidden_2 = relu,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "9b0a4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.8466\n",
      "-------------------\n",
      "Train set accuracy\n",
      "Accuracy : 0.8896\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_relu.evaluate(X_test, y_test)\n",
    "print('-------------------')\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_relu.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cea5c6",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part8'></a>\n",
    "\n",
    "## Part 8 -  Best model analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527ef8fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.04607803665487075\n",
      "Loss after epoch 20 : 0.04104042821755706\n",
      "Loss after epoch 30 : 0.038521107876168324\n",
      "Loss after epoch 40 : 0.03687437226356114\n",
      "Loss after epoch 50 : 0.03566473378327373\n",
      "Loss after epoch 60 : 0.034708709830504655\n",
      "Loss after epoch 70 : 0.033923364927821054\n",
      "Loss after epoch 80 : 0.03325601077676521\n",
      "Loss after epoch 90 : 0.032674283541531686\n",
      "Loss after epoch 100 : 0.03216074830328571\n",
      "Loss after epoch 110 : 0.03169910679698007\n",
      "Loss after epoch 120 : 0.031275072198433534\n",
      "Loss after epoch 130 : 0.030883990976763212\n",
      "Loss after epoch 140 : 0.03052078433427354\n",
      "Loss after epoch 150 : 0.030184239973228\n",
      "Loss after epoch 160 : 0.02986566277574556\n",
      "Loss after epoch 170 : 0.02956283855493977\n",
      "Loss after epoch 180 : 0.029277423265509823\n",
      "Loss after epoch 190 : 0.029005499104993723\n",
      "Loss after epoch 200 : 0.028744801380934198\n",
      "Loss after epoch 210 : 0.02849044180108226\n",
      "Loss after epoch 220 : 0.02824605189139131\n",
      "Loss after epoch 230 : 0.028011371716131005\n",
      "Loss after epoch 240 : 0.027785093555899202\n",
      "Loss after epoch 250 : 0.027567839312602278\n",
      "Loss after epoch 260 : 0.027357692299944076\n",
      "Loss after epoch 270 : 0.027154115265983946\n",
      "Loss after epoch 280 : 0.026960377633874055\n",
      "Loss after epoch 290 : 0.026771635345263705\n",
      "Loss after epoch 300 : 0.026587438548501797\n",
      "Loss after epoch 310 : 0.026410132689011257\n",
      "Loss after epoch 320 : 0.026236702820054674\n",
      "Loss after epoch 330 : 0.02606724583932638\n",
      "Loss after epoch 340 : 0.025902689921069495\n",
      "Loss after epoch 350 : 0.0257410276203514\n",
      "Loss after epoch 360 : 0.025583925144547697\n",
      "Loss after epoch 370 : 0.025429632729698504\n",
      "Loss after epoch 380 : 0.025279588766098527\n",
      "Loss after epoch 390 : 0.02513490565376109\n",
      "Loss after epoch 400 : 0.02499313344404534\n",
      "Loss after epoch 410 : 0.02485470570037955\n",
      "Loss after epoch 420 : 0.02472075475260909\n",
      "Loss after epoch 430 : 0.02458699386269366\n",
      "Loss after epoch 440 : 0.02445816472280314\n",
      "Loss after epoch 450 : 0.024331140907911313\n",
      "Loss after epoch 460 : 0.024208484406357544\n",
      "Loss after epoch 470 : 0.02408806293100579\n",
      "Loss after epoch 480 : 0.023967674025157308\n",
      "Loss after epoch 490 : 0.023850469319229895\n",
      "Loss after epoch 500 : 0.023735596324511974\n",
      "Loss after epoch 510 : 0.023620390313886866\n",
      "Loss after epoch 520 : 0.023508566513765563\n",
      "Loss after epoch 530 : 0.023397907719577123\n",
      "Loss after epoch 540 : 0.023290811515269436\n",
      "Loss after epoch 550 : 0.02318448086264002\n",
      "Loss after epoch 560 : 0.023080105932302614\n",
      "Loss after epoch 570 : 0.022977133772330682\n",
      "Loss after epoch 580 : 0.022876205667202408\n",
      "Loss after epoch 590 : 0.02277734101527944\n",
      "Loss after epoch 600 : 0.02268020327981046\n",
      "Loss after epoch 610 : 0.022585244831749854\n",
      "Loss after epoch 620 : 0.022489626157540788\n",
      "Loss after epoch 630 : 0.022397401872518898\n",
      "Loss after epoch 640 : 0.022308057013619716\n",
      "Loss after epoch 650 : 0.022217529845984116\n",
      "Loss after epoch 660 : 0.02212969754021347\n",
      "Loss after epoch 670 : 0.02204319355362901\n",
      "Loss after epoch 680 : 0.02195789065732894\n",
      "Loss after epoch 690 : 0.021874421411427918\n",
      "Loss after epoch 700 : 0.021788445648754394\n",
      "Loss after epoch 710 : 0.021703602307812122\n",
      "Loss after epoch 720 : 0.021622065525658915\n",
      "Loss after epoch 730 : 0.021539610150945635\n",
      "Loss after epoch 740 : 0.02146006460191273\n",
      "Loss after epoch 750 : 0.02138018233394781\n",
      "Loss after epoch 760 : 0.021302890747833512\n",
      "Loss after epoch 770 : 0.02122791677756342\n",
      "Loss after epoch 780 : 0.021155628502708006\n",
      "Loss after epoch 790 : 0.02108379400804501\n",
      "Loss after epoch 800 : 0.021010701822663406\n",
      "Loss after epoch 810 : 0.0209383759031974\n",
      "Loss after epoch 820 : 0.020869081269854855\n",
      "Loss after epoch 830 : 0.020798012629338877\n",
      "Loss after epoch 840 : 0.020729674068371787\n",
      "Loss after epoch 850 : 0.020662517521301573\n",
      "Loss after epoch 860 : 0.020593591690907957\n",
      "Loss after epoch 870 : 0.02052811400339891\n",
      "Loss after epoch 880 : 0.02046264506550694\n",
      "Loss after epoch 890 : 0.020395947325835127\n",
      "Loss after epoch 900 : 0.02033488811668684\n",
      "Loss after epoch 910 : 0.02027045841742732\n",
      "Loss after epoch 920 : 0.020208711725666417\n",
      "Loss after epoch 930 : 0.020147360455685694\n",
      "Loss after epoch 940 : 0.02008482682967949\n",
      "Loss after epoch 950 : 0.020024694307787194\n",
      "Loss after epoch 960 : 0.019964908521037023\n",
      "Loss after epoch 970 : 0.0199050822779085\n",
      "Loss after epoch 980 : 0.01984510815689701\n",
      "Loss after epoch 990 : 0.01978728782007546\n",
      "Loss after 4epoch 1001 : 0.019734673329759596\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2711fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy\n",
      "Accuracy : 0.859\n",
      "-------------------\n",
      "Train set accuracy\n",
      "Accuracy : 0.9322833333333334\n"
     ]
    }
   ],
   "source": [
    "# Test set \n",
    "print('Test set accuracy')\n",
    "acc = nn_relu.evaluate(X_test, y_test)\n",
    "print('-------------------')\n",
    "# Train set \n",
    "print('Train set accuracy')\n",
    "acc = nn_relu.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a3418",
   "metadata": {},
   "source": [
    "### Fashion MNIST labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de365d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "labels = '''T-shirt/top\n",
    "Trouser\n",
    "Pullover\n",
    "Dress\n",
    "Coat\n",
    "Sandal\n",
    "Shirt\n",
    "Sneaker\n",
    "Bag\n",
    "Ankle boot'''.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962aeeb",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd981266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b9cb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn_relu.predict_class(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdd91454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABUrklEQVR4nO2dd3hUZfbHP2dm0hNSKaGFIkVAmkgRpQgirr0uir3hT7DgWsDurlhWcS3YWCysXRQVC1VFFAFBegsEQggB0kMqKTPv7487gYCQTLmXZOL7eZ77ZObOveeeuTNz8pbznq8opdBoNJrGiK2+HdBoNBqr0AFOo9E0WnSA02g0jRYd4DQaTaNFBziNRtNocdS3AzWJig1SCa1CTLebuynYdJsAImK+UZsFNgHldFli1yokxJrPTJVXWGI3UDhICRWq3K8v2TnDI1RuntOjY/9YXz5fKTXan+v5Q4MKcAmtQnh89imm2/2wa2vTbQLYQkNNtynB1vywnYWFlti1CntSB0vsOrfvNN+ozW6+TQCXZ0HEG1aoH/y2kZPnZMV8z35TQYk7Evy+oB80qACn0WgCAYVTBUaPQAc4jUbjFQpwERgLBHSA02g0XuNCt+A0Gk0jRKGo1F1UjUbTGFGAU3dR/WPLe5Hs+DwCBGI6VTLomTzWvRxNxk+h2IIUkW2dDHo6j+Amxo3e+FYUO76IQGyKfg8X0PLMco+vFRTiYursFIKCFXaH4pfvYnj/hRZe+5yQWM59L+wgtmklyiXM/aQZX7932M5lt+zjlod28/dT+1KYH+S5f8Eu/v3BOsM/u+LXBQl8+GoS7bsUM+HJFMLCnWRmhPLv+7pQVuLbR9q0ZQX3v7yb2GZVKBd8/0E8X73d1CdbNbn3xd0MGFlEQY6DcWd18ercex78g/6D9lOQH8IdN44EIDKqgslP/E6zFqVk7Q/nmcf7U1wcTJ9+Wdxw2yaCglxUVtp4540erFvjvf/9hhVy+7/2Yrcp5n4cx2fTmntt41hcfHMW516VgwjM/SiBL99uZopdq/yti0AZg7M00VdERotIsoikiMgkT88rzbSR/H4koz/P5PxvMlEu2PVdOImnH+S8bzI5b04WTdpVsWl6EwAOpDhI+z6M87/dz1kzclj5z1ivZtgry4UHrujI/53dhf87uwv9hhXRtW+J1+/XWSX89+kkxo3qxcTLunP+tZm0PakUMIJfnzMOkJnhfRpIZYUw+YaeTLi4LxMu6UO/M/Lp0quQu5/azrtT23HHhafy28J4Lr95j9e2a/o+/Z8tuXVoV+4+vxMX3JBD204HfbZXzYJP43h4bHufzl00N4lH7x98xL4rx25j7R9NuXXsKNb+0ZQrxm4D4MCBYJ6cPJA7bhzBi8+cyj8eXuX19Ww2xfinM3hkbHtuHdaF4RcVmHIPkrqUce5VOdx1flduH3UyA0YeoGV7/+1a5W9dKMCplEdbfWNZgBMRO/AacC7QDbhKRLp5er5ygvOg4KqCqjIhvJmTxDPKsbkbKAm9yindb+Qfpf8QRtLfyrAHQ2RrJ1Ftq8hd700gEQ6WGrYcQQp7kMKXzyY/O5gdmyIAKCuxk54SSnyLSgDGPZLG28+2wbd/fDX8cyjsDhcoaN2+jI0rowFY81ssg0fl+GIcgLysIFI2hB/he0Jipc/2qtm4IpKifN9alRvXJ1BUdGRLd+DgfSyalwTAonlJDDpjHwA7t8eQlxsGQFpqFMHBThxB3uWRdelTyt5dwezfHUJVpY3FX8cw6JwDPvlek7YnHWTLmgjKD9pwOYX1yyMZPLrAb7tW+esJLg+3+sbKFlx/IEUptVMpVQF8AlzkyYnhzV2cfFMxX52VyOwzEwmOUiSecWSXc8cXEbQcYvy3Ksu0E5F4+Msc3sJJWaZ3yZc2m+L1hcl8un4Ta5ZEkrwmwqvzj6ZZq3I6di8leW0EA0bkk7M/mNStvtu02RSvfrmaj5YuZ81vsSSvb8Ku7eEMPCsPgDNHZ5OQaE6WfvPWFXTsUcbW1eGm2DOTmNhy8vOMBOv8vFCiY/88FDF46F52bI+hqtK770B8i0qy9x7+x5izL8iUIL8rOZRTBhQTFVNFSKiL084qpGlL/+1a5W9dKBROD7f6xsoA1wpIr/F8j3vfEYjIbSKySkRWFeUbH075AWHPD6FctGg/ly7ZR1WZkDrn8I9t45tRiAPaXWB0/455G71cjOJyCXec3YWxp3ajS+9SkrqUeWegBqHhTh55fRtv/SsJZ5UwZnwG77/k32oKl0u485K+XDdsAJ17FpHUqYSXHurM+WP38vIXawiLcFJV6f8yr9BwJ4/O2MWbj7WktNiiDH0LaduukJvGbeLVqb29PvdYK+/M6GWlp4Tx2evNeebj7Uz5IIXUzWE4q/z/rKzyty6UgkoPt/rGygB3rE/wT29ZKTVdKdVPKdUvKtbojuxfFkpkayehcS5sQdDm7DKy1xj/qXZ+GU7GT6EMfj7v0Acc3txJyb7DP8bS/XbCmvm2zKWk0M66ZZGcNrzIp/PtDhePvL6dn+Yk8Nv8OBKTymnRupzXv9vAe0vWkNCigle/2Uhsgm+trZIiBxt+j+bUM/PZkxrOIzefwt2X9eHn75qyb7d/S8fsDsWjM3bx4+xYls6N8cuWVRTkhxAbZ7TcY+MOciD/8Nrl+KZlPPrUcqY+fSr790Z6bTtnXxBNWx7+XBISK8nd7/lkUG3M/ySBCeeezH2Xd6aowE5Gqv9rrq30t3YEp4dbfWNlgNsDtKnxvDWw15MTIxKd5KwLpqpMUAr2LwshukMle38JYdOMKIa+kYsj7HCsbH1WGWnfh+GsgOI9dorSHMT39DyARMdVEdHECIjBoS76nllMeoovwUJxz7OppO8I48u3EwHYlRzOVf1P5YYhfbhhSB9y9gdz5wU9yM/xfIywSWwFEVFVhn8hTnoPKmDPzjCi44z3KKIYc3s633+S6IPPh32/d2o66dtDmT3d/9lTq1i+tAUjR6cBMHJ0GsuXGu85IrKCJ5/9jfemd2fzxnifbCevDadV+wqatynHEeRi2EUFLF8QbYrf0fFG76RpywoGn1vA4q9j/bZppb+1oQCX8myrb6xME1kJdBKR9kAGMAa42pMTE3pV0HZUGXMvbYY4IPbkCk76ewnfnt8CVwX8eJOxfje+VwUDniwgplMVSeeW8e15LRC7ot9jBV6tf45rXsl9L+/GZgObDZZ8E82KRU28fsPd+xUz8tIcUreGMe3bDQDMfKENKxfHeG3rCP+aVvKPZ5Ox2RUi8Mu8BH5fHM9F12Zw/lhjkH3pgngWzvY9RaB7/xJGXpHPzs2hvL4wGYB3n0lk5Y/e34eaTHo9jZ6DiomOq+KDVZt5f2pz5n/sWQB64LGV9OydTZPoCv43ay4fvHsysz7qzOQnVjLqvDSyM8N5+vH+AFxwyU5atiphzHXJjLnO8P+R+wZzoMDzlpLLKbz2cCue/mgnNjss+CSOtG3mFFR4bPpOomKdOKuEaQ+3ofiA/z89K/2ti4bQOvMEsVJ0RkT+BrwE2IF3lFJTaju+fY9IpauJ6GoiAPZOupqIVdVEClWeX9Gpe89g9cl3nuXx9Wyb8YdSqp8/1/MHSxN9lVLfA99beQ2NRnNiUUClCoxauQ12JYNGo2mYKARngBQD1wFOo9F4jUsFxhicDnAajcYrjMX2OsBpNJpGieDUY3AajaYxYlT01QHOa3I3BVuS0jF/71rTbQKc07K36TYtrH5gjV2L0oycKamW2LUEC9I5GjJKCRUqMJbxNagAp9FoAgOXHoPTaDSNEWOSQXdRNRpNo0RPMmg0mkaKnmTQaDSNGqdO9NVoNI0RhVCpAiN0BIaXbvxVfvpyRgJzP4xHKTh3bB6X3prN+y+0YO5HcUTHGVP9N07eS/8RRVRVwn/ua0vKBqP66sgr8hhzZ5bH1zJLqQusU+uqSeuOB3nojV2HnrdoW8H7L7Tgyxn+qz9Zpfw0c/kmyortuFyGaM6df/NOtetYWOGrP8pidVEfqlp6kgEQkXeA84EspVQPM2xWKz+lbAgnLMLJtHnbWL0kit3b6y5btGtrKHM/jOeV77YRFKx46OqODBhhCHRccms2V/xf9hHHL/kmhspy4a0fkzlYKtw27GSGXVxAizaeFdKsVuo6WGrH7lC8+FUKK3+MYutq73UZqtW6dmyKICzCyStzNrLm1ybsTgn3S62rJnt2hHLHqK6Aof/w4R+bTKnqW638NHlMB3L2BfHq99tZPj/ao8/MEx644iQKfRS1ORqrfF3waRxz3k3g/pfT6z7YC6y+t8dDIQHTRbUyDL8HjDbToD/KT7u3h3By31JCwxV2B/QcVFzrD1gEDpbacFZBxUEbjmAX4ZHeJHSao9QFVqp1HZveZxSxLy2ELD+DJtSv8pO3WOWrP8pitVG/qlo2j7b6xjIPlFJLgDyr7Hur/NSu60E2rIigMM/OwVJh5Y9NyN5rdOe+ebcpt4/owtSJbSgqMILSmecXEBru4qrePbjmtG5cfns2TWK9y1g3W6kLzFfrOhbDLipg8VcxptiyVPlJCU9/vINpc5M5d6zvkonV1JdKla/Um6qWAqeyebTVN/XuQU1VrUo8U6P3Rfmpbadyrrwji8ljOvLw2I6071aG3aE4//oc3l22mdcXJhPXvJLpT7YEIHlNBDa74qM1G/nfii188WZT9qV516IxU6kLrFHrOhpHkIuBow6w5NsYU+xZqfw08eJOTBjdhYev6cCFN+TQY0CxX/bqS6XKV+pNVQuhUtk92upCRCaKyCYR2SgiH4tIqIjEichCEdnu/htb4/jJbiH5ZBE5py779R7gaqpqBVF3/Xx/lJ9GX53Hawu2MfXLFKJinLRqX05s0yrsdkOL4dyxeSSvNVqEP30ZQ7/hRTiCICahim6nlbBtnW86of4qdYH1al3VnDa8iJQN4RTkmKPOZKXyU16mYedAbhBL50bTtXepX/bqT6XKN+rTXyc2j7baEJFWwF1AP/c4vR1Du2US8INSqhPwg/s5buH4MUB3jOGv190C88el3gOcd/in/FSQY4yDZO0JYun30Qy7uIDczMNjI7/NjaZdF0OSrmmrStb+GolSxljc1tURtDnpoMfXMk+pC6xS6zoWwy7ON617CtYpP4WEOQmLcB56fOrQInYl+ze4Xl8qVb5Sf6pagkt5tnmAAwgTEQcQjqG8dxEw0/36TOBi9+OLgE+UUuVKqVQgBUNgvlbjAYO/yk//vKUdRfkO7EGKCU/vISrGyb/vbMuOTWGIGON6d/3bmOm68MYcpk5sy23Du4ASRv09lw7dPA9wZil1gXVqXUcTEuqi75AiXn6wTd0He4hVyk+xTat4/G2j4ojdDj99FcOqxf4pgFnlqz/KYvXhryd4kSaSICKrajyfrpSaDqCUyhCRF4DdQBmwQCm1QESaK6X2uY/ZJyLVuUqtgOU1bB1TTL4mlqlqicjHwDAgAcgEHldKvV3bOU0kTg2QEab7ElDlkixQ6gJwlXs2vuk1Vg36BFh5p0DBDFWtNj2aqHtnDfTo2Hu7LTyuqpZ7bO0L4O9AATAL+ByYppSKqXFcvlIqVkReA5YppT5w738b+F4p9cXxrm9ZC04pdZVVtjUaTX1immr9SCBVKZUNICKzgdOBTBFJdLfeEoHqDHuvxeQDbAxOo9HUN4ZsoCmzqLuBgSISLiICjAC2AHOA693HXA987X48BxgjIiFuQflOwO+1XSCgxuA0Gk39o5TgMiHHTSm1QkQ+B1YDVcAaYDoQCXwmIjdjBMEr3MdvEpHPgM3u48crpWpNTtUBTqPReI1ZSbxKqceBx4/aXY7RmjvW8VOAKZ7a1wFOo9F4hVEPLjDWouoAp9FovERX9PUNESTI/wXeR2NFOgfA6ev8WzVwLH7rY41akf2k9pbYdW7faYldR5J5uXg1qdq123yjf7GUFoVWttdoNI2U6rWogYAOcBqNxmsaQikkT9ABTqPReIVRLkl3UTUaTSNFj8FpNJpGiVFNRHdRNRpNI8RYqqUDnClMfD6VAWcVUJAbxO2jDO2aydNSaN3BKF0U2cRJcaGd8X/zXdfGH9Wjsl2Q/MDh21i+R2hzh5OidUJZmtGMdxYJ9ihF78+qDh+3D9ZcEkSb/3PS6npX7f69kMaAkQcM/0Z2AyAqpoqHXk+leZsKMtODmfJ/7Sk+UPfHec+Df9B/0H4K8kO448aRAERGVTD5id9p1qKUrP3hPPN4f4qLg+nTL4sbbttEUJCLykob77zRg3VrvKvDZ6ai1MV/38GoC3ajENJ2RPGfKb1pk1TM+PvXExzswukUXn/hFLZtia3b2DHwV7WtNqxQAIP6UdUigFpwlnkpIm1E5CcR2eIuSXy3L3YWzkrgkes7H7HvmQknMf5vPRj/tx78Oi+WpfN8+0JXs+DTOB4e61ueWFg76P1ZFb0/q6LXx1XYQiHuLBddnnce2h83wkX8WUcGsdTnHcSe4Vme04JZcTx8zUlH7Lty/H7WLI3ipjO7s2ZpFH8fn+mRrUVzk3j0/sFH2hq7jbV/NOXWsaNY+0dTrhi7DYADB4J5cvJA7rhxBC8+cyr/eHjVsUzW7rsf97Ym8QllXHBFKvfcNITx1wzDZlMMHbmXG8dv5qN3OnPnDUP5YEYXbhy/xedrVKu23Tq0K3ef34kLbsihbSfPawDWxQNXnMQdo7qaFtyqVbUeGdueW4d1YfhFBab6WxsuxKOtvrEyDFcB/1BKnQwMBMa7Sw57xcbfoygqOF7LRDHkvDwWz/GveKBZqkcHVgihbRShLQ/vUwpyF9hIOPdwgMv9UQhtrQjr6FmA27gi6pAYTjWDRh1g0SzjfS+aFc+gcwo8s7U+gaKiI8taDxy8j0Xzkgxb85IYdMY+AHZujyEvNwyAtNQogoOdOIK8E94xU1HKblcEhzix2V2EhDrJzQlBKSE8wmgZR0RWkZfjez09f1Tb6oP6UtWqnkX1ZKtvrKwHtw+orspZJCJbMKpvbjbrGj36F5OfE8TeXSemimld5MyzkTD6yJZa4WohKF4RZsQPnKWQ8a6d7m9VkTHT92TJ2IQq8rKMQJWXFURMfFUdZxyfmNhy8vOMe5ifF0p07J+LYw4eupcd22OoqqyfBM/cnDBmf9yR975cREW5ndW/N2XN783IyQzjn/9Zzs0TNiM2uG/c4LqNeYC3qm114lYAQ8F3H8Qz98MEv00eS1Wra1//dCk8JVC6qCdkDE5E2gF9gBXHeO024DaAULz7Mg27MNfv1ptZuCoh72cbbe8+8j9+ztwjg176G3ZaXuPCbtLv5kTQtl0hN43bxMP3nV5vPkRGVTDwzP3cdPkISoqCmDxlFcPP2UPnbgX895Xu/La4JWectZd7Jq/j4bsH+XUtX1Tb6mLixZ3IywwiOr6SZz/ZQXpKKBtXRPplsz5VtQIlTcTyMCwikRhlie9RShUe/foRqlrieUvMZlcMHp3Pkm/iTPTWdwp+FSK6KoJrxFtVBXk/HBngijYIaS/Z+ePcIPZ9aCNjhp19H3v/MeTnOIhrZgTTuGaVFOT6/r+qID+E2Dhj7CY27iAH8g+rm8U3LePRp5Yz9elT2b/Xvx+kP/Tul0Pm3nAKC0JwOm38tjiRk0/JY8S56fy22BDi+fXHRDp3K/DrOv6ottWG2QpgUH+qWgqoUjaPtvrGUg9EJAgjuH2olJptpu0+ZxSSviOMnP3mL873hey5R46zARSsEMLaK0JqTGyd8l4Vp86t5NS5lSSOddHqFieJV9U+i3osli+MZuQVuQCMvCKXZX6oKS1f2oKRo9MMW6PTWL7UCBgRkRU8+exvvDe9O5s31m9LOTszjC7d8wkJqQIUvfrlkL4rirycUE7pY9yHXqfmsDfdHxFs/1TbjocVCmBQvypgLmXzaKtvLOuiuksQvw1sUUq96KudSa/soOegIprEVvH+8rV88J9WzP+0KcMuyGXxHHNab/6qHjnL4MByGx0fPap7eowxOZ/8m5ZKz0FFhn8rN/D+1EQ+ndaCh99MZfSYXLIygplyu2czlQ88tpKevbNpEl3B/2bN5YN3T2bWR52Z/MRKRp2XRnZmOE8/biixXXDJTlq2KmHMdcmMuc5QMXvkvsEcKKhbv/aQ7yYpSiVvjmXpTy15+b0lOJ02dm5rwtyv27JjWxPG3bMJm11RWWHj1ed6em27Gn9V246HFQpgUI+qWp5LAtY7VqpqnQH8AmwAqn/lDymlvj/eOU1s8Wpg0GjTfVGV5pc1AqvKJYWZbhPA3jHJEruWlUtq19YSu3/1cklmqGrFdm2mznrnco+OnT34jeOqap0IrJxF/RUaQCKMRqMxnUBpwTX4lQwajaZhoQteajSaRotCqHLV/wSCJ+gAp9FovKYhLMPyBB3gNBqNdyjdRfUJAcRuftNXOa1ZXrSsn/mJrw+n1CrU7TPP9q2/JF1fcGXn1rcLmuOgx+A0Gk2jRgc4jUbTKFEITj3JoNFoGit6kkGj0TRKlJ5k0Gg0jRmlA5xGo2mcBM5i+wYf4BISy7nvhR3ENq1EuYS5nzTj6/daHHr9slv2cctDu/n7qX0pzPe8FpaZQi41OZZITodupdw5ZRfBIYYwyrRHkti2ru60jRVvN2XtZ3GIQNPOB7ng+d04QhQrZyaw6n8J2ByKk4YXMmKSUWI8c0socx9pQ3mxDRG46ettOEJqX7BtsylenrWa3MwQnrijB9feuYuBZ+XiUkbtshcf6kJetufVQ47GLNGZoGAXz3+0kaBgF3aH4td58XzwSluuvWc3g0bkHfJ36oOdyMvyvYSWVSIujUt0RrfgEJFQYAkQ4r7O50qpx72146wS/vt0Ejs2RRAW4eSVORtZ82sTdqeEk5BYTp8zDpCZ4f0XesGsOOa815T7X9p1aF+1kMtnr7XgyvH7+fv4TN5+upVXdhfOSuCbmc2478XUQ/tunpzOhy+3ZNXiGE4bXsAtk/fwwJiutdop3B/EypkJjFuwlaBQxewJSWz6JpboVhVsWxjNrd8n4whRlOQYH6GrCubcm8SFL6bR/OSDlObbsTnqrkZx0bUZpO8IJzzSqFf2+Tutef/VdgBceE0GV9+xm2lPdvLqHtRkwadxzHk3gftfTvfZBkBlhTDpuu4cLLVjd7h44ZONrFoSyxczWvL+S0blkQuv28fVE9KZ9lhHn65RLeIyeUwHcvYF8er321k+P5rd280pQfTAFSdRaJI+BVjv7/FQCpyuwAhwVs71lgNnKaV6Ab2B0SIy0Fsj+dnB7NhkFDGsFgKJb2HUXRv3SBpvP9vGyDz0EjOFXI6weyyRHMWhABIR5SQ3y7OWpsspVB204aqCyjIbUc0rWf1hPKffnnmoZRaRYGgx7PwlimZdy2h+slGZNzzWia2O/Ob45uWcNjSP+V8cbhGXlRz2PTTM6XfFHvNEZ4SDpcYbcjgUDodCKSgtPtJfX74L1dSXiIuv1Ke/gaKqZWW5JAUUu58GuTe/fi7NWpXTsXspyWsjGDAin5z9waRu9aeC65GYKeRSkzf/2ZYp/9vGrQ+nIza499KT6zynSYtKBt6SxatndCMoVNH+jEI6nFnED8+2ZPfKSBZPTcQRohgxOYOWvcrISw0FgY+v70BJnoPu5xcwaFxWrdcYN2kH77zQ/lC12WquuzuVERdmUlLsYNINvheQNBubTfHKV+to2fYg337YguR1UQBcPzGNEZdkU1JkZ9K1vuvjWiri0ohEZxSB00W1umS5XUTWAlnAQqXUMUVnRGSViKyq4M9qTtWEhjt55PVtvPWvJJxVwpjxGbz/UmvrnDeR86/J4q1/teHaQb15659tmfjvXXWeU3bAzrZF0Yz/eTN3LdtIZZmdDV/FopxwsNDODbO3c9bkvcy+s50xbe+E9FURXPSfNK7/bDvJC6JJXXr8cb7+Q3MpyAsiZXPUn17738vtuX7EQBZ/24wLxu71562bisslTLiwN9ee2Y/OPYtJ6lQCwMz/JHHdkH78NKcpF1yzz2f7Voq4TLy4ExNGd+Hhazpw4Q059BhQXPdJdVBfojPVkwyebPWNpQFOKeVUSvUGWgP9ReRP/15ris4Ec+zBbLvDxSOvb+enOQn8Nj+OxKRyWrQu5/XvNvDekjUktKjg1W82EpvgX4VdM4VcajLyslyWzjXEqX/5LpbOver+cu9aGklM6woi4p3Yg6DLOQXs+SOCqBaVdD3nACLQqlcpYoPSPDtRLSpJGlBCeJyToDBFx2GF7N90/OrA3foWMnB4Lu8uXMGDU7fQc0AB9z239YhjFn/XjMFn5/j35i2gpMjB+hXR9BtScMT+xd8kMPgc39ewWini0phEZ8AIpJ5s9c0JWW+hlCoAFgM+1CNX3PNsKuk7wvjybUMMZVdyOFf1P5UbhvThhiF9yNkfzJ0X9CA/xz8BGjOFXGqSmxVEz4FFAPQeXOSRjmuTlpVkrA2nskxQCnb9FkXCSQfpfPYBdi0zWma5O0NwVgrhcU46DCkic2solWWCqwp2r4gk4aTjt4jf+097rjtrIDeePYDn/nEy61fE8MKDXWmZVHbomAHDc9mzs2HoG0bHVRIRZQwZBIc46XN6Aek7w47wd+CIfPbs9L3ku1UiLo1RdEYp8WirCxGJEZHPRWSriGwRkUEiEiciC0Vku/tvbI3jJ4tIiogki8g5ddm3cha1KVCplCoQkTBgJPCct3a69ytm5KU5pG4NY9q3GwCY+UIbVi6O8cs/M4VcjrB7DJGclx9sx+1P7MZuV1SU23h5Urs67bTqXUrX0Qd4+4Iu2ByK5t3K6DMmFxH49sE2TB/dBVuQ4sLndyMCYdFOBtyczTsXd0YEOg4rpNNZf1JprJMbJ6bSqn0pyiVk7Q3xawYVzBOdiW1awX3/TsFmU4hN8cvcBH7/KY6Hp22ldfuyQ/6++lgHn321SsSlsYnOGLOoprWNXgbmKaUuF5FgIBx4CPhBKfWsiEwCJgEPikg3YAzQHWgJLBKRzkop5/GMWyk60xOYCdgxWoqfKaX+Wds50bZ4NTD0b6b74qqorPsgHxC7+WWYHkq2qlzSUEvsOgusmbWzRZg3eVQTV0mJ+Ub/YqIzYSe1VO1fuM2jY7dc8uRxRWdEpAmwDuigagQiEUkGhiml9olIIrBYKdVFRCYDKKWecR83H3hCKbXseNe3chZ1PYaavUajaWSYNIvaAcgG3hWRXsAfwN1Ac6XUPuM6ap+INHMf3wpYXuP8Pe59xyUwap5oNJoGg8Kz8Td3EEyozpJwbzWbfg6gL/CGUqoPUILRHT0ex4qqtTZzG/xSLY1G0/DwovOcU4su6h5gT430sc8xAlymiCTW6KJm1Ti+TY3zWwO15jHpFpxGo/EOBcolHm21mlFqP5AuItULc0cAm4E5wPXufdcDX7sfzwHGiEiIiLQHOgG1DlrrFpxGo/EaE1cy3Al86J5B3QnciHtSUkRuBnYDVxjXVJtE5DOMIFgFjK9tBhV0gNNoND5g1gSvUmotcKwu7IjjHD8FmOKp/eMGOBF5lVq62kqpuzy9iKcopXAdPGi2Wcuwt2tT90Fe8kyvM023CXDm0v2W2P25p++JtbVhi4ut+yAfsCRN5C9GIK1Fra0Ft+qEeaHRaAIHBQR6gFNKzaz5XEQilFL6359Go2kQ60w9oc5ZVPfasM3AFvfzXiLyuuWeaTSaBopnM6h1zaKeCDxJE3kJOAfIBVBKrQOGWOiTRqNp6CgPt3rGo1lUpVS6HLnertapWY1G04hRjWOSoZp0ETkdUO5clbtwd1c1Gs1flAbQOvMETwLc7RglTVoBGcB8YLyVTh2Ppi0ruP/l3cQ2q0K54PsP4vnq7aam2DZLnejCK3ZwzgVpiMD8OUl8PasjDz65ktZtjSKXEZGVlBQHceeNwz22GRTs4vkP1xuKUnb4dX48H7yadOj1y27awy0P7uLvAwfUqSxWmipsfuBw3byDe4R2d1QSc5qLbf8KxlUBYodOD1fQ5BRF3jIbqS8FoSpBgqDDvZXEDnB57LtZqlqt2hYzacqaQ89btCrlg+mdWf9HPOMf3EBYmJPMfWE8/3hvykp8L/qoVbU8pZG04JRSOcBYXy8gInaMlJMMpdT5vtoB44sx/Z8tSdkQTliEk2nztrF6SZTfKkJmqRMltS/knAvSuPfWIVRW2fjX1GWsXNac5x4/7dAxN0/YSGmxdz/Aygph0vWnHFaU+mg9q5bEsnVdExJalNPn9AIyMzyT9gtvr+g3yyiEqZywbGQoCSNcbHsyiKTbK4k/00XuLzZ2/ieI3u9UEBSj6PFqOSHNoGS7sP7/Qhi0yPNcRbNUtTJ2R3LntUaOoM2m+N+3P/Db4uY89Mxq3n7lZDauiefsC9K57JqdfPCWb8FDq2p5gef/4+oVT2ZRO4jINyKSLSJZIvK1iHhTVfBuTOrS5mUFkbLBqDBbrbCVkOh/rTez1InatCsieVMs5eUOXE4bG9YkMGhITY0AxZnDM/h5kXdShMdWlDL+g46bvJO3n2/nU5chf4WNsDYuQlsqEHCWGDadRUJIU8Ng1MmKEHexmvCTFK5ycHlRGd48Va3D9Doth317wsneH07rpBI2rokDYM2KBAYP9z2hWatqeUh1HpwnWz3jySzqR8BnQCJGFc1ZwMeeGBeR1sB5wAxfHTwezVtX0LFHGVtX+19S+1jqRL4EzrSdTejRO5eoJhWEhFTRb1AmTZsdLqndvVcuBfkh7N1Tt+jz0dhsimlfreHj31aw5rcYktdHMeCsXHKygklN9t4eQPY8O83ONeaLOj5Qyc4XHSw/O4QdLwbR/u4/K4rlLLQR2dWFzb/K8H4z5Oy9/LygJQBpOyIZOCQTgDNG7COhxv32FrO+B8fErao1bW4y5441R+fCUn/rIFA0GTz51ypKqfdrPP9ARCZ4aP8l4AHgz9JN1caN+lC3AYTiWbAKDXfy6IxdvPlYS0qL/a+qa5Y6UXpaFJ9/0Imn/vMbB8scpKZE43QeNj50ZAY/L/JNCczlEiZc3IeIqCoefW0L7bqUMOb2dB6+yTeZPFcl5Cy20/5u4wex7zMHHe+vpOnZLrLm20l+PIhe/z3cVCtJEXa+FETPt/wT9vEXh8PFgDMzmfm6IZz90lO9GPePTVx183aWL2lOVZXvBXKsVtXKywwiOr6SZz/ZQXpKKBtX+PaPqZr6U9Ui8CcZRCTO/fAnd130TzDe1t+B7+oyLCLnA1lKqT9EZNjxjlNKTQemAzSRuDpvm92heHTGLn6cHcvSuTF1He4RZqoTLfguiQXfGRMA1922mdxsY62mze7i9KH7uPtm/0qHVytKDRqRayiLfW0MvCe0KOfV2Wu554peHonv5P1qI+pkF8FueYT9c+x0fNAIdk1HOdn2xOH3X74fNk0MpuuUSsLa1O83u9/pWexIjqYgzxhz3JMWyaN3DQCgZZtiThtcuxZsbZxoVS1/A1x9qmo1hO6nJ9T27+4PjMmBvwPjgJ8wlLH+D6OkSV0MBi4UkV0YwfEsEfnAH2dBce/UdNK3hzJ7ujmzp2CuOlF0jDGA37R5KacP3XdovK1Pv2z2pEUeCnhe2Yz9s6LUjs2RXHX6AG4YcRo3jDiNnP0h3Hlpb4+VxbLmHu6eAoQ0VRxYZXwdClbYCGtrBLKqQtgwIYT2d1US3af+R5aHjDrcPQWIjjXut4hizE0pzP0y6Xin1olW1fIcUZ5t9U1ta1G9l5Q68vzJwGQAdwvuPqXUNf7Y7N6/hJFX5LNzcyivL0wG4N1nEln5o38KRWaqEz005XeaNKmgymnjjRd7UlxkBJwhI3yZXDCIbVbBfc9uw2ZXiMAv8xL4fXFc3SceB2cZ5C+z0/nRw+M1nR+vJOW5IJQTbMHQ+XGjZZDxiYOy3ULa9CDSphvH9nyz/FDLry7MUtUCCAlx0qd/DtOeOeXQvqGj9nL+5WkA/PZTCxZ+47sYuFbV8hAl0ACWYXmCR6pabsHmbsChu6eU+p/HFzkc4GpNE2kicWqAHLMMVIPE0aGd6TZd2b4LF9fGmUt977rVhlXlkhxtfA9UtVGVvsd8o38xVa2QpDYqcfLdHh2b9n/3H1dV60RQ5ySDiDwODMMIcN8D5wK/Ah4HOKXUYozurUajaQw0gO6nJ3gy5XQ5RnXN/UqpG4FegGdZpRqNpnHSiBbblymlXCJS5RZqzcLQM9RoNH9FGkPByxqsEpEY4L8YM6vF1KFko9FoGjcNYYbUEzxZi3qH++GbIjIPaOJWrddoNH9VAj3AiUjf2l5TSq22xiWNRtPQaQwtuKm1vKaAs0z2JeCo2rmrvl3wGKvSOT5MX2qJ3bFtBlti1x5jfiKss8CaBe62cP/XWR+NlJmk9R7oY3BKKc8Llmk0mr8ODWSG1BO08LNGo/EeHeA0Gk1jRep/WbJH6ACn0Wi8J0BacJ5U9BURuUZEHnM/bysi/a13TaPRNEQ8rSTSEGZaPZlSeR0YBFzlfl4EvGaZRxqNpuETICXLPemiDlBK9RWRNQBKqXy3fGC9YJWKkBV2A8lXf9Wv5r2dyE8fNUchDL9qP+feso+PnmrH6kWxOIIUzZMOctvU7UREO8lOD+H+4X1I7GiUFz+pbzE3P7PDq+uZfQ9sNsXLs1aTmxnCE3f04No7dzHwrFxcyihS+eJDXcjL9m0JdlCIi6mzUwgKVtgdil++i+H9F1r4ZCshsZz7nk8hNqESpWDuJ835emYiHU4u4c5/7SQo2IXTKbz2eHu2rT9uIW3/aQCtM0/wJMBVupWxFICINMVDTR13scsiDKHoKn/LplilImSF3UDyFfxTv0rfGs5PHzXnn9+uxxHk4rlru9NnRD49zizg75N2YXfAx08nMee11lz1kFG7rXnSQZ6Zv84nX624Bxddm0H6jnDCI43ClJ+/05r3X20HwIXXZHD1HbuZ9mQnn2xXlgsPXNHRrYqmePGrFFb+GMXW1RFe23JWCf99JokdmyIJi3DyylfrWbM0mpsfTOPDV1qzakkspw3N5+YHd/Pg2O4++esJDaH76QmedFFfAb4EmonIFIxSSU97cY3hSqneZtSEskpFyAq7geQr+Kd+tTcljJP6FhMS5sLugJMHHGDlvHh6Di3A7jZ5Up8i8vaZU4TG7HsQ37yc04bmMf+Lw62qspLD9yI0zOlnabYaqmhBCnuQ8tlefnYwOzZFun20k74jjPjmFSjFoeAcHuUkN9PC0uXKmEX1ZKtvPFmL+qGI/IFRMkmAi5VS9aJsfywVoa59Sxuk3UDy1V9adynls38nUZTvIDjUxdqfYunQs/iIY37+rDkDLzisJpWdHspDo3sRFunkivt303VAocfXM/sejJu0g3deaH+orHg1192dyogLMykpdjDphp4+2we3Ktr8bbRsV8E378WTvMb71tvRNGt1kI7dSkheF8lbT7XjqXe3cMvkNEQU/7jylLoN+ENjacGJSFugFPgGmAOUuPd5ggIWiMgfbvWsY9m/TURWiciqSsrr8OUYFzDhRlthN5B89ZdWncq44I49PHt1d567phttu5ViqyF29tUrrbHbFYMvyQYgplkFL69YxdPz1nHNY6m8dmdnSos8V0cz8x70H5pLQV4QKZv/PF71v5fbc/2IgSz+thkXjN3r2wXcuFzCHWd3Yeyp3ejSu5SkLr7LG4KhLPfIa9t466l2lBY7OO/qTKZPacd1Z57K9KfbcY+XY5peEyD14Dzpon4HfOv++wOwE5jrof3BSqm+GFWAx4vIkKMPUEpNV0r1U0r1C6qjjqZVKkJW2A0kX81g2Jgspsxdx2NfbCQyupIW7Y0f8JJZTVnzQyx3vLrtUGAKClFExRoiOu17ltA86SD7d3q+VtbMe9CtbyEDh+fy7sIVPDh1Cz0HFHDfc1uPOGbxd80YfLY5WqYlhXbWLYvktOFFPtuwO1w88loyP81J4LcFhr7FyEuzWTrf0On45ft4uvQqrs2E3zSaNBGl1ClKqZ7uv52A/hjjcHWilNrr/puFMY7nV/6cVSpCVtgNJF/N4ECOEWByMoJZOS+e0y/KZt1PMXzzRmv+8c4WQsIOD8gU5jpwuXuDWWkh7E8NpVnbgx5fy8x78N5/2nPdWQO58ewBPPePk1m/IoYXHuxKy6TDLawBw3PZs9P3he/RcVVENDHecHCoi75nFpOe4uuEiOKeZ3aQnhLGl+8cVhfLzQzmFHc3v/egQjJ2nQDxmQDA61FlpdRqETmtruNEJAKwKaWK3I9HAf/0wcdDWKUiZIXdQPIV/Fe/evm2LhQVBOFwKG54aicRMU5mPtqBygobz1xtzOZVp4NsXRHN51PbYrcrbHbFTc/sINLdovOEE6EmdePEVFq1L0W5hKy9IT7PoALENa/kvpd3Y7OBzQZLvolmxSLfVLW6n1rEyEtySN0azrQ5xiz0zKlteeXhDox7dBd2u6Ki3MYrD1tcdLsBtM48oU5VLRG5t8ZTG9AXiFdKnVPHeR0wWm1gBNKPlFJTajsn0FS1NLpcEgRWuaTlZd9xwJnjVwZuaMs2qt1t99Z9IJD85L11qmq509BWARlKqfPdovOfAu2AXcCVSql897GTgZsxUs/uUkrNr822Jy24mqOvVRhjcV/UdZJSaieGQI1Go2lsmNuCuxvYAlQ3aycBPyilnhWRSe7nD4pIN2AM0B1oCSwSkc5KKeexjEIdAc4dWSOVUveb8CY0Gk0jQDBvAkFEWgPnAVOA6mbhRRhSpQAzMSRHH3Tv/0QpVQ6kikgKxrj+suPZP+4kg4g43JHxuKXLNRrNXxTP00QSqtPA3NvR6WIvAQ9w5Oqo5kqpfQDuv83c+1sBNZfa7HHvOy61teB+xwhua0VkDjALKDn0/pSaXZthjUbTSPEuBSTneGNwInI+kKWU+kNEhnlg61hjh7V64skYXByQi6HBoNwXUYAOcBrNXxVzlmENBi4Ukb8BoUATEfkAyBSRRKXUPhFJxNBiBqPF1qbG+a2BWjOwa8uDa+aeQd0IbHD/3eT+u9GXd6PRaBoHZiT6KqUmK6VaK6XaYUwe/KiUugZjxdT17sOuB752P54DjBGREBFpD3SiDo3m2lpwdiASH5qFPhMeinTvYbpZtcqaeOxon2S6TVdOnuk2AWwR5qccgHXpHPnf+Z53Vhux52033aaEmFNE4GhcpeavMVbKpBXw1ubBPQt8JiI3A7uBKwCUUptE5DNgM0ZGx/jaZlCh9gC3TynlV2KuRqNphFiwzlQptRhjthSlVC5GcY9jHTcFY8bVI2oLcPVfjlOj0TRIGsI6U0+oLcDpJQUajebYBHqAU0pZMxik0WgCnoZQzNITtGygRqPxjgZS680TdIDTaDReIQTOAH2DDHAT71rGgH4ZFBwI5fY7zz/itcsu3sytN63hyrGXUVgUyvChqVx+yeEK6u3b5TNh4rnsTI3z+Hr+KkrV5MIrdnLOhWmIwPw5bfn6s46cMXwvV9+cTJukIibeOoSUrTFe2QwKdvH8h+sJCnZht8Ov8+P54FUjReXCa/ZywTX7cFYJv/8cyzvPt/fKdkRkJXc9tomkjsWA8NKT3bno6jRaJxkpChFRlZQUBXHnVYO8slsTfxSwpNhJ+CtZ2NOMas8l9zQnaFUpQcuLQUDFOCiZ2BwVb3yV7anlhE/LQkpdIFD4UhsI9qSuq4FZ34WExHLun7qT2KaVKJfw/cdN+fq9FkRGV/HQtBSatyonMyOEp8efRHGhbz9DM7+3XqNbcCAiMcAMoAfGLblJKXXchbHVLPyhA99824X7Jv52xP6EhBL69t5PZtbhnK6ffm7PTz8bP+p2Sfk8/vASr4Ib+KcoVZOk9oWcc2Ea995yJpVVNv41dTkrf2tO2s4opjx0GhPu901FqrJCmHT9KW5VJhcvfLSeVUtiCQ51MXBELndc0IfKShvRcRV1GzuK2+7fyh+/JfDMA71xOFyEhDp5btLhIjA3T0ymtNj3r4m/Clhh07OpPDWckocSoVIh5S4OJgVz8FqjVl3InALCPs6ldEJzcCrCX9hP6T9a4OwQghQ6we5dW8Os74KrSvjvlLakbIogLMLJq99sZM2v0Zx9eTZrlzbhszdbcuXte7ny//bxznNt6jZooa++ECizqJ7/a/ONl4F5SqmuGKWTPBKr2bipOUXFf5ZeHXfzH8x4r89xBWWHDUlj8RLvk2/9UZSqSZt2xSRviqW83IHLaWPD2ngGDdlHeloUGbsj/bBcQ5XJoXA4FEoJ5121n8+mt6Gy0vgYD+R5J1cbFlFFj775LPjKWK9cVWWjpLhm6W/FmWfv5+d5vml4gp8KWKVOHBvLqBjlrqITJKhIO4Qf1m+Qg65DIg2O1aU424Xg7GAk3qomdq8DnFnfhbzsYFI2GcIyZSV20lPCiG9RwaCzC1j0RQIAi75I4PRR+T5fwyxffSJANBksuzsi0gQYAtwAoJSqALxvYrgZ2H8PubnhpO6KPe4xQ85I48kpQ329hN+k7Yziutu2ENWkgopyG/0GZXndHT0eNpvildlradm2jG8/SiR5fRSt2pXRo98Brp+4i8pyGzP+3Z5tGzwX+01sVcqB/GAmPrGJ9p2LSNnShLee70L5QeNr0b1vPgV5IexN910Byh8FLPu+KlS0nfD/ZGJPrcB5Ugil45pCqI3QmTmE/FiEirBR9IwRoO0ZFSAQ+WgGcsBJxZBIyi/3rjVvBc1bldOxWynJayOJSagkL9u4H3nZwUTHV9azdz6gAmcW1coWXAcgG3hXRNaIyAx36fIjOEJVq+rYX/yQ4CrGXLGR/310fOm2Lp1zKC+3k7Y7xiT3vSc9LYrPPzyJp15axj9fXE5qShOcTnOGY10uYcLFfbh2aH869ywmqVMJdrsiskkVE6/sxYx/t2fyS1vx5t+mza44qWsR33/emruuHsTBMjtX3Ljr0OtDz/Gv9QZ+KmC5FPaUcsr/FkPRq21RoTZCZxktnoPXJ3BgZnsqhkUR8o27RegEx+YySu5rQdG/WxO8rATH2vqVVAwNd/LIG9t5619tKS32XDmswRMgLTgrA5wDo9zSG0qpPhilliYdfdARqlqOY6+XTEwsokXzYt54+Xtm/vcrEhJKmfbSXGJjDguDDD0zjcW/tLPkjXjDgm+TuPumoTw4/gyKCoP9av0ci5IiB+tXRNPvzHxyMoNZujAeELZtiEK5hGgvtA1ys0LJyQoheWMMAEt/aM5JXQ3hEpvdxelnZbFkgX8Bzh8FLFe8A1eCA2dXY7yucnAkjpQjxWkqhkUR/JuhIOVKcFDVIwwVbYdQG5X9wrHvqF2K0krsDhePvrGdn76OP6R4VZATRFxT437ENa3gQG79K6L5QqNR1fKDPcAepdQK9/PP8bF45q60WMZcdznX33ox1996MTk54Uy451zyCwypORHFmYPT+NmH8TeziY4xflBNm5dy+tB9/Lyo1np8ntmMrSQiyghcwSFO+pxeQPrOcJYtiqf3QKP10qpdGY4gFwe8GJPJzw0hOzOUVklGmb9e/XPZnWoE5D4D8tizK4LcLP/EXPxRwFJxDlxNHdj2GAHBsa4UZ9tgbBmHA2bQ8hKcrY0uX1XfcOy7KuCgC5wKx4YynG28G5c0D8XE51LZnRLG7LcTD+1dviiGkZcZEoQjL8th2cKYevLPTwKkBWfZGJxSar+IpItIF6VUMsbSr82enDvpvl/p2SOTJk3Kef+d2XzwcU/mLzzpuMef0j2LnNxw9md6Pv50xPX8VJSqyUNPr6RJkwqqqmy8MfUUiouCGTRkH7dP3EB0TAVPPL+cndujeexez9MuYptVcN+z27DZFSLwy7wEfl8chyPIxcSnt/PGN6upqhSmTuqMtxlKbz3XlfunbMAR5GL/njBeesKo5jJklP/dU/BfAatsXDMint8PVQpXiyBK72lO+CuZ2DMqUQKuZkGUjjcKvqooO+UXx9BkYjoIVPaLoKq/dy1os74L3fsVM/LSXFK3hvHad0Y1m/eeb82nbyTy0LQdnHNlNll7Q5gy/vjf6xPlqy80hNaZJ9SpquWXcZHeGGkiwRiC0TdWq+MciyYRLdXA7uNM90OXS7KuXFLV/kxL7OpySaDKze9er1A/UKjy/BoYDm/aRnW91DNVrTXT61bVshJL55iVUmuBentzGo3GfMwUnbGaBrmSQaPRNHB0gNNoNI0VsXBoy0x0gNNoNN7RQGZIPUEHOI1G4zV6DE6j0TRaAmWpVsMKcKUHLUvpsIKq1LT6dsFjXEVFlti1hVuTfmJFOgfAF3uWm27zstYDTbcJYIvyLa+zNqTYpNx+3YLTaDSNkgayDMsTdIDTaDTeowOcRqNpjOhEX41G06gRV2BEOB3gNBqNd+g8OOvwR8DkRNsNJF/NtJuQWM59z6cQm1CJUjD3k+Z8PTORDieXcOe/dhIU7MLpFF57vD3b1vs2U+ivr9/OaMGij5uhFJx9dRbn37Kfj59vze/zY7HZIDqhkgkv7iCuRSVVlcIb93dg54YInE5h2OXZXDph7wn1t5raBIgALrtpD7c8uIu/DxxAYb51teYCJU3EsnpwItJFRNbW2ApF5B5/bFYLmDwytj23DuvC8IsKaNvpYN0n1oPdQPLVbLvOKuG/zyQxbnRvJl5+Cudfs5+2J5Vy84NpfPhKayZc2IsPXmrDzQ/urhdfd28NY9HHzXju2428uGA9qxbFsndnKBfdvo//LNrA1AUbOHVEAbNeag3Asm/jqKwQ/vPDep6fu4EFHzQnK93zCiJm3ttqAaLxF/Vl/MW9OfXMfLr2MoqUJrQop8/pBWRmWFPd5AgCpB6cZQFOKZWslOqtlOoNnAqUAl/6Y9MvAZMTbDeQfDXbbn52MDs2GSI7ZSV20neEEd+8AqUgPNIJQHiUk9xM31oY/vq6JyWMzn2KCQlzYXdA94GF/D4vlvAo56Fjystsh0vrCRwsteOsgoqDNhxBLsIiPa+cbO5ndmwBIoBxk3fy9vPtTkhg0RV9j2QEsEMp5Vdm7LEETBIS/RftsMJuIPlqpd1mrQ7SsVsJyesieeupdtw8KY3//fIHt0zaxXsv+FZPz19f23YpZfOKKIryHZSX2Vj9Yww5e41Wz4fPteG20/qw5MsExtxnyPENOi+P0HAnt/Q9lXH9+3DhuH1ExTpru4Sp/h6NzaaY9tUaPv5tBWt+iyF5fRQDzsolJyuY1GR/1Ns8RGEIa3iy1TMnKsCNAT4+1gtHiM5Qe4E/vwRMTrDdQPLVKruh4U4eeW0bbz3VjtJiB+ddncn0Ke247sxTmf50O+55Zke9+Nq600EuvmMvT151Mv+6pivtupVidxgGxj6YzvSVaxhySQ5z3zUqGqesjcBmg//+sZo3lq3hm+mJ7E/zvBto9r09WoCoXZcSxtyezvsvn7iS/eLybKtvLA9wIhIMXAjMOtbrR4jOUPuXxh8BkxNtN5B8tcKu3eHikdeS+WlOAr8tMMpoj7w0+5D4yi/fx9OlV3G9+TryqmxemLeBp77YTGRMFYntjxwTO+PiHJbPdfv6VQK9hxXgCFJEJ1TR9bQidqz3vBS6VZ9ZtQDRoBG5tGhdzutfr+G9H1aS0KKcV2evJTbBZ5XOWqnOg9NdVINzgdVKKb9rW/sjYHKi7QaSr+bbVdzzzA7SU8L48p2Wh/bmZgZzygBjQLz3oEIydvkmaGOGrwdyjASC7Ixgls+N44yLcti787A/qxbE0qqjodqW0LKCjb81QSk4WGpj2+pIWnX0fJLAzHt7LAGiHZsjuer0Adww4jRuGHEaOftDuPPS3uTnWCS442n3tAF0UU9EmshVHKd76i3+CpicSLuB5KvZdrufWsTIS3JI3RrOtDnrAJg5tS2vPNyBcY/uwm5XVJTbeOXhDvXm6/O3daYo34Hdobh1SiqRMU5ev78De3eGIaJo2rqCcc/sBGD0Dft57d6O3DOiJygYfmU27bp5rrdq5r09ngDRiaYhtM48wWrRmXAgHeiglKpz2qiJxKkBMsIyfzTmY1U1EVepNYLNf/VqIsuL53DAmeOX6ExUTGvVZ8jdHh37yzcPNGrRmVLgxOiYaTSaE0agtOBO1CyqRqNpLCjAqTzbakFE2ojITyKyRUQ2icjd7v1xIrJQRLa7/8bWOGeyiKSISLKInFOXqzrAaTQarzFpFrUK+IdS6mRgIDBeRLoBk4AflFKdgB/cz3G/NgboDowGXhcRe20X0AFOo9F4jwmzqEqpfUqp1e7HRcAWoBVwETDTfdhM4GL344uAT5RS5UqpVCAF6F/bNXSA02g0XuNFCy6hOpHfvd12THsi7YA+wAqguVJqHxhBEGjmPqwVxqRlNXvc+45LwFUT0Wg09Yx3C+lz6ppFFZFI4AvgHqVUoRxr6Yf70ON4c1waXoCz1dql9g2X5+sGveL4H0TDw6J0INfB2pfX+Yo4rPlqXtZmkOk2v81YZbpNgPNbnWq6TaX8Xz8lgNQxgeCxLZEgjOD2oVJqtnt3pogkKqX2iUgikOXevwdoU+P01kCtdat0F1Wj0XiNKOXRVqsNo6n2NrBFKfVijZfmANe7H18PfF1j/xgRCRGR9kAn4PfartHwWnAajaZhY16tt8HAtcAGEVnr3vcQ8CzwmYjcDOwGrgBQSm0Skc+AzRgzsOOVUrV2z3SA02g0XmLOOlOl1K8ce1wNjBJrxzpnCjDF02voAKfRaLwmUFYy6ACn0Wi8pwFUCvEEHeA0Go13KPNmUa2mwQe4e19IY8DIAxTkOBg3shsAUTFVPPR6Ks3bVJCZHsyU/2tP8QHf30rTlhXc//JuYptVoVzw/QfxfPV2U1P8n7l8E2XFdlwuQ4zlzr918cte644HeeiNXYeet2hbwfsvtODLGc2Of5IH3PvibgaMLDLu81n++WjVZzbx+V0MGHGAglwHt5/d/YjXLrttP7c+ksGVvXpRmO/f19pmU7w6dxu5+4N47HrPSzp9PaMZ8z9KAAXnXJ3DRbdmHXpt9pvNeedfrflww1qi45z8NDuO2W8cVtbatSWMl+dtoUOPMq98tUphrU4CI75ZmyYiIhPdi2g3isjHIuJ1EawFs+J4+JqTjth35fj9rFkaxU1ndmfN0ij+Pt6/WprOKmH6P1ty69Cu3H1+Jy64IccUpapqHrjiJO4Y1dXv4AawZ0cod4zqyh2jujJhdBfKy2wsnRvjt90Fn8bx8Nj2ftsB6z6zhbPieeS6Tn/an5BYQd8zi8jcY06Bx4tvySZ9u3fKVLu2hjL/owRe/G4Lry7czO+LosnYadjIzghizZIomrY6nDM4/NI8Xl24hVcXbuEfr6TSrE2F18HNKoU1TzAjTeREYKVsYCvgLqCfUqoHYMdYKOsVG1dEUVRwZPLvoFEHWDTLqMK0aFY8g84p8MvXvKwgUjYYdc3KSuykp4SaIrhiNb3PKGJfWghZGf7/sDeuiKTIz5bPYVvWfGYbf/+zXYBxj6cz4+lWprQqEhIr6D+ikLkfe1fla8/2ULr2LSE0TGF3QI+BRSybFwPAf59ow40PZxw3L/znr+IYelGe175apbDmEQFS0dfqRF8HECYiDiCcOrKOPSU2oYq8LKOmfV5WEDHxnku41UXz1hV07FHG1tUmFXJUwtMf72Da3GTOHZtjjk03wy4qYPFXMabatAqrPrOBZxeQuz+Y1C3mfF63P5nBjKda4m3Cf1LXg2xcHklhnp2DZcKqH6PJ2RvMigXRxCdW0KH78Vtnv3wTx5CLvQ9wVimh1YkCXB5u9YxlY3BKqQwReQEjUa8MWKCUWnD0ce7Ft7cBhGJNdVhPCQ138uiMXbz5WEtKi81ZMjbx4k7kZQYRHV/Js5/sID0llI0r/Jd2cwS5GDjqAO88k2iCl4FJSKiLMRP28dA1nU2xVz1umLIhnJ6Dirw6t02ng1w+fj+PXtWZ0Agn7buVYbcrPn0lkX99tO245yWvDickzEW7rt53La1SWKvzujSM7qcnWNlFjcUob9IeaAlEiMg1Rx/njapWNfk5DuKaGf+p4ppVUpDrf5y2OxSPztjFj7NjTRnTqibPLW58IDeIpXOj6drbnFLcpw0vImVDOAU5/qsznQis+MwSk8pp0aaCN+ZtZubSDSQkVjDt+83ENvWtFdOtXwkDRxUyc/kmJr+eRq/BRTzwiudSvqOuyuXl+Vt4bvY2omKqaNamgszdwdx5djduGtCDnH3B3HNON/KzDr/3JV/71j0F69S6PMLl8myrZ6zsoo4EUpVS2UqpSmA2cLoZhpcvjGbkFbnGRa7IZZnfqlKKe6emk749lNnTzZk9BQgJcxIW4Tz0+NShRexK9l8gBmDYxfkB0z0FKz4z2JUcxpi+vbh+8ClcP/gUcvYFM+Fv3cjP9u1H/u6zLbmmX3euH9idZ+5IYt3SKP59l+daowVupa6sjCCWzY1lxOW5fLh+Pe+s2Mg7KzaSkFjBS/M3E9vM6J67XPDrt7EM8THAWaWwVie6iwoYXdOBbuGZMoylF16XXZg0LZWeg4qIjqvig5UbeH9qIp9Oa8HDb6YyekwuWRnBTLndv9m/7v1LGHlFPjs3h/L6wmQA3n0mkZU/NvHLbmzTKh5/OxUAux1++iqGVYv9swlG16zvkCJefrBN3Qd7yKTX0+g5qNi4z6s28/7U5sz3cqD9kC2LPrNJr+6k56AimsRW8f6K9XzwYkvmf5rgk49W8PStHQ4pdd0+ZTeRMbVXsdm4PJKExApaJPmmX2qVwponBEoX1WpVrSeBv2MsjF0D3KKUOm59nSYSpwbYR5nviC6XZN3gjBXlrQCxWXNvldP878K3ewKnXNIK9QOFKs+vmxsd3lIN6nSzR8fOX/9Uo1bVehx43MpraDSaE03DSAHxhAa/kkGj0TQwqlW1AgAd4DQajdcEyhicDnAajcZ7dIDTaDSNEgW4dIDTaDSNEj3J4DtWpXRYQYB8yFZiWTpHlXnri2tib+J/HuLRWJHOATAnY6XpNgePLjHHUIB89xtegNNoNA0bBTgbwDIFD9ABTqPReInC63Ir9YQOcBqNxnt0F1Wj0TRK9CyqRqNp1OgWnEajabQESICzumS56fQbVsiMX7by7tItXDnBP7EZq+0Gkq9m2p34/C4+Wb2ONxdu+tNrl922n3m7/6BJrH9pIPe+uJtP12/irR+T/bJTjc2meHX2ap540/D5jHOyeeObP/h28y906uFddd9j4c+9nTOjORPO6s744T34+r9HqmZ9+WYLLmx1GoV5R7ZVsjOCubJTX758s4Xfvv8JpcDp9GyrZ6xW1brbrai1SUTu8deeVSpCVtgNJF/Ntnsi1K/MVAEDuOi6DNJ3Hi6Zn7Y9gqfuOpmNq/wvIOnPvU3bGsaCjxKY+t0WXlm4kVWLotl7SK0rmLVLmhyh1lXNjCfa0He4hQI0f3XRGRHpAdwK9Ad6AeeLyJ+/9V5glYqQFXYDyVez7Z4I9SszVcDim5dz2tA85s863NpJ3xlORqo5GiH+3Nv07aF06VtCSJgLuwO6Dyxi2bxYAN5+og03PJz+p7KEy+fF0KJtOW27eCdD6BV/9QAHnAwsV0qVKqWqgJ+BS/wxaJWKkBV2A8lXK+1WY7b6lZmMe2gH77zQ3rKJQX/ubVLXMjYtj6Iwz055mY0/foxxq3XFEJ9YSfuj1LoOltr44rVExtxrioDdcVDGLKonWz1j5STDRmCKiMRjlCz/G8coWe6NqpZVKkJW2A0kX620C+arX5lJ/2G5FOQGk7IpilP6F1hyDX/ubZtOB7l0/D4eu6oLoREu2ncrxW5XzHolkSePodb10QutuOjW/YRFWJiIq0D91RN9lVJbROQ5YCFQDKzDKF1+9HHTgelglCyvzaZVKkJW2A0kX620C0eqXxm2DfWruy882WeBGLPo1reQgWflctrQPIKCXYRHOrnv31t54YGupl3D33s76qocRl1laOr+75lWxDSt4ucvQ7j77O5u+4Za19TvNrNtTQS/fRfLe1PaUFJoR2wQFOLi/BuzTHs/gF6qBaCUeht4G0BEngb2+GOvpopQ7v4ghl1UwLPjPVc9OpF2A8lXK+3CYfWramYu3cCd559MoUljaP7w3ovtee9FY7LilP4FXHZThqnBDfy/twU5DmISqsjOCGbZ3Fien7OFC285PBN7y4CevDh3M03iqnj2y62H9n80tSVhERYEN6UahCSgJ1j6DRORZkqpLBFpC1wKDPLHnlUqQlbYDSRfzbZ7ItSvzFQBOxaDRubwf4/sIDqukife3MTOrRE8esspPtny994+e+tJNdS60upU6zohNIAJBE+wWlXrFyAeqATuVUr9UNvxTSRODZARlvmjMR9xWPM/MpDKJTkLC023CVaVS9rH6nXl/qlq2RPUwLDzPDp2Qcn/GrWq1plW2tdoNPVBw0gB8YT6HwTRaDSBhV5sr9FoGisKawS0rSDg1qJqNJp6RrkLXnqy1YGIjBaRZBFJEZFJZruqW3AajcZrlAldVBGxA68BZ2OkkK0UkTlKqc1+G3ejW3AajcZ7zGnB9QdSlFI7lVIVwCfARWa6aWmaiLeISDaQ5sGhCUCOBS5ou4Hla6DZbQi+JimlmvpzMRGZ576mJ4QCNUunTHevXkJELgdGK6VucT+/FhiglJrgj381aVBdVE9vvIissiK3RtsNLF8DzW4g+VobSqnRJpk6Vj6eqS0u3UXVaDT1xR6gTY3nrQFTy6DoAKfRaOqLlUAnEWkvIsHAGGCOmRdoUF1UL5iu7VpmN5B8DTS7geSr5SilqkRkAjAfsAPvKKX+XOfeDxrUJINGo9GYie6iajSaRosOcBqNptEScAHOiqUdIvKOiGSJyEYz7LltthGRn0Rki1tV7G6T7IaKyO8iss5t90kz7NawbxeRNSLyrYk2d4nIBhFZKyJ/Klvvo80YEflcRLa677FftQbdNru4fazeCs1Qg3Pbnuj+vDaKyMci4n8RP8xXrmt0KKUCZsMYiNwBdACCMcqgdzPB7hCgL7DRRF8Tgb7ux1HANpN8FSDS/TgIWAEMNNHve4GPgG9NtLkLSDD5uzATuMX9OBiIseC7th8jMdZfW62AVCDM/fwz4AYT7PbA0D4Jx5gwXAR0MvM+BPoWaC04S5Z2KKWWAHn+2jnK5j6l1Gr34yJgC8YX3V+7SilV7H4a5N5MmSkSkdbAecAMM+xZhYg0wfin9DaAUqpCKVVg8mVGADuUUp6srPEEBxAmIg6MgGRGvpfpynWNjUALcK2A9BrP92BC0LAaEWkH9MFobZlhzy4ia4EsYKFSyhS7wEvAA4DZBfcVsEBE/nCrqPlLByAbeNfdnZ4hIhEm2K3JGOBjMwwppTKAF4DdwD7ggFJqgQmmNwJDRCReRMIxlOva1HHOX4pAC3CWL+0wGxGJBL4A7lFKmVLbWinlVEr1xsj87u8W2fYLETkfyFJK/eGvrWMwWCnVFzgXGC8iQ/y058AYUnhDKdUHKAFMK7XjTjq9EJhlkr1YjJ5Ge6AlECEi1/hrVym1BahWrpvHcZTr/soEWoCzfGmHmYhIEEZw+1ApNdts++5u2WLAjLWBg4ELRWQXRtf/LBH5wAS7KKX2uv9mAV9iDDX4wx5gT42W6+cYAc8szgVWK6Uy6zzSM0YCqUqpbKVUJTAbON0Mw0qpt5VSfZVSQzCGWbabYbexEGgBzvKlHWYhIoIxRrRFKfWiiXabikiM+3EYxo9na60neYBSarJSqrVSqh3Gff1RKeV3K0NEIkQkqvoxMAqja+WPr/uBdBHp4t41AjCthhhwFSZ1T93sBgaKSLj7ezECY0zWb0SkmftvtXKdmX4HPAG1VEtZtLRDRD4GhgEJIrIHeFwZmq7+MBi4FtjgHi8DeEgp9b2fdhOBme5igTbgM6WUaSkdFtAc+NL4XeMAPlJKzTPB7p3Ah+5/dDuBG02wiXss62xgnBn2AJRSK0Tkc2A1RhdyDeYtr/pCRKqV68YrpfJNstso0Eu1NBpNoyXQuqgajUbjMTrAaTSaRosOcBqNptGiA5xGo2m06ACn0WgaLTrABRAi4nRXudgoIrPcKQ2+2nrPrWqEe6lTt1qOHSYiXiemuquI/El96Xj7jzqmuLbXj3H8EyJyn7c+aho3OsAFFmVKqd5KqR5ABXB7zRfduXFeo5S6RdUutjsMkzLvNZoTiQ5wgcsvwEnu1tVPIvIRRlKxXUSeF5GVIrJeRMaBsbJCRKaJyGYR+Q5oVm1IRBaLSD/349Eistpdb+4Hd6GA24GJ7tbjme7VFF+4r7FSRAa7z40XkQXuBfBvcey1w0cgIl+5F+FvOnohvohMdfvyg4g0de/rKCLz3Of8IiJdTbmbmkZJQK1k0Bi4S+6ci7HAGoy1nT2UUqnuIHFAKXWaiIQAS0VkAUY1ky7AKRirCzYD7xxltynwX2CI21acUipPRN4EipVSL7iP+wj4j1LqV/cSofkYpXseB35VSv1TRM4DPKkccpP7GmHAShH5QimVC0RgrAf9h4g85rY9AWMFwO1Kqe0iMgB4HTjLh9uo+QugA1xgEVZj2dcvGGtdTwd+V0qluvePAnpWj68B0UAnjPppHyulnMBeEfnxGPYHAkuqbSmljlcjbyTQzb38CqCJe73pEIz1kCilvhMRT5YN3SUi1TXM2rh9zcUo2fSpe/8HwGx3ZZbTgVk1rh3iwTU0f1F0gAssytxlkg7h/qGX1NwF3KmUmn/UcX+j7tJS4sExYAxtDFJKlR3DF4/X/onIMIxgOUgpVSoii4HjlfJW7usWHH0PNJrjocfgGh/zgf9zl2pCRDq7q3gsAca4x+gSgeHHOHcZMFRE2rvPjXPvL8Iou17NAozuIu7jersfLgHGuvedC8TW4Ws0kO8Obl0xWpDV2IDqVujVGF3fQiBVRK5wX0NEpFcd19D8hdEBrvExA2N8bbUYIjpvYbTUv8SoFbYBeAOjvPURKKWyMcbNZovIOg53Eb8BLqmeZADuAvq5JzE2c3g290mMCrOrMbrKu+vwdR7gEJH1wL+A5TVeKwG6i8gfGGNs/3TvHwvc7PZvEyaUrNc0XnQ1EY1G02jRLTiNRtNo0QFOo9E0WnSA02g0jRYd4DQaTaNFBziNRtNo0QFOo9E0WnSA02g0jZb/BwmdcMJIYs0VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e67cc",
   "metadata": {},
   "source": [
    "We can see that the model confuses category 0 and 6, 2 and 6, 2 and 4, 4 and 6. Let's see what they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "485fad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0 is : T-shirt/top\n",
      "Category 1 is : Trouser\n",
      "Category 2 is : Pullover\n",
      "Category 3 is : Dress\n",
      "Category 4 is : Coat\n",
      "Category 5 is : Sandal\n",
      "Category 6 is : Shirt\n",
      "Category 7 is : Sneaker\n",
      "Category 8 is : Bag\n",
      "Category 9 is : Ankle boot\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(f'Category {i} is : {labels[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33697ae3",
   "metadata": {},
   "source": [
    "### Model confuses :\n",
    "\n",
    "- T-shirt/top with Shirt\n",
    "- Pullover and Shirt\n",
    "- Pullover and Coat\n",
    "- Coat and Shirt\n",
    "\n",
    "This makes sense, let's look at some misclassified items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87273e13",
   "metadata": {},
   "source": [
    "### Misclassified items \n",
    "\n",
    "Let's look at some misclassified items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7955f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img to display\n",
    "img = X_test.reshape(-1,28,28)\n",
    "# Show some misclassified examples\n",
    "misclassified_idx = np.where(y_pred != y_test)[0]\n",
    "figure = plt.figure(figsize=(16,16))\n",
    "cols, rows = 3, 3\n",
    "for k in range(1, cols*rows+1):\n",
    "    i = np.random.choice(misclassified_idx)\n",
    "    figure.add_subplot(rows, cols, k)\n",
    "    plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[y_pred[i]]));\n",
    "    plt.imshow(img[i], cmap='gray') \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
