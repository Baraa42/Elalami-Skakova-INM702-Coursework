{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4484a339",
   "metadata": {},
   "source": [
    "Used resources:\n",
    "- Lab 6 and 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97227f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4432939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid activation function with forward pass\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "#Sigmoid activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_sigmoid(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "#ReLU activation function with forward pass\n",
    "@np.vectorize\n",
    "def relu (x):\n",
    "  return max(0,x)\n",
    "\n",
    "#ReLU activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_relu (x):\n",
    "  if x<0:\n",
    "    return 0\n",
    "  if x>0:\n",
    "    return 1\n",
    "\n",
    "#loss method - cross entropy\n",
    "def cross_entropy(output, target):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "# output function - softmax\n",
    "def softmax(x):\n",
    "    a = x - np.max(x, axis=0, keepdims=True)\n",
    "    new_a = np.exp(a)\n",
    "    result = new_a / np.new_a(new_a, axis=0, keepdims=True)\n",
    "    \n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e400dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_nodes,\n",
    "                 learning_rate,\n",
    "                 epochs):\n",
    "        self.no_nodes = no_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    # bring as an output weigths list with weight considering input nodes and output nodes    \n",
    "    def create_weight_matrices(self): \n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        weights = []\n",
    "        for i in range(len(self.no_nodes)-1):\n",
    "            rad = 0.5 \n",
    "            X = truncated_normal(mean=1, sd=1, low=-rad, upp=rad)\n",
    "            weight = X.rvs((self.no_nodes[i], self.no_nodes[i+1]))\n",
    "            weights.append(weight)\n",
    "        return weights  \n",
    "    \n",
    "    #bias\n",
    "    def f_bias (self):\n",
    "        biases = []\n",
    "        for i in range(1, len(self.no_nodes)):\n",
    "            rad = 0.5 \n",
    "            tn = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            bias = tn.rvs(self.no_nodes[i]).reshape(-1,1) \n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, X_train_trans):\n",
    "        biases = self.f_bias()\n",
    "        weights = self.create_weight_matrices()\n",
    "        output_list = []\n",
    "        for i in range(len(weights)):\n",
    "            #input vector\n",
    "            if i == 0:\n",
    "                output_vector = np.dot(weights[i].T, X_train_trans) + biases[i]\n",
    "                output_vector_in = activation_function(output_vector)\n",
    "                output_list.append(output_vector_in)\n",
    "            else:\n",
    "                output_vector = np.dot (weights[i].T, output_list[i-1]) + biases[i]\n",
    "                output_vector_out = activation_function(output_vector)\n",
    "                output_list.append(output_vector_out)\n",
    "        return output_vector_out, output_list\n",
    "    \n",
    "\n",
    "    #training with forward pass and backpropagation\n",
    "    def train(self, X_train, y_train):\n",
    "        weights = self.create_weight_matrices()\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        X_train_trans = np.array(X_train, ndmin=2).T\n",
    "        y_train_trans = np.array(y_train, ndmin=2).T\n",
    "    \n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            # forward pass\n",
    "            forward = self.forward(X_train_trans)\n",
    "            output = forward[0]            \n",
    "            output_list = forward[1]\n",
    "            \n",
    "            #backprop   \n",
    "            for i in reversed(range(len(weights))):\n",
    "               \n",
    "                if i == (len(weights)-1):\n",
    "                    # derivative of the loss for the output\n",
    "                    output_errors = (y_train_trans - output)\n",
    "                    # derivative of the activation function\n",
    "                    derivative_output = activation_derivative (output)  \n",
    "                    tmp = output_errors * derivative_output\n",
    "                    # multiply with the previous activation (output_vector_hidden)\n",
    "                    who_update = self.learning_rate * (np.dot(tmp, output_list[i-1].T))\n",
    "                    weights[i] += who_update.T \n",
    "\n",
    "                elif i == 0:\n",
    "                    #from hidden to input layer\n",
    "                    hidden_errors = np.dot(weights[i+1], output_errors * derivative_output)\n",
    "                    derivative_hidden = activation_derivative(output_list[i])  \n",
    "                    tmp = hidden_errors * derivative_hidden\n",
    "                    wih_update = self.learning_rate * np.dot(tmp, X_train_trans.T)\n",
    "                    weights[i] += wih_update.T\n",
    "\n",
    "                elif i > 0 and i < (len(weights)-1):\n",
    "                   # hidden layers\n",
    "                    hidden_errors = np.dot(weights[i+1], output_errors * derivative_output)\n",
    "                    derivative_hidden = activation_derivative(output_list[i])  \n",
    "                    tmp = hidden_errors * derivative_hidden\n",
    "                    whh_update = self.learning_rate * np.dot(tmp, output_list[i-1].T)\n",
    "                    weights[i] += whh_update.T\n",
    "                \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def run(self, X_test, weights):\n",
    "        biases = self.f_bias()\n",
    "        for i in range(len(weights)):\n",
    "            if i == 0:\n",
    "                input_vector = np.array(X_test, ndmin=2).T\n",
    "                output_vector = np.dot(weights[0].T, input_vector) + biases[i]\n",
    "                output_vector = activation_function(output_vector)\n",
    "           \n",
    "            else:\n",
    "                output_vector = np.dot(weights[i].T, output_vector) + biases[i]\n",
    "                output_vector = activation_function(output_vector)                \n",
    "            y_hat = output_vector.T\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ec7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#introducing dataset and preparing for manipulation\n",
    "fashion = fashion_mnist.load_data()\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27344f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93f5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean (X_train, axis = (0,1,2))\n",
    "std = np.std (X_train, axis = (0,1,2))\n",
    "\n",
    "X_train = (X_train- mean)/(std+1e-7)\n",
    "X_test = (X_test- mean)/(std+1e-7)\n",
    "\n",
    "#reshaping datset input\n",
    "X_train = X_train.reshape((X_train.shape[0], 28*28))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28*28))\n",
    "\n",
    "#onehot encoding the output\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291ded7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = sigmoid\n",
    "activation_derivative = d_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6090b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork([784, 32, 10], 0.001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5daa5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbb68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.run(X_test, new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326351eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8999\n"
     ]
    }
   ],
   "source": [
    "y_hat[y_hat>0.5]=1\n",
    "y_hat[y_hat<0.5]=0\n",
    "accuracy = np.mean(sum(y_hat==y_test)/len(y_hat))\n",
    "print(f\"Accuracy :  {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
