{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e2b0c2",
   "metadata": {},
   "source": [
    "## Content:\n",
    "- [Part 1](#part1)- Importing the libraries, packages\n",
    "- [Part 2](#part2)- Useful Functions\n",
    "- [Part 3](#part3) -  One Hidden Layer Class\n",
    "- [Part 4](#part4) -  Two Hidden Layers Class \n",
    "- [Part 5](#part5) -  Loading Fashion MNIST \n",
    "- [Part 6](#part6)-  Loading Fashion MNIST\n",
    "- [Part 7](#part7)-  Fashion MNIST One Hidden Layer\n",
    "- [Part 8](#part8)) -  Fashion MNIST Two Hidden Layers\n",
    "- [Part 9](#part9) -  Results \n",
    "- [Part 10](#part10) -  --\n",
    "- [Part 11](#part11) -  --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5b78e",
   "metadata": {},
   "source": [
    "Weight initialisation :\n",
    "\n",
    "- https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "- https://www.deeplearning.ai/ai-notes/initialization/\n",
    "- https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ab8ba",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "### Part 1 -   Importing the libraries, packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "968d07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import random \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd83fdb",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "### Part 2 -   Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "ae6751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "0936fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def softmax(X):\n",
    "    e = np.exp(X - np.max(X))\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(target, output):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "def cross_entropy_matrix(output, target):\n",
    "    target = np.array(target)\n",
    "    output = np.array(output)\n",
    "    product = target*np.log(output)\n",
    "    errors = -np.sum(product, axis=1)\n",
    "    m = len(errors)\n",
    "    errors = np.sum(errors) / m\n",
    "    return errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ds(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "\n",
    "def dr(x):\n",
    "    dr = (np.sign(x) + 1) / 2\n",
    "    return dr\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-x)\n",
    "    return (a-b)/(a+b)\n",
    "\n",
    "def dt(x):\n",
    "    return 1-tanh(x)**2\n",
    "    \n",
    "def leaky(x,a):\n",
    "    leaky = np.maximum(x,0)*x + a*np.minimum(x,0)\n",
    "    return leaky\n",
    "\n",
    "def dl(x,a):\n",
    "    dl = (np.sign(x)+1)/2 - a*(np.sign(x)-1)/2\n",
    "    return dl\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid :\n",
    "        return ds\n",
    "    if f == tanh :\n",
    "        return dt\n",
    "    if f == relu :\n",
    "        return dr\n",
    "    if f == leaky :\n",
    "        return dl\n",
    "    return None\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i][y[i]]=1\n",
    "    return ind\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f8411",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "### Part 3 -   One Hidden Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7220f",
   "metadata": {},
   "source": [
    "# One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5c79",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden x N_features\n",
    "- **b1**    : Hidden\n",
    "- **W2**    : Output x Hidden\n",
    "- **b2**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "26933734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenOne:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes,\n",
    "                 activation_hidden,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes = hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.hidden_derivative = derivative(self.activation_hidden)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):       \n",
    "        if self.activation_hidden == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes/2 )  # output x hidden\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes )  # output x hidden\n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    def create_biases(self):    \n",
    "        #tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        #self.b1 = tn.rvs(self.hidden_nodes).reshape(-1,1) \n",
    "        #self.b2 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "        self.b1 =  np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.b2 = np.zeros((self.output_nodes, 1 ))\n",
    "          \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1 # Hidden x N_samples\n",
    "        A1 = self.activation_hidden(Z1)      # Hidden x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2  # Output x N_samples\n",
    "        A2 = softmax(Z2)      #Output x N_samples\n",
    "        return A2, Z2, A1, Z1\n",
    "    \n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backpropSGD(self, X, target):\n",
    "        m = X.shape[0]                  #N_samples\n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(m))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        for i in range(m) :\n",
    "            # Forward prop\n",
    "            x = X_SGD[i,:].reshape(1,-1)                   # 1 x N_features\n",
    "            a2, z2, a1, z1 = self.forward(x)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,i].reshape(-1,1), a2)/m\n",
    "            # deltas\n",
    "            dz2 = a2 - target[:,i].reshape(-1,1)                    #Output x 1\n",
    "            dW2 = dz2.dot(a1.T)                                     #Output x hidden\n",
    "            db2 = dz2                                               #Output x 1\n",
    "            dz1 = self.W2.T.dot(dz2)*self.hidden_derivative(z1)     # Hidden x 1\n",
    "            dW1 = dz1.dot(x)                                        # Hidden x N_Features\n",
    "            db1 = dz1                                               # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A2)/pass_length\n",
    "            # deltas\n",
    "            dZ2 = A2 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   #Output x hidden\n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)              # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              #Hidden x1                                            # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backprop_adam_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)  \n",
    "            t = target_SGD[:,k:k+batch_size].reshape(-1,batch_size)\n",
    "            cost = cost + self.backpropADAM(X, t)/pass_length\n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2    \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "    def predict(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "             \n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs     \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "       \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a904e9",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "### Part 4 -   Two Hidden Layers Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1ee59",
   "metadata": {},
   "source": [
    "# Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1383e",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden1 x N_features\n",
    "- **b1**    : Hidden1\n",
    "- **W2**    : Hidden2 x Hidden1\n",
    "- **b2**    : Hidden2\n",
    "- **W3**    : Output x Hidden\n",
    "- **b3**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "de86d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenTwo:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes_1,\n",
    "                 hidden_nodes_2,\n",
    "                 activation_hidden_1,\n",
    "                 activation_hidden_2,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                 \n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes_1 = hidden_nodes_1    \n",
    "        self.hidden_nodes_2 = hidden_nodes_2    \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden_1 = activation_hidden_1\n",
    "        self.activation_hidden_2 = activation_hidden_2\n",
    "        self.hidden_derivative_1 = derivative(self.activation_hidden_1)\n",
    "        self.hidden_derivative_2 = derivative(self.activation_hidden_2)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        if self.activation_hidden_1 == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1/2 )  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2/2 )  # output x hidden2\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1)  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2)  # output x hidden2\n",
    "        \n",
    "    def create_biases(self):  \n",
    "        self.b1 =  np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.b2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.b3 = np.zeros((self.output_nodes, 1 ))\n",
    "     \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Vdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Vdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Sdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Sdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1      # Hidden1 x N_samples\n",
    "        A1 = self.activation_hidden_1(Z1)      # Hidden1 x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2      # Hidden2 x N_samples\n",
    "        A2 = self.activation_hidden_2(Z2)      # Hidden2 x N_samples\n",
    "        Z3 = self.W3.dot(A2) + self.b3       # Output x N_samples\n",
    "        A3 = softmax(Z3)                     #Output x N_samples\n",
    "        return A3, Z3, A2, Z2, A1, Z1\n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N_samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "     \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr*dW3\n",
    "        self.b3 -= lr*db3\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A3)/pass_length\n",
    "            # deltas\n",
    "            dZ3 = A3 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW3 = dZ3.dot(A2.T)/batch_size                                   #Output x hidden_2\n",
    "            db3 = np.sum(dZ3, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)            # Hidden2 x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   # Hidden2 x Hidden1 \n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              # Hidden2 x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)            # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              # Hidden x 1                        \n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W3 -= lr*dW3\n",
    "            self.b3 -= lr*db3\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N samples\n",
    "        m = X.shape[0]   \n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdw3 = beta1*self.Vdw3 + (1-beta1)*dW3\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        self.Vdb3 = beta1*self.Vdb3 + (1-beta1)*db3\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdw3 = beta2*self.Sdw3 + (1-beta2)*dW3**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2\n",
    "        self.Sdb3 = beta2*self.Sdb3 + (1-beta2)*db3**2  \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr * self.Vdw3 / (np.sqrt(self.Sdw3)+1e-8)\n",
    "        self.b3 -= lr * self.Vdb3 / (np.sqrt(self.Sdb3)+1e-8)\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "      \n",
    "    def predict(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A3\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    # To be deleted               \n",
    "    def xrun(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        for i in range(epochs):\n",
    "            cost = self.backprop(X_train, target)\n",
    "            costs.append(cost)\n",
    "            if i%10 == 0:\n",
    "                print(f'Loss after epoch {i} : {cost}')\n",
    "        costs.pop(0)\n",
    "        return costs  \n",
    "         \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    \n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 3epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 4epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    print(f'Loss after epoch {i} : {cost}')\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 5epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 6epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 7epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after 8epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "896a8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 9,\n",
    "               hidden_nodes_2 = 7,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu,\n",
    "               optimizer='adam',\n",
    "              #batch_size=200\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "bc8f6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1975488547415771\n",
      "Loss after epoch 20 : 0.14997360424852205\n",
      "Loss after epoch 30 : 0.13539839591865355\n",
      "Loss after epoch 40 : 0.12082195270255933\n",
      "Loss after epoch 50 : 0.10849031963453487\n",
      "Loss after epoch 60 : 0.09933015158491876\n",
      "Loss after epoch 70 : 0.09242548581001123\n",
      "Loss after epoch 80 : 0.08535395251213562\n",
      "Loss after epoch 90 : 0.07905414503163215\n",
      "Loss after epoch 100 : 0.07459900842701588\n",
      "Loss after epoch 110 : 0.07104861957478723\n",
      "Loss after epoch 120 : 0.06828063573786859\n",
      "Loss after epoch 130 : 0.06520633758146221\n",
      "Loss after epoch 140 : 0.06268914849925311\n",
      "Loss after epoch 150 : 0.060934624902296466\n",
      "Loss after epoch 160 : 0.059322254995836946\n",
      "Loss after epoch 170 : 0.05824582616622491\n",
      "Loss after epoch 180 : 0.057193997812181624\n",
      "Loss after epoch 190 : 0.05634407472788578\n",
      "Loss after epoch 200 : 0.05559537018714807\n",
      "Loss after epoch 210 : 0.05500607944048086\n",
      "Loss after epoch 220 : 0.05443849314650221\n",
      "Loss after epoch 230 : 0.05385838817668915\n",
      "Loss after epoch 240 : 0.05339052397527112\n",
      "Loss after epoch 250 : 0.052943424384366805\n",
      "Loss after epoch 260 : 0.05253738600565815\n",
      "Loss after epoch 270 : 0.05215641875217221\n",
      "Loss after epoch 280 : 0.05179315973082056\n",
      "Loss after epoch 290 : 0.05147537991815006\n",
      "Loss after epoch 300 : 0.05113874537147515\n",
      "Loss after epoch 310 : 0.050830386677397844\n",
      "Loss after epoch 320 : 0.05054063128596157\n",
      "Loss after epoch 330 : 0.05028095664652804\n",
      "Loss after epoch 340 : 0.04997781515406134\n",
      "Loss after epoch 350 : 0.04974307655820771\n",
      "Loss after epoch 360 : 0.04948550298477338\n",
      "Loss after epoch 370 : 0.04928404200560364\n",
      "Loss after epoch 380 : 0.049046284353539235\n",
      "Loss after epoch 390 : 0.04883739242300195\n",
      "Loss after 1epoch 401 : 0.048654915259337586\n"
     ]
    }
   ],
   "source": [
    "c=nn.run(X_train, y_train_cat, epochs=400 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "4d435827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8118"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "57d8fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 5,\n",
    "               #hidden_nodes_2 = 1,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               #activation_hidden_2 = relu,\n",
    "               optimizer='minibatch',\n",
    "              batch_size=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "d0f73c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.08597256371026611\n",
      "Loss after epoch 20 : 0.07169481260925979\n",
      "Loss after epoch 30 : 0.06567010778371164\n",
      "Loss after epoch 40 : 0.0620478213421242\n",
      "Loss after epoch 50 : 0.05950785690640602\n",
      "Loss after epoch 60 : 0.05757478829274955\n",
      "Loss after epoch 70 : 0.056051360056106406\n",
      "Loss after epoch 80 : 0.05482564820920087\n",
      "Loss after epoch 90 : 0.05382021171029689\n",
      "Loss after epoch 100 : 0.052978879908718995\n",
      "Loss after epoch 110 : 0.05226140707559768\n",
      "Loss after epoch 120 : 0.05163947120984345\n",
      "Loss after epoch 130 : 0.051092929718120156\n",
      "Loss after epoch 140 : 0.05060807398578545\n",
      "Loss after epoch 150 : 0.050173258883087074\n",
      "Loss after epoch 160 : 0.04978128511246916\n",
      "Loss after epoch 170 : 0.04942493618253959\n",
      "Loss after epoch 180 : 0.04909974908371374\n",
      "Loss after epoch 190 : 0.04880092430208747\n",
      "Loss after epoch 200 : 0.048525469148654576\n",
      "Loss after epoch 210 : 0.048270381760029984\n",
      "Loss after epoch 220 : 0.048033530775136404\n",
      "Loss after epoch 230 : 0.04781163041760578\n",
      "Loss after epoch 240 : 0.04760384867747466\n",
      "Loss after epoch 250 : 0.047407687836759395\n",
      "Loss after epoch 260 : 0.04722157935607572\n",
      "Loss after epoch 270 : 0.04704549836988835\n",
      "Loss after epoch 280 : 0.04687827713495671\n",
      "Loss after epoch 290 : 0.04671871032100539\n",
      "Loss after epoch 300 : 0.04656555049237146\n",
      "Loss after epoch 310 : 0.04641961949408336\n",
      "Loss after epoch 320 : 0.04627971255766707\n",
      "Loss after epoch 330 : 0.04614595049651034\n",
      "Loss after epoch 340 : 0.04601791607354456\n",
      "Loss after epoch 350 : 0.04589458398749881\n",
      "Loss after epoch 360 : 0.04577525373177072\n",
      "Loss after epoch 370 : 0.045659454133379226\n",
      "Loss after epoch 380 : 0.0455481768166657\n",
      "Loss after epoch 390 : 0.04544054654851674\n",
      "Loss after epoch 401 : 0.04534677023976035\n"
     ]
    }
   ],
   "source": [
    "c=nn.run(X1, y11, epochs=400 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "1bbf046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8292666666666667\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "97e00dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = nn.backprop(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "43dc61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3, Z3, A2, Z2, A1, Z1 = nn.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a9285bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10485694 -0.89514306 -0.89514306 ...  0.10485694 -0.89514306\n",
      "   0.10485694]\n",
      " [ 0.06021112  0.06021112  0.06021112 ...  0.06021112  0.06021112\n",
      "   0.06021112]\n",
      " [ 0.0907259   0.0907259   0.0907259  ...  0.0907259   0.0907259\n",
      "   0.0907259 ]\n",
      " ...\n",
      " [ 0.09401693  0.09401693  0.09401693 ...  0.09401693  0.09401693\n",
      "   0.09401693]\n",
      " [ 0.09631682  0.09631682  0.09631682 ...  0.09631682  0.09631682\n",
      "   0.09631682]\n",
      " [-0.86754949  0.13245051  0.13245051 ...  0.13245051  0.13245051\n",
      "   0.13245051]]\n"
     ]
    }
   ],
   "source": [
    "print(A3-y_train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca59cb4",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "### Part 6 -  Loading Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "2b19461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "cfcacf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "8bf5c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "58f6e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "c94784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[1]\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "d4d38a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(N_train, M*M, 1).squeeze()\n",
    "X_test = X_test.reshape(N_test, M*M, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "9846a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "7bbb3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX = 255\n",
    "#X_train = X_train/ MAX\n",
    "#X_test =X_test/ MAX\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "8a528470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897e6a3",
   "metadata": {},
   "source": [
    "# Fashion MNIST with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "1e4d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "e552d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.19769455850518092\n",
      "Loss after epoch 20 : 0.18447470664662308\n",
      "Loss after epoch 30 : 0.17788118929486993\n",
      "Loss after epoch 40 : 0.1733091170234118\n",
      "Loss after epoch 50 : 0.16971472610771088\n",
      "Loss after epoch 60 : 0.1667337580919098\n",
      "Loss after epoch 70 : 0.16420194915662806\n",
      "Loss after epoch 80 : 0.16200510514988745\n",
      "Loss after epoch 90 : 0.1600520905801858\n",
      "Loss after epoch 100 : 0.15827740724881514\n",
      "Loss after epoch 110 : 0.15663658039234027\n",
      "Loss after epoch 120 : 0.1550994347574728\n",
      "Loss after epoch 130 : 0.15364506952402274\n",
      "Loss after epoch 140 : 0.15225863050225014\n",
      "Loss after epoch 150 : 0.15092928672653272\n",
      "Loss after epoch 160 : 0.14964894702059203\n",
      "Loss after epoch 170 : 0.14841142958676912\n",
      "Loss after epoch 180 : 0.14721191364184538\n",
      "Loss after epoch 190 : 0.14604656980465372\n",
      "Loss after epoch 200 : 0.1449123055584184\n",
      "Loss after epoch 210 : 0.1438065860696706\n",
      "Loss after epoch 220 : 0.1427273054123765\n",
      "Loss after epoch 230 : 0.1416726922897992\n",
      "Loss after epoch 240 : 0.14064123983480328\n",
      "Loss after epoch 250 : 0.1396316524723903\n",
      "Loss after epoch 260 : 0.13864280506148538\n",
      "Loss after epoch 270 : 0.13767371107186097\n",
      "Loss after epoch 280 : 0.13672349760577704\n",
      "Loss after epoch 290 : 0.13579138574827684\n",
      "Loss after epoch 300 : 0.1348766751299455\n",
      "Loss after epoch 310 : 0.13397873183564085\n",
      "Loss after epoch 320 : 0.1330969789905208\n",
      "Loss after epoch 330 : 0.13223088951702233\n",
      "Loss after epoch 340 : 0.13137998060163694\n",
      "Loss after epoch 350 : 0.13054380923437928\n",
      "Loss after epoch 360 : 0.1297219679107437\n",
      "Loss after epoch 370 : 0.12891407977925368\n",
      "Loss after epoch 380 : 0.1281197935228744\n",
      "Loss after epoch 390 : 0.12733877906290036\n",
      "Loss after epoch 400 : 0.1265707245719751\n",
      "Loss after epoch 410 : 0.1258153342330192\n",
      "Loss after epoch 420 : 0.12507232619035685\n",
      "Loss after epoch 430 : 0.12434143073746648\n",
      "Loss after epoch 440 : 0.12362238889782456\n",
      "Loss after epoch 450 : 0.12291495127804584\n",
      "Loss after epoch 460 : 0.12221887699558617\n",
      "Loss after epoch 470 : 0.12153393267041443\n",
      "Loss after epoch 480 : 0.1208598916101242\n",
      "Loss after epoch 490 : 0.1201965332646083\n",
      "Loss after epoch 501 : 0.11960846687787019\n"
     ]
    }
   ],
   "source": [
    "c = nn.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "c62e0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7287\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "1168acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5\n",
    "nn = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='minibatch',\n",
    "                delta_stop = 1e-7,\n",
    "                patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "5a65f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.07201086010993014\n",
      "Loss after epoch 20 : 0.06456045855266555\n",
      "Loss after epoch 30 : 0.06075946433965692\n",
      "Loss after epoch 40 : 0.058230730209001644\n",
      "Loss after epoch 50 : 0.05641187204620377\n",
      "Loss after epoch 60 : 0.05500763006864552\n",
      "Loss after epoch 70 : 0.05388005931007338\n",
      "Loss after epoch 80 : 0.05294915246231096\n",
      "Loss after epoch 90 : 0.052159997077627474\n",
      "Loss after epoch 100 : 0.05147454478004901\n",
      "Loss after epoch 110 : 0.050868256916434304\n",
      "Loss after epoch 120 : 0.050330873441123324\n",
      "Loss after epoch 130 : 0.04985564676707296\n",
      "Loss after epoch 140 : 0.04942866576952212\n",
      "Loss after epoch 150 : 0.04903962599001507\n",
      "Loss after epoch 160 : 0.04868285820185186\n",
      "Loss after epoch 170 : 0.04835792550200942\n",
      "Loss after epoch 180 : 0.048062425352798074\n",
      "Loss after epoch 190 : 0.047790328795408774\n",
      "Loss after epoch 201 : 0.04756171465217987\n"
     ]
    }
   ],
   "source": [
    "c = nn.run(X_train, y_train_cat, epochs=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "d7256149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8241\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031658ae",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d447d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_SGD = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='SGD',\n",
    "               #batch_size = 28,\n",
    "                delta_stop = 1e-3,\n",
    "                patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b3b06f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.05229169746879698\n",
      "Loss after epoch 20 : 0.050989022653112885\n",
      "Loss after epoch 30 : 0.05047409297339392\n",
      "Loss after epoch 40 : 0.05006307293211981\n",
      "Loss after epoch 51 : 0.04998152154949375\n"
     ]
    }
   ],
   "source": [
    "c = nn_SGD.run(X_train, y_train_cat, epochs=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6e90b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7947\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6a5f0",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "283a815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_adam = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "               optimizer='adam',\n",
    "               #batch_size = 28,\n",
    "                #delta_stop = 1e-3,\n",
    "                #patience = 5,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4ea52b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.23329427329855534\n",
      "Loss after epoch 20 : 0.22581268191815174\n",
      "Loss after epoch 30 : 0.21756016625246444\n",
      "Loss after epoch 40 : 0.2123454866753959\n",
      "Loss after epoch 50 : 0.20782633720502175\n",
      "Loss after epoch 60 : 0.20225578209607875\n",
      "Loss after epoch 70 : 0.19722409237726474\n",
      "Loss after epoch 80 : 0.1927940246394797\n",
      "Loss after epoch 90 : 0.18775465784580125\n",
      "Loss after epoch 100 : 0.18286143393355006\n",
      "Loss after epoch 110 : 0.179449483118376\n",
      "Loss after epoch 120 : 0.17719147826304704\n",
      "Loss after epoch 130 : 0.17550718613772454\n",
      "Loss after epoch 140 : 0.1741688920517013\n",
      "Loss after epoch 150 : 0.17307389693182942\n",
      "Loss after epoch 160 : 0.17215258888543558\n",
      "Loss after epoch 170 : 0.1713067040286042\n",
      "Loss after epoch 180 : 0.1704599507829086\n",
      "Loss after epoch 190 : 0.16944868175105815\n",
      "Loss after epoch 200 : 0.16803511558947992\n",
      "Loss after epoch 210 : 0.16629823343822747\n",
      "Loss after epoch 220 : 0.16506756526536606\n",
      "Loss after epoch 230 : 0.16371251732833814\n",
      "Loss after epoch 240 : 0.16212530740493597\n",
      "Loss after epoch 250 : 0.16063024544099513\n",
      "Loss after epoch 260 : 0.1596440995742393\n",
      "Loss after epoch 270 : 0.15893763904438937\n",
      "Loss after epoch 280 : 0.15832828543660246\n",
      "Loss after epoch 290 : 0.1578148706405395\n",
      "Loss after 1epoch 301 : 0.1573958698976654\n"
     ]
    }
   ],
   "source": [
    "c = nn_adam.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "681780b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7947\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4e772",
   "metadata": {},
   "source": [
    "# 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "14c1d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 5,\n",
    "               hidden_nodes_2 = 7,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu,\n",
    "               optimizer='adam',\n",
    "              #delta_stop = 1e-5,\n",
    "              #patience=5\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "8dc1dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.23009918657484552\n",
      "Loss after epoch 20 : 0.20925003257289768\n",
      "Loss after epoch 30 : 0.19687719692833824\n",
      "Loss after epoch 40 : 0.1830438426783252\n",
      "Loss after epoch 50 : 0.17300164977829482\n",
      "Loss after epoch 60 : 0.1633852997230326\n",
      "Loss after epoch 70 : 0.15475056906538215\n",
      "Loss after epoch 80 : 0.14603352547099815\n",
      "Loss after epoch 90 : 0.13740840593385814\n",
      "Loss after epoch 100 : 0.1305420250692707\n",
      "Loss after epoch 110 : 0.12421605815376298\n",
      "Loss after epoch 120 : 0.11855536991873979\n",
      "Loss after epoch 130 : 0.11297379956453496\n",
      "Loss after epoch 140 : 0.1063691771139391\n",
      "Loss after epoch 150 : 0.09924395750899022\n",
      "Loss after epoch 160 : 0.09293506478708678\n",
      "Loss after epoch 170 : 0.0884201466092327\n",
      "Loss after epoch 180 : 0.08514571794258906\n",
      "Loss after epoch 190 : 0.08252271186843954\n",
      "Loss after epoch 200 : 0.08016558721847178\n",
      "Loss after epoch 210 : 0.07820037675723318\n",
      "Loss after epoch 220 : 0.07662357319133095\n",
      "Loss after epoch 230 : 0.07534951891270372\n",
      "Loss after epoch 240 : 0.07426149051262518\n",
      "Loss after epoch 250 : 0.07326491128926658\n",
      "Loss after epoch 260 : 0.07236944643555333\n",
      "Loss after epoch 270 : 0.07155769904895488\n",
      "Loss after epoch 280 : 0.07080840353019328\n",
      "Loss after epoch 290 : 0.07010440714313587\n",
      "Loss after epoch 300 : 0.06943946973441535\n",
      "Loss after epoch 310 : 0.06881287890128478\n",
      "Loss after epoch 320 : 0.06822220096015842\n",
      "Loss after epoch 330 : 0.06767571826644363\n",
      "Loss after epoch 340 : 0.06717422561748802\n",
      "Loss after epoch 350 : 0.06669187431872327\n",
      "Loss after epoch 360 : 0.06621943821464064\n",
      "Loss after epoch 370 : 0.06577187386506314\n",
      "Loss after epoch 380 : 0.06535031900857631\n",
      "Loss after epoch 390 : 0.06495463073841164\n",
      "Loss after epoch 400 : 0.06458704471858404\n",
      "Loss after epoch 410 : 0.0642439281001637\n",
      "Loss after epoch 420 : 0.06392091110174031\n",
      "Loss after epoch 430 : 0.0636086643628413\n",
      "Loss after epoch 440 : 0.06330831959904278\n",
      "Loss after epoch 450 : 0.06302189961501167\n",
      "Loss after epoch 460 : 0.06274441752467823\n",
      "Loss after epoch 470 : 0.062483793696241255\n",
      "Loss after epoch 480 : 0.06223645710419121\n",
      "Loss after epoch 490 : 0.06199820647968754\n",
      "Loss after epoch 500 : 0.06176707193248624\n",
      "Loss after epoch 510 : 0.06154587423860521\n",
      "Loss after epoch 520 : 0.0613310730763439\n",
      "Loss after epoch 530 : 0.06112568060032136\n",
      "Loss after epoch 540 : 0.06092470664085353\n",
      "Loss after epoch 550 : 0.06072606131716795\n",
      "Loss after epoch 560 : 0.060530635950777505\n",
      "Loss after epoch 570 : 0.06033848437439993\n",
      "Loss after epoch 580 : 0.060147632554802814\n",
      "Loss after epoch 590 : 0.0599552989837943\n",
      "Loss after 1epoch 601 : 0.0597821826185979\n"
     ]
    }
   ],
   "source": [
    "c= nn.run(X_train, y_train_cat, epochs=600 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "221879da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7435\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "18af2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = 64,\n",
    "               hidden_nodes_2 = 32,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu,\n",
    "               optimizer='adam',\n",
    "               delta_stop = 1e-5,\n",
    "               patience=5\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "03e71832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 0.3347957865433994\n",
      "Loss after epoch 1 : 0.5320168134159778\n",
      "Loss after epoch 2 : 1.157130499416637\n",
      "Loss after epoch 3 : 1.5418694393832684\n",
      "Loss after epoch 4 : 0.5432880684295185\n",
      "Loss after epoch 5 : 0.3182735420275621\n",
      "Loss after epoch 6 : 0.23358968794332943\n",
      "Loss after epoch 7 : 0.19897185593060784\n",
      "Loss after epoch 8 : 0.18063892545249638\n",
      "Loss after epoch 9 : 0.1791005209125011\n",
      "Loss after epoch 10 : 0.18049939152442432\n",
      "Loss after epoch 10 : 0.18049939152442432\n",
      "Loss after epoch 11 : 0.1844177825873545\n",
      "Loss after epoch 12 : 0.18075155984887634\n",
      "Loss after epoch 13 : 0.17418912013981377\n",
      "Loss after epoch 14 : 0.16705448213065835\n",
      "Loss after epoch 15 : 0.16054894526335764\n",
      "Loss after epoch 16 : 0.15604620473849787\n",
      "Loss after epoch 17 : 0.17019228153304622\n",
      "Loss after epoch 18 : 0.1552243908167947\n",
      "Loss after epoch 19 : 0.14732152623433026\n",
      "Loss after epoch 20 : 0.14343440518989856\n",
      "Loss after epoch 20 : 0.14343440518989856\n",
      "Loss after epoch 21 : 0.14389630115708066\n",
      "Loss after epoch 22 : 0.14001972816354902\n",
      "Loss after epoch 23 : 0.1416655513831446\n",
      "Loss after epoch 24 : 0.14191214204208782\n",
      "Loss after epoch 25 : 0.13654424339730062\n",
      "Loss after epoch 26 : 0.13289195998461303\n",
      "Loss after epoch 27 : 0.1282002879509425\n",
      "Loss after epoch 28 : 0.13128353420972955\n",
      "Loss after epoch 29 : 0.1397026488623973\n",
      "Loss after epoch 30 : 0.15034425993094766\n",
      "Loss after epoch 30 : 0.15034425993094766\n",
      "Loss after epoch 31 : 0.14690885259316805\n",
      "Loss after epoch 32 : 0.13112625855546203\n",
      "Loss after epoch 33 : 0.12553133105178824\n",
      "Loss after epoch 34 : 0.13075989387393608\n",
      "Loss after epoch 35 : 0.13581793111145796\n",
      "Loss after epoch 36 : 0.13821055187523498\n",
      "Loss after epoch 37 : 0.13781914014031224\n",
      "Loss after epoch 38 : 0.13451347900900734\n",
      "Loss after epoch 39 : 0.12986648184948762\n",
      "Loss after epoch 40 : 0.12386541796389011\n",
      "Loss after epoch 40 : 0.12386541796389011\n",
      "Loss after epoch 41 : 0.11964333895527655\n",
      "Loss after epoch 42 : 0.11679085636347414\n",
      "Loss after epoch 43 : 0.13871292704729868\n",
      "Loss after epoch 44 : 0.12301580120908434\n",
      "Loss after epoch 45 : 0.12600891853028365\n",
      "Loss after epoch 46 : 0.12507476931121622\n",
      "Loss after epoch 47 : 0.12184772239448469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baraa/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/baraa/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in multiply\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 48 : nan\n",
      "Loss after epoch 49 : nan\n",
      "Loss after epoch 50 : nan\n",
      "Loss after epoch 50 : nan\n",
      "Loss after epoch 51 : nan\n",
      "Loss after epoch 52 : nan\n",
      "Loss after epoch 53 : nan\n",
      "Loss after epoch 54 : nan\n",
      "Loss after epoch 55 : nan\n",
      "Loss after epoch 56 : nan\n",
      "Loss after epoch 57 : nan\n",
      "Loss after epoch 58 : nan\n",
      "Loss after epoch 59 : nan\n",
      "Loss after epoch 60 : nan\n",
      "Loss after epoch 60 : nan\n",
      "Loss after epoch 61 : nan\n",
      "Loss after epoch 62 : nan\n",
      "Loss after epoch 63 : nan\n",
      "Loss after epoch 64 : nan\n",
      "Loss after epoch 65 : nan\n",
      "Loss after epoch 66 : nan\n",
      "Loss after epoch 67 : nan\n",
      "Loss after epoch 68 : nan\n",
      "Loss after epoch 69 : nan\n",
      "Loss after epoch 70 : nan\n",
      "Loss after epoch 70 : nan\n",
      "Loss after epoch 71 : nan\n",
      "Loss after epoch 72 : nan\n",
      "Loss after epoch 73 : nan\n",
      "Loss after epoch 74 : nan\n",
      "Loss after epoch 75 : nan\n",
      "Loss after epoch 76 : nan\n",
      "Loss after epoch 77 : nan\n",
      "Loss after epoch 78 : nan\n",
      "Loss after epoch 79 : nan\n",
      "Loss after epoch 80 : nan\n",
      "Loss after epoch 80 : nan\n",
      "Loss after epoch 81 : nan\n",
      "Loss after epoch 82 : nan\n",
      "Loss after epoch 83 : nan\n",
      "Loss after epoch 84 : nan\n",
      "Loss after epoch 85 : nan\n",
      "Loss after epoch 86 : nan\n",
      "Loss after epoch 87 : nan\n",
      "Loss after epoch 88 : nan\n",
      "Loss after epoch 89 : nan\n",
      "Loss after epoch 90 : nan\n",
      "Loss after epoch 90 : nan\n",
      "Loss after epoch 91 : nan\n",
      "Loss after epoch 92 : nan\n",
      "Loss after epoch 93 : nan\n",
      "Loss after epoch 94 : nan\n",
      "Loss after epoch 95 : nan\n",
      "Loss after epoch 96 : nan\n",
      "Loss after epoch 97 : nan\n",
      "Loss after epoch 98 : nan\n",
      "Loss after epoch 99 : nan\n",
      "Loss after epoch 100 : nan\n",
      "Loss after epoch 100 : nan\n",
      "Loss after epoch 101 : nan\n",
      "Loss after epoch 102 : nan\n",
      "Loss after epoch 103 : nan\n",
      "Loss after epoch 104 : nan\n",
      "Loss after epoch 105 : nan\n",
      "Loss after epoch 106 : nan\n",
      "Loss after epoch 107 : nan\n",
      "Loss after epoch 108 : nan\n",
      "Loss after epoch 109 : nan\n",
      "Loss after epoch 110 : nan\n",
      "Loss after epoch 110 : nan\n",
      "Loss after epoch 111 : nan\n",
      "Loss after epoch 112 : nan\n",
      "Loss after epoch 113 : nan\n",
      "Loss after epoch 114 : nan\n",
      "Loss after epoch 115 : nan\n",
      "Loss after epoch 116 : nan\n",
      "Loss after epoch 117 : nan\n",
      "Loss after epoch 118 : nan\n",
      "Loss after epoch 119 : nan\n",
      "Loss after epoch 120 : nan\n",
      "Loss after epoch 120 : nan\n",
      "Loss after epoch 121 : nan\n",
      "Loss after epoch 122 : nan\n",
      "Loss after epoch 123 : nan\n",
      "Loss after epoch 124 : nan\n",
      "Loss after epoch 125 : nan\n",
      "Loss after epoch 126 : nan\n",
      "Loss after epoch 127 : nan\n",
      "Loss after epoch 128 : nan\n",
      "Loss after epoch 129 : nan\n",
      "Loss after epoch 130 : nan\n",
      "Loss after epoch 130 : nan\n",
      "Loss after epoch 131 : nan\n",
      "Loss after epoch 132 : nan\n",
      "Loss after epoch 133 : nan\n",
      "Loss after epoch 134 : nan\n",
      "Loss after epoch 135 : nan\n",
      "Loss after epoch 136 : nan\n",
      "Loss after epoch 137 : nan\n",
      "Loss after epoch 138 : nan\n",
      "Loss after epoch 139 : nan\n",
      "Loss after epoch 140 : nan\n",
      "Loss after epoch 140 : nan\n",
      "Loss after epoch 141 : nan\n",
      "Loss after epoch 142 : nan\n",
      "Loss after epoch 143 : nan\n",
      "Loss after epoch 144 : nan\n",
      "Loss after epoch 145 : nan\n",
      "Loss after epoch 146 : nan\n",
      "Loss after epoch 147 : nan\n",
      "Loss after epoch 148 : nan\n",
      "Loss after epoch 149 : nan\n",
      "Loss after epoch 150 : nan\n",
      "Loss after epoch 150 : nan\n",
      "Loss after epoch 151 : nan\n",
      "Loss after epoch 152 : nan\n",
      "Loss after epoch 153 : nan\n",
      "Loss after epoch 154 : nan\n",
      "Loss after epoch 155 : nan\n",
      "Loss after epoch 156 : nan\n",
      "Loss after epoch 157 : nan\n",
      "Loss after epoch 158 : nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/1938637759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/2744446092.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X_train, target, epochs)\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropADAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss after epoch {i} : {cost}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/2744446092.py\u001b[0m in \u001b[0;36mbackpropADAM\u001b[0;34m(self, X, target)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropADAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# Forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/2744446092.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_hidden_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Hidden1 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m      \u001b[0;31m# Hidden2 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_hidden_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Hidden2 x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m       \u001b[0;31m# Output x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mA3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m#Output x N_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/pdn9mc513_g7d3f2pzq7z8v00000gn/T/ipykernel_57131/1340189629.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c= nn.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "a985cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.779\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2beee",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "ab2ec79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0 : 4.0248719069912555\n",
      "Loss after epoch 1 : 2.276420196595539\n",
      "Loss after epoch 2 : 1.4171308091484893\n",
      "Loss after epoch 3 : 0.4909401642439696\n",
      "Loss after epoch 4 : 0.23462532199671818\n",
      "Loss after epoch 5 : 0.23362828058409388\n",
      "Loss after epoch 6 : 0.2339391955562715\n",
      "Loss after epoch 7 : 0.23403101872804918\n",
      "Loss after epoch 8 : 0.23408034835165503\n",
      "Loss after epoch 9 : 0.23408641247524933\n",
      "Loss after epoch 10 : 0.23405133302759268\n",
      "Loss after epoch 11 : 0.2339796667265713\n",
      "Loss after epoch 12 : 0.23387689997194383\n",
      "Loss after epoch 13 : 0.23374862251877318\n",
      "Loss after epoch 14 : 0.2336030481667226\n",
      "Loss after epoch 15 : 0.23349178793024195\n",
      "Loss after epoch 16 : 0.23336433962006922\n",
      "Loss after epoch 17 : 0.23322363763084164\n",
      "Loss after epoch 18 : 0.23307246338027093\n",
      "Loss after epoch 19 : 0.23291342250478173\n",
      "Loss after epoch 20 : 0.23274895271160082\n",
      "Loss after epoch 21 : 0.23258134424862817\n",
      "Loss after epoch 22 : 0.23241275768847888\n",
      "Loss after epoch 23 : 0.23224522931747094\n",
      "Loss after epoch 24 : 0.23208066076667053\n",
      "Loss after epoch 25 : 0.23192079490351467\n",
      "Loss after epoch 26 : 0.23176718345221467\n",
      "Loss after epoch 27 : 0.23162115313651943\n",
      "Loss after epoch 28 : 0.23148377670723858\n",
      "Loss after epoch 29 : 0.23135585365984151\n",
      "Loss after epoch 30 : 0.2312379034055165\n",
      "Loss after epoch 31 : 0.23113017163556143\n",
      "Loss after epoch 32 : 0.23103264893146572\n",
      "Loss after epoch 33 : 0.23094509947037054\n",
      "Loss after epoch 34 : 0.2308670969823254\n",
      "Loss after epoch 35 : 0.23079806487893575\n",
      "Loss after epoch 36 : 0.23073731759976354\n",
      "Loss after epoch 37 : 0.23068410060661618\n",
      "Loss after epoch 38 : 0.2306376269931126\n",
      "Loss after epoch 39 : 0.230597109277107\n",
      "Loss after epoch 40 : 0.23056178553445997\n",
      "Loss after epoch 41 : 0.23053093956241674\n",
      "Loss after epoch 42 : 0.2305039151974631\n",
      "Loss after epoch 43 : 0.23048012524154063\n",
      "Loss after epoch 44 : 0.23045905567155422\n",
      "Loss after epoch 45 : 0.23044026592989525\n",
      "Loss after epoch 46 : 0.230423386134111\n",
      "Loss after epoch 47 : 0.23040811202048364\n",
      "Loss after epoch 48 : 0.23039419836784145\n",
      "Loss after epoch 49 : 0.23038145155142808\n",
      "Loss after epoch 50 : 0.23036972176634304\n",
      "Loss after epoch 51 : 0.2303588953470823\n",
      "Loss after epoch 52 : 0.2303488875020399\n",
      "Loss after epoch 53 : 0.2303396356847255\n",
      "Loss after epoch 54 : 0.23033109373983177\n",
      "Loss after epoch 55 : 0.23032322689327017\n",
      "Loss after epoch 56 : 0.23031600760071688\n",
      "Loss after epoch 57 : 0.230309412228013\n",
      "Loss after epoch 58 : 0.23030341850738947\n",
      "Loss after epoch 59 : 0.23029800369411227\n",
      "Loss after epoch 60 : 0.2302931433369295\n",
      "Loss after epoch 61 : 0.23028881057089448\n",
      "Loss after epoch 62 : 0.23028497584115631\n",
      "Loss after epoch 63 : 0.23028160696983938\n",
      "Loss after epoch 64 : 0.23027866948404205\n",
      "Loss after epoch 65 : 0.23027612713044018\n",
      "Loss after epoch 66 : 0.23027394251030095\n",
      "Loss after epoch 67 : 0.230272077777435\n",
      "Loss after epoch 68 : 0.2302704953503856\n",
      "Loss after epoch 69 : 0.23026915859876268\n",
      "Loss after epoch 70 : 0.23026803247188882\n",
      "Loss after epoch 71 : 0.23026708404575677\n",
      "Loss after epoch 72 : 0.23026628297159166\n",
      "Loss after epoch 73 : 0.23026560181601546\n",
      "Loss after epoch 74 : 0.23026501628885457\n",
      "Loss after epoch 75 : 0.23026450535994983\n",
      "Loss after epoch 76 : 0.2302640512708797\n",
      "Loss after epoch 77 : 0.2302636394512376\n",
      "Loss after epoch 78 : 0.23026325835197983\n",
      "Loss after epoch 79 : 0.23026289921039328\n",
      "Loss after epoch 80 : 0.23026255576241486\n",
      "Loss after epoch 81 : 0.23026222391842485\n",
      "Loss after epoch 82 : 0.2302619014183027\n",
      "Loss after epoch 83 : 0.23026158748055867\n",
      "Loss after epoch 84 : 0.23026128245886565\n",
      "Loss after epoch 85 : 0.23026098751741728\n",
      "Loss after epoch 86 : 0.2302607043343873\n",
      "Loss after epoch 87 : 0.230260434840453\n",
      "Loss after epoch 88 : 0.2302601809970421\n",
      "Loss after epoch 89 : 0.23025994461672764\n",
      "Loss after epoch 90 : 0.2302597272261595\n",
      "Loss after epoch 91 : 0.2302595299701283\n",
      "Loss after epoch 92 : 0.23025935355387642\n",
      "Loss after epoch 93 : 0.2302591982196112\n",
      "Loss after epoch 94 : 0.23025906375237024\n",
      "Loss after epoch 95 : 0.23025894950989334\n",
      "Loss after epoch 96 : 0.2302588544709831\n",
      "Loss after epoch 97 : 0.23025877729692443\n",
      "Loss after epoch 98 : 0.23025871640084322\n",
      "Loss after epoch 99 : 0.23025867002038114\n",
      "Loss after epoch 100 : 0.23025863628967733\n",
      "Loss after epoch 100 : 0.23025863628967733\n",
      "Loss after epoch 101 : 0.23025861330735023\n",
      "Loss after epoch 102 : 0.2302585991978999\n",
      "Loss after epoch 103 : 0.23025859216468453\n",
      "Loss after epoch 104 : 0.23025859053331427\n",
      "Loss after epoch 105 : 0.2302585927849348\n",
      "Loss after epoch 106 : 0.2302585975794219\n",
      "Loss after epoch 107 : 0.23025860376896307\n",
      "Loss after epoch 108 : 0.2302586104028612\n",
      "Loss after epoch 109 : 0.23025861672465286\n",
      "Loss after epoch 110 : 0.23025862216280293\n",
      "Loss after epoch 111 : 0.23025862631631464\n",
      "Loss after epoch 112 : 0.2302586289366058\n",
      "Loss after epoch 113 : 0.23025862990694296\n",
      "Loss after epoch 114 : 0.2302586292206236\n",
      "Loss after epoch 115 : 0.23025862695895702\n",
      "Loss after epoch 116 : 0.23025862326993043\n",
      "Loss after epoch 117 : 0.23025861834827427\n",
      "Loss after epoch 118 : 0.23025861241746073\n",
      "Loss after epoch 119 : 0.2302586057140061\n",
      "Loss after epoch 120 : 0.230258598474281\n",
      "Loss after epoch 121 : 0.23025859092390252\n",
      "Loss after epoch 122 : 0.230258583269657\n",
      "Loss after epoch 123 : 0.23025857569381164\n",
      "Loss after epoch 124 : 0.230258568350594\n",
      "Loss after epoch 125 : 0.230258561364576\n",
      "Loss after epoch 126 : 0.23025855483066107\n",
      "Loss after epoch 127 : 0.23025854881536337\n",
      "Loss after epoch 128 : 0.2302585433590762\n",
      "Loss after epoch 129 : 0.23025853847903402\n",
      "Loss after epoch 130 : 0.23025853417270783\n",
      "Loss after epoch 131 : 0.23025853042139532\n",
      "Loss after epoch 132 : 0.2302585271938158\n",
      "Loss after epoch 133 : 0.23025852444954614\n",
      "Loss after epoch 134 : 0.23025852214218298\n",
      "Loss after epoch 135 : 0.2302585202221397\n",
      "Loss after epoch 136 : 0.2302585186390348\n",
      "Loss after epoch 137 : 0.23025851734364242\n",
      "Loss after epoch 138 : 0.23025851628940872\n",
      "Loss after epoch 139 : 0.2302585154335549\n",
      "Loss after epoch 140 : 0.23025851473780024\n",
      "Loss after epoch 141 : 0.23025851416875362\n",
      "Loss after epoch 142 : 0.23025851369802\n",
      "Loss after epoch 143 : 0.2302585133020802\n",
      "Loss after epoch 144 : 0.2302585129619934\n",
      "Loss after epoch 145 : 0.23025851266297526\n",
      "Loss after epoch 146 : 0.2302585123938958\n",
      "Loss after epoch 147 : 0.23025851214673695\n",
      "Loss after epoch 148 : 0.23025851191604355\n",
      "Loss after epoch 149 : 0.2302585116983937\n",
      "Loss after epoch 150 : 0.23025851149190968\n",
      "Loss after epoch 151 : 0.23025851129582176\n",
      "Loss after epoch 152 : 0.23025851111009304\n",
      "Loss after epoch 153 : 0.23025851093511102\n",
      "Loss after epoch 154 : 0.23025851077144335\n",
      "Loss after epoch 155 : 0.2302585106196547\n",
      "Loss after epoch 156 : 0.2302585104801791\n",
      "Loss after epoch 157 : 0.2302585103532408\n",
      "Loss after epoch 158 : 0.23025851023881672\n",
      "Loss after epoch 159 : 0.23025851013662813\n",
      "Loss after epoch 160 : 0.2302585100461597\n",
      "Loss after epoch 161 : 0.23025850996669261\n",
      "Loss after epoch 162 : 0.2302585098973502\n",
      "Loss after epoch 163 : 0.23025850983714582\n",
      "Loss after epoch 164 : 0.23025850978503315\n",
      "Loss after epoch 165 : 0.2302585097399514\n",
      "Loss after epoch 166 : 0.23025850970086617\n",
      "Loss after epoch 167 : 0.23025850966680234\n",
      "Loss after epoch 168 : 0.23025850963686903\n",
      "Loss after epoch 169 : 0.23025850961027736\n",
      "Loss after epoch 170 : 0.23025850958634983\n",
      "Loss after epoch 171 : 0.23025850956452346\n",
      "Loss after epoch 172 : 0.23025850954434646\n",
      "Loss after epoch 173 : 0.230258509525472\n",
      "Loss after epoch 174 : 0.23025850950764648\n",
      "Loss after epoch 175 : 0.23025850949069787\n",
      "Loss after epoch 176 : 0.23025850947452098\n",
      "Loss after epoch 177 : 0.23025850945906354\n",
      "Loss after epoch 178 : 0.2302585094443132\n",
      "Loss after epoch 179 : 0.23025850943028398\n",
      "Loss after epoch 180 : 0.23025850941700662\n",
      "Loss after epoch 181 : 0.23025850940451803\n",
      "Loss after epoch 182 : 0.2302585093928546\n",
      "Loss after epoch 183 : 0.23025850938204612\n",
      "Loss after epoch 184 : 0.23025850937211334\n",
      "Loss after epoch 185 : 0.2302585093630632\n",
      "Loss after epoch 186 : 0.23025850935489045\n",
      "Loss after epoch 187 : 0.23025850934757608\n",
      "Loss after epoch 188 : 0.23025850934108893\n",
      "Loss after epoch 189 : 0.2302585093353873\n",
      "Loss after epoch 190 : 0.23025850933042089\n",
      "Loss after epoch 191 : 0.23025850932613276\n",
      "Loss after epoch 192 : 0.23025850932246206\n",
      "Loss after epoch 193 : 0.23025850931934533\n",
      "Loss after epoch 194 : 0.23025850931671937\n",
      "Loss after epoch 195 : 0.230258509314522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 196 : 0.23025850931269418\n",
      "Loss after epoch 197 : 0.23025850931118047\n",
      "Loss after epoch 198 : 0.23025850930993022\n",
      "Loss after epoch 199 : 0.2302585093088975\n",
      "Loss after 1epoch 201 : 0.2302585093088975\n"
     ]
    }
   ],
   "source": [
    "nn = HiddenTwo(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes_1 = M,\n",
    "               hidden_nodes_2 = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden_1 = relu,\n",
    "               activation_hidden_2 = relu,\n",
    "              optimizer = 'adam')\n",
    "c= nn.run(X_train, y_train_cat, epochs=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6b772b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.1\n"
     ]
    }
   ],
   "source": [
    "acc = nn.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
