{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef32d7be",
   "metadata": {},
   "source": [
    "## Content:\n",
    "- [Part 1](#part1)- Importing the libraries, packages\n",
    "- [Part 2](#part2)- Useful Functions\n",
    "- [Part 3](#part3) -  One Hidden Layer Class\n",
    "- [Part 4](#part4) -  Two Hidden Layers Class \n",
    "- [Part 5](#part5) -  Loading Fashion MNIST \n",
    "- [Part 6](#part6)-  Fashion MNIST One Hidden Layer\n",
    "- [Part 7](#part7)-  Fashion MNIST Two Hidden Layers\n",
    "- [Part 8](#part8)) - Results\n",
    "- [Part 9](#part9) -  Results \n",
    "- [Part 10](#part10) -  --\n",
    "- [Part 11](#part11) -  --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c13058",
   "metadata": {},
   "source": [
    "Weight initialisation :\n",
    "\n",
    "- https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "- https://www.deeplearning.ai/ai-notes/initialization/\n",
    "- https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd74d74",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "### Part 1 -   Importing the libraries, packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "64b9ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import random \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910cdf6",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "### Part 2 -   Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "ae6751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "0936fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def softmax(X):\n",
    "    e = np.exp(X - np.max(X))\n",
    "    return e / e.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(target, output):\n",
    "    return -np.mean(target*np.log(output))\n",
    "\n",
    "def cross_entropy_matrix(output, target):\n",
    "    target = np.array(target)\n",
    "    output = np.array(output)\n",
    "    product = target*np.log(output)\n",
    "    errors = -np.sum(product, axis=1)\n",
    "    m = len(errors)\n",
    "    errors = np.sum(errors) / m\n",
    "    return errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ds(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "  \n",
    "\n",
    "def dr(x):\n",
    "    dr = (np.sign(x) + 1) / 2\n",
    "    return dr\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-x)\n",
    "    return (a-b)/(a+b)\n",
    "\n",
    "def dt(x):\n",
    "    return 1-tanh(x)**2\n",
    "    \n",
    "def leaky(x,a):\n",
    "    leaky = np.maximum(x,0)*x + a*np.minimum(x,0)\n",
    "    return leaky\n",
    "\n",
    "def dl(x,a):\n",
    "    dl = (np.sign(x)+1)/2 - a*(np.sign(x)-1)/2\n",
    "    return dl\n",
    "\n",
    "def derivative(f):\n",
    "    if f == sigmoid :\n",
    "        return ds\n",
    "    if f == tanh :\n",
    "        return dt\n",
    "    if f == relu :\n",
    "        return dr\n",
    "    if f == leaky :\n",
    "        return dl\n",
    "    return None\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i][y[i]]=1\n",
    "    return ind\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fdadb",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "### Part 3 -   One Hidden Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7220f",
   "metadata": {},
   "source": [
    "# One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5c79",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden x N_features\n",
    "- **b1**    : Hidden\n",
    "- **W2**    : Output x Hidden\n",
    "- **b2**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "26933734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenOne:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes,\n",
    "                 activation_hidden,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes = hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.hidden_derivative = derivative(self.activation_hidden)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):       \n",
    "        if self.activation_hidden == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes/2 )  # output x hidden\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden x features\n",
    "            self.W2 = np.random.randn(self.output_nodes, self.hidden_nodes )/np.sqrt(self.hidden_nodes )  # output x hidden\n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    def create_biases(self):    \n",
    "        #tn = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        #self.b1 = tn.rvs(self.hidden_nodes).reshape(-1,1) \n",
    "        #self.b2 = tn.rvs(self.output_nodes).reshape(-1,1) \n",
    "        self.b1 =  np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.b2 = np.zeros((self.output_nodes, 1 ))\n",
    "          \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.output_nodes, self.hidden_nodes ))\n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1 # Hidden x N_samples\n",
    "        A1 = self.activation_hidden(Z1)      # Hidden x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2  # Output x N_samples\n",
    "        A2 = softmax(Z2)      #Output x N_samples\n",
    "        return A2, Z2, A1, Z1\n",
    "    \n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backpropSGD(self, X, target):\n",
    "        m = X.shape[0]                  #N_samples\n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(m))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        for i in range(m) :\n",
    "            # Forward prop\n",
    "            x = X_SGD[i,:].reshape(1,-1)                   # 1 x N_features\n",
    "            a2, z2, a1, z1 = self.forward(x)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,i].reshape(-1,1), a2)/m\n",
    "            # deltas\n",
    "            dz2 = a2 - target[:,i].reshape(-1,1)                    #Output x 1\n",
    "            dW2 = dz2.dot(a1.T)                                     #Output x hidden\n",
    "            db2 = dz2                                               #Output x 1\n",
    "            dz1 = self.W2.T.dot(dz2)*self.hidden_derivative(z1)     # Hidden x 1\n",
    "            dW1 = dz1.dot(x)                                        # Hidden x N_Features\n",
    "            db1 = dz1                                               # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "        \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A2)/pass_length\n",
    "            # deltas\n",
    "            dZ2 = A2 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   #Output x hidden\n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)              # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              #Hidden x1                                            # Hidden x 1\n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backprop_adam_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)  \n",
    "            t = target_SGD[:,k:k+batch_size].reshape(-1,batch_size)\n",
    "            cost = cost + self.backpropADAM(X, t)/pass_length\n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A2)\n",
    "        # N samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ2 = A2 - target                                       #Output x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                   #Output x hidden\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m              #Output x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2    \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "    def predict(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A2, axis=0)\n",
    "        return y_pred\n",
    "                   \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 1epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "             \n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 14epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss 16after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 21epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 19epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 44epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "            elif self.optimizer == 'mini_adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_adam_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 23epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs     \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after 4epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after 2epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "        \n",
    "       \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e01484",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "### Part 4 -   Two Hidden Layers Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1ee59",
   "metadata": {},
   "source": [
    "# Two Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1383e",
   "metadata": {},
   "source": [
    "# Variables :\n",
    "\n",
    "- **X**     : N_Samples x N_features\n",
    "- **W1**    : Hidden1 x N_features\n",
    "- **b1**    : Hidden1\n",
    "- **W2**    : Hidden2 x Hidden1\n",
    "- **b2**    : Hidden2\n",
    "- **W3**    : Output x Hidden\n",
    "- **b3**    : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "de86d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenTwo:\n",
    "     \n",
    "    def __init__(self, \n",
    "                 input_nodes, \n",
    "                 output_nodes, \n",
    "                 hidden_nodes_1,\n",
    "                 hidden_nodes_2,\n",
    "                 activation_hidden_1,\n",
    "                 activation_hidden_2,\n",
    "                 learning_rate=0.01,\n",
    "                 optimizer = None,\n",
    "                 beta1 = 0.9,   #ADAM optimization parameter, default value taken from practical experience\n",
    "                 beta2 = 0.999, #ADAM optimization parameter, default value taken from practical experience\n",
    "                 batch_size = None,\n",
    "                 delta_stop = None,\n",
    "                 patience = 1,\n",
    "                 leaky_intercept=0.01\n",
    "                 \n",
    "                ):         \n",
    "        # Initializations\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes       \n",
    "        self.hidden_nodes_1 = hidden_nodes_1    \n",
    "        self.hidden_nodes_2 = hidden_nodes_2    \n",
    "        self.learning_rate = learning_rate \n",
    "        self.activation_hidden_1 = activation_hidden_1\n",
    "        self.activation_hidden_2 = activation_hidden_2\n",
    "        self.hidden_derivative_1 = derivative(self.activation_hidden_1)\n",
    "        self.hidden_derivative_2 = derivative(self.activation_hidden_2)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.delta_stop = delta_stop\n",
    "        self.patience = patience\n",
    "        self.leaky_intercept = leaky_intercept\n",
    "        self.create_weight_matrices()\n",
    "        self.create_biases()\n",
    "        self.reset_adam()\n",
    "             \n",
    "    def create_weight_matrices(self):\n",
    "        if self.activation_hidden_1 == relu : # He initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes/2 ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1/2 )  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2/2 )  # output x hidden2\n",
    "        else : # Xavier initialization\n",
    "            self.W1 = np.random.randn(self.hidden_nodes_1, self.input_nodes )/np.sqrt(self.input_nodes ) # hidden1 x features\n",
    "            self.W2 = np.random.randn(self.hidden_nodes_2, self.hidden_nodes_1 )/np.sqrt(self.hidden_nodes_1)  # hidden2 x hidden1\n",
    "            self.W3 = np.random.randn(self.output_nodes, self.hidden_nodes_2 )/np.sqrt(self.hidden_nodes_2)  # output x hidden2\n",
    "        \n",
    "    def create_biases(self):  \n",
    "        self.b1 =  np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.b2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.b3 = np.zeros((self.output_nodes, 1 ))\n",
    "     \n",
    "    def reset_adam(self):\n",
    "        '''\n",
    "        Creates Adam optimizations variables\n",
    "        '''\n",
    "        self.Vdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Vdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Vdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Vdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Vdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Vdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "        \n",
    "        self.Sdw1 = np.zeros((self.hidden_nodes_1, self.input_nodes ))\n",
    "        self.Sdw2 = np.zeros((self.hidden_nodes_2, self.hidden_nodes_1 ))\n",
    "        self.Sdw3 = np.zeros((self.output_nodes, self.hidden_nodes_2))\n",
    "       \n",
    "        self.Sdb1 = np.zeros((self.hidden_nodes_1, 1 ))\n",
    "        self.Sdb2 = np.zeros((self.hidden_nodes_2, 1 ))\n",
    "        self.Sdb3 = np.zeros((self.output_nodes, 1 ))\n",
    "                \n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1.dot(X.T) + self.b1      # Hidden1 x N_samples\n",
    "        A1 = self.activation_hidden_1(Z1)      # Hidden1 x N_samples\n",
    "        Z2 = self.W2.dot(A1) + self.b2      # Hidden2 x N_samples\n",
    "        A2 = self.activation_hidden_2(Z2)      # Hidden2 x N_samples\n",
    "        Z3 = self.W3.dot(A2) + self.b3       # Output x N_samples\n",
    "        A3 = softmax(Z3)                     #Output x N_samples\n",
    "        return A3, Z3, A2, Z2, A1, Z1\n",
    "    \n",
    "    def backprop(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N_samples\n",
    "        m = X.shape[0]\n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "     \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr*dW3\n",
    "        self.b3 -= lr*db3\n",
    "        self.W2 -= lr*dW2\n",
    "        self.b2 -= lr*db2\n",
    "        self.W1 -= lr*dW1\n",
    "        self.b1 -= lr*db1\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def backprop_minibatch(self, X, target):\n",
    "        n = X.shape[1]               # N_features\n",
    "        batch_size = X.shape[0]      # N_samples\n",
    "        if self.batch_size == None :\n",
    "            batch_size = self.minibatch_size(batch_size)\n",
    "        else :\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        X_SGD = X.copy()\n",
    "        u = rng.shuffle(np.arange(X.shape[0] ))\n",
    "        X_SGD = X_SGD[u,:].squeeze()    # N_samples x N_Features\n",
    "        target_SGD = target[:,u].squeeze() # Output x N_samples\n",
    "        cost = 0\n",
    "        \n",
    "        pass_length = int(X.shape[0]/batch_size)\n",
    "        for i in range(pass_length) :\n",
    "            k = i*batch_size\n",
    "            # Forward prop\n",
    "            X = X_SGD[k:k+batch_size,:].reshape(batch_size,-1)              #batch_size x N_features\n",
    "            A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "            # cost update\n",
    "            cost = cost + cross_entropy(target_SGD[:,k:k+batch_size].reshape(-1,batch_size), A3)/pass_length\n",
    "            # deltas\n",
    "            dZ3 = A3 - target_SGD[:,k:k+batch_size].reshape(-1,batch_size)   #Output x batch_size\n",
    "            dW3 = dZ3.dot(A2.T)/batch_size                                   #Output x hidden_2\n",
    "            db3 = np.sum(dZ3, axis=1, keepdims=True)/batch_size              #Output x 1\n",
    "            dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)            # Hidden2 x batch_size\n",
    "            dW2 = dZ2.dot(A1.T)/batch_size                                   # Hidden2 x Hidden1 \n",
    "            db2 = np.sum(dZ2, axis=1, keepdims=True)/batch_size              # Hidden2 x 1\n",
    "            dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)            # Hidden x batch_size\n",
    "            dW1 = dZ1.dot(X)/batch_size                                      # Hidden x N_Features\n",
    "            db1 = np.sum(dZ1, axis=1, keepdims=True)/batch_size              # Hidden x 1                        \n",
    "            # Update\n",
    "            lr = self.learning_rate\n",
    "            self.W3 -= lr*dW3\n",
    "            self.b3 -= lr*db3\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "        return cost\n",
    "    \n",
    "    def backpropADAM(self, X, target):\n",
    "        # Forward prop\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X)\n",
    "        # Compute cost\n",
    "        cost = cross_entropy(target, A3)\n",
    "        # N samples\n",
    "        m = X.shape[0]   \n",
    "        # deltas\n",
    "        dZ3 = A3 - target                                      #Output x N_samples\n",
    "        dW3 = dZ3.dot(A2.T)/m                                  #Output x Hidden_2\n",
    "        db3 = np.sum(dZ3, axis=1, keepdims=True)/m             #Output x 1\n",
    "        dZ2 = self.W3.T.dot(dZ3)*self.hidden_derivative_2(Z2)    # Hidden2 x N_samples\n",
    "        dW2 = dZ2.dot(A1.T)/m                                     # Hidden2 x Hidden1 \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)/m             # Hidden2 x 1\n",
    "        dZ1 = self.W2.T.dot(dZ2)*self.hidden_derivative_1(Z1)     # Hidden x N_samples\n",
    "        dW1 = dZ1.dot(X)/m                                      # Hidden x N_Features\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)/m              # Hidden x 1\n",
    "        # Adam updates\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        # V\n",
    "        self.Vdw1 = beta1*self.Vdw1 + (1-beta1)*dW1\n",
    "        self.Vdw2 = beta1*self.Vdw2 + (1-beta1)*dW2\n",
    "        self.Vdw3 = beta1*self.Vdw3 + (1-beta1)*dW3\n",
    "        self.Vdb1 = beta1*self.Vdb1 + (1-beta1)*db1\n",
    "        self.Vdb2 = beta1*self.Vdb2 + (1-beta1)*db2\n",
    "        self.Vdb3 = beta1*self.Vdb3 + (1-beta1)*db3\n",
    "        # S\n",
    "        self.Sdw1 = beta2*self.Sdw1 + (1-beta2)*dW1**2\n",
    "        self.Sdw2 = beta2*self.Sdw2 + (1-beta2)*dW2**2\n",
    "        self.Sdw3 = beta2*self.Sdw3 + (1-beta2)*dW3**2\n",
    "        self.Sdb1 = beta2*self.Sdb1 + (1-beta2)*db1**2\n",
    "        self.Sdb2 = beta2*self.Sdb2 + (1-beta2)*db2**2\n",
    "        self.Sdb3 = beta2*self.Sdb3 + (1-beta2)*db3**2  \n",
    "        # Update\n",
    "        lr = self.learning_rate\n",
    "        self.W3 -= lr * self.Vdw3 / (np.sqrt(self.Sdw3)+1e-8)\n",
    "        self.b3 -= lr * self.Vdb3 / (np.sqrt(self.Sdb3)+1e-8)\n",
    "        self.W2 -= lr * self.Vdw2 / (np.sqrt(self.Sdw2)+1e-8)\n",
    "        self.b2 -= lr * self.Vdb2 / (np.sqrt(self.Sdb2)+1e-8)\n",
    "        self.W1 -= lr * self.Vdw1 / (np.sqrt(self.Sdw1)+1e-8)\n",
    "        self.b1 -= lr * self.Vdb1 / (np.sqrt(self.Sdb1)+1e-8)\n",
    "        return cost  \n",
    "    \n",
    "      \n",
    "    def predict(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        return A3\n",
    "    \n",
    "    def predict_class(self, X_predict):\n",
    "        A3, Z3, A2, Z2, A1, Z1 = self.forward(X_predict)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    # To be deleted               \n",
    "    def xrun(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        for i in range(epochs):\n",
    "            cost = self.backprop(X_train, target)\n",
    "            costs.append(cost)\n",
    "            if i%10 == 0:\n",
    "                print(f'Loss after epoch {i} : {cost}')\n",
    "        costs.pop(0)\n",
    "        return costs  \n",
    "         \n",
    "    def run(self, X_train, target, epochs=10):\n",
    "        costs = [1e-10]\n",
    "        if self.delta_stop == None : \n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    \n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :\n",
    "                for i in range(epochs):  \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            \n",
    "        else :\n",
    "            counter = 0\n",
    "            if self.optimizer == 'adam':\n",
    "                self.reset_adam()\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropADAM(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                    \n",
    "            elif self.optimizer == 'SGD' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backpropSGD(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            elif self.optimizer == 'minibatch' :\n",
    "                for i in range(epochs):\n",
    "                    cost = self.backprop_minibatch(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                    else :\n",
    "                        counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')\n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "                \n",
    "            else :  \n",
    "                for i in range(epochs): \n",
    "                    cost = self.backprop(X_train, target)\n",
    "                    costs.append(cost)\n",
    "                    n = len(costs)\n",
    "                    delta = np.abs(costs[n-1]/costs[n-2]-1)\n",
    "                    if(delta < self.delta_stop) :\n",
    "                        counter+=1\n",
    "                        if(counter>=self.patience):\n",
    "                            print(f'Early stop at epoch {i}, the cost is : {cost}')\n",
    "                            costs.pop(0)\n",
    "                            return costs\n",
    "                        else :\n",
    "                            counter =0\n",
    "                    if i%10 == 0 and i>0 :\n",
    "                        print(f'Loss after epoch {i} : {cost}')        \n",
    "                print(f'Loss after epoch {len(costs)} : {costs[-1]}')        \n",
    "                costs.pop(0)\n",
    "                return costs\n",
    "          \n",
    "            \n",
    "       \n",
    "    def evaluate(self, X_evaluate, target):\n",
    "        '''\n",
    "        return accuracy score, target must be the classes and not the hot encoded target\n",
    "        '''\n",
    "        \n",
    "        y_pred = self.predict_class(X_evaluate)\n",
    "        accuracy = classification_rate(y_pred, target)\n",
    "        print('Accuracy :', accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def minibatch_size(self, n_samples):\n",
    "        '''\n",
    "        Compute minibatch size in case its not provided\n",
    "        '''\n",
    "        if n_samples < 2000:\n",
    "            return n_samples\n",
    "        if n_samples < 12800:\n",
    "            return 64\n",
    "        if n_samples < 25600:\n",
    "            return 128\n",
    "        if n_samples < 51200:\n",
    "            return 256\n",
    "        if n_samples < 102400:\n",
    "            return 512\n",
    "        return 1024\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca59cb4",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "### Part 5 -  Loading Fashion MNIST\n",
    "Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "2b19461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "cfcacf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "8bf5c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "58f6e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "c94784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = X_train.shape[1]\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "d4d38a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(N_train, l*l, 1).squeeze()\n",
    "X_test = X_test.reshape(N_test, l*l, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "9846a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train).T\n",
    "y_test_cat = to_categorical(y_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a729a1",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "b9b33674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX = 255\n",
    "#X_train = X_train/ MAX\n",
    "#X_test =X_test/ MAX\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "44282a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1]\n",
    "K = y_train_cat.shape[0]\n",
    "M=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "77f8226f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3d61e",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "### Part 6 -  Fashion MNIST with 1 hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8dbbd",
   "metadata": {},
   "source": [
    "# Activation : Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "1e4d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sigmoid = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "e552d3e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.22878686917662514\n",
      "Loss after epoch 20 : 0.22324330296231312\n",
      "Loss after epoch 30 : 0.21951122002251572\n",
      "Loss after epoch 40 : 0.21682870435876497\n",
      "Loss after epoch 50 : 0.21474906125226936\n",
      "Loss after epoch 60 : 0.21304089540720955\n",
      "Loss after epoch 70 : 0.2115789143401526\n",
      "Loss after epoch 80 : 0.21028976361325366\n",
      "Loss after epoch 90 : 0.20912740855936438\n",
      "Loss after epoch 100 : 0.20806138458023749\n",
      "Loss after epoch 110 : 0.20707074711843926\n",
      "Loss after epoch 120 : 0.2061407151166888\n",
      "Loss after epoch 130 : 0.2052606698925594\n",
      "Loss after epoch 140 : 0.2044228751755826\n",
      "Loss after epoch 150 : 0.2036216050299323\n",
      "Loss after epoch 160 : 0.2028525234367993\n",
      "Loss after epoch 170 : 0.2021122377120528\n",
      "Loss after epoch 180 : 0.20139798405825327\n",
      "Loss after epoch 190 : 0.20070741692970584\n",
      "Loss after epoch 200 : 0.20003847739535777\n",
      "Loss after epoch 210 : 0.19938931727843567\n",
      "Loss after epoch 220 : 0.1987582589087303\n",
      "Loss after epoch 230 : 0.19814377497191293\n",
      "Loss after epoch 240 : 0.19754447797836727\n",
      "Loss after epoch 250 : 0.19695911318580325\n",
      "Loss after epoch 260 : 0.1963865518947187\n",
      "Loss after epoch 270 : 0.19582578392751285\n",
      "Loss after epoch 280 : 0.19527590909458323\n",
      "Loss after epoch 290 : 0.1947361278762872\n",
      "Loss after epoch 300 : 0.19420573167081157\n",
      "Loss after epoch 310 : 0.1936840929442396\n",
      "Loss after epoch 320 : 0.19317065556277058\n",
      "Loss after epoch 330 : 0.1926649255293884\n",
      "Loss after epoch 340 : 0.19216646230059609\n",
      "Loss after epoch 350 : 0.19167487082183382\n",
      "Loss after epoch 360 : 0.19118979438717382\n",
      "Loss after epoch 370 : 0.19071090839466995\n",
      "Loss after epoch 380 : 0.19023791503092616\n",
      "Loss after epoch 390 : 0.18977053887783957\n",
      "Loss after epoch 400 : 0.18930852339431395\n",
      "Loss after epoch 410 : 0.1888516281904275\n",
      "Loss after epoch 420 : 0.18839962698511284\n",
      "Loss after epoch 430 : 0.18795230612344965\n",
      "Loss after epoch 440 : 0.1875094635267082\n",
      "Loss after epoch 450 : 0.18707090795591302\n",
      "Loss after epoch 460 : 0.1866364584851357\n",
      "Loss after epoch 470 : 0.18620594410061728\n",
      "Loss after epoch 480 : 0.18577920336294113\n",
      "Loss after epoch 490 : 0.18535608408928922\n",
      "Loss after epoch 501 : 0.18497825451950434\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "fbbd76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.4723\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811f559",
   "metadata": {},
   "source": [
    "### More epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "d3109aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1845201455241048\n",
      "Loss after epoch 20 : 0.1841070651372288\n",
      "Loss after epoch 30 : 0.18369708327252177\n",
      "Loss after epoch 40 : 0.1832900887719804\n",
      "Loss after epoch 50 : 0.1828859775081273\n",
      "Loss after epoch 60 : 0.1824846519753738\n",
      "Loss after epoch 70 : 0.18208602088695125\n",
      "Loss after epoch 80 : 0.18168999878248254\n",
      "Loss after epoch 90 : 0.1812965056500274\n",
      "Loss after epoch 100 : 0.1809054665652534\n",
      "Loss after epoch 110 : 0.18051681134937325\n",
      "Loss after epoch 120 : 0.18013047424665227\n",
      "Loss after epoch 130 : 0.17974639362165884\n",
      "Loss after epoch 140 : 0.17936451167595469\n",
      "Loss after epoch 150 : 0.17898477418360056\n",
      "Loss after epoch 160 : 0.17860713024464278\n",
      "Loss after epoch 170 : 0.17823153205562078\n",
      "Loss after epoch 180 : 0.177857934696077\n",
      "Loss after epoch 190 : 0.17748629593004103\n",
      "Loss after epoch 200 : 0.1771165760214742\n",
      "Loss after epoch 210 : 0.17674873756269946\n",
      "Loss after epoch 220 : 0.17638274531489245\n",
      "Loss after epoch 230 : 0.17601856605976515\n",
      "Loss after epoch 240 : 0.17565616846162854\n",
      "Loss after epoch 250 : 0.1752955229390832\n",
      "Loss after epoch 260 : 0.17493660154563698\n",
      "Loss after epoch 270 : 0.17457937785860544\n",
      "Loss after epoch 280 : 0.1742238268756978\n",
      "Loss after epoch 290 : 0.1738699249187364\n",
      "Loss after epoch 300 : 0.17351764954400156\n",
      "Loss after epoch 310 : 0.1731669794587302\n",
      "Loss after epoch 320 : 0.1728178944433324\n",
      "Loss after epoch 330 : 0.17247037527892525\n",
      "Loss after epoch 340 : 0.17212440367980875\n",
      "Loss after epoch 350 : 0.17177996223053899\n",
      "Loss after epoch 360 : 0.1714370343272809\n",
      "Loss after epoch 370 : 0.17109560412314\n",
      "Loss after epoch 380 : 0.17075565647720134\n",
      "Loss after epoch 390 : 0.17041717690701896\n",
      "Loss after epoch 400 : 0.17008015154431919\n",
      "Loss after epoch 410 : 0.16974456709370028\n",
      "Loss after epoch 420 : 0.16941041079412034\n",
      "Loss after epoch 430 : 0.16907767038298946\n",
      "Loss after epoch 440 : 0.16874633406268594\n",
      "Loss after epoch 450 : 0.16841639046933876\n",
      "Loss after epoch 460 : 0.16808782864371816\n",
      "Loss after epoch 470 : 0.1677606380041009\n",
      "Loss after epoch 480 : 0.1674348083209737\n",
      "Loss after epoch 490 : 0.16711032969345718\n",
      "Loss after epoch 500 : 0.16678719252733423\n",
      "Loss after epoch 510 : 0.16646538751458004\n",
      "Loss after epoch 520 : 0.16614490561429412\n",
      "Loss after epoch 530 : 0.16582573803494255\n",
      "Loss after epoch 540 : 0.16550787621782714\n",
      "Loss after epoch 550 : 0.16519131182169822\n",
      "Loss after epoch 560 : 0.1648760367084417\n",
      "Loss after epoch 570 : 0.1645620429297646\n",
      "Loss after epoch 580 : 0.16424932271481943\n",
      "Loss after epoch 590 : 0.16393786845870337\n",
      "Loss after epoch 600 : 0.16362767271177506\n",
      "Loss after epoch 610 : 0.16331872816973722\n",
      "Loss after epoch 620 : 0.16301102766443315\n",
      "Loss after epoch 630 : 0.16270456415531068\n",
      "Loss after epoch 640 : 0.16239933072150955\n",
      "Loss after epoch 650 : 0.16209532055453002\n",
      "Loss after epoch 660 : 0.16179252695144455\n",
      "Loss after epoch 670 : 0.16149094330861555\n",
      "Loss after epoch 680 : 0.16119056311588437\n",
      "Loss after epoch 690 : 0.16089137995120106\n",
      "Loss after epoch 700 : 0.16059338747566201\n",
      "Loss after epoch 710 : 0.16029657942892966\n",
      "Loss after epoch 720 : 0.16000094962500563\n",
      "Loss after epoch 730 : 0.15970649194833414\n",
      "Loss after epoch 740 : 0.15941320035021028\n",
      "Loss after epoch 750 : 0.1591210688454731\n",
      "Loss after epoch 760 : 0.15883009150946104\n",
      "Loss after epoch 770 : 0.1585402624752126\n",
      "Loss after epoch 780 : 0.15825157593089184\n",
      "Loss after epoch 790 : 0.15796402611742344\n",
      "Loss after epoch 800 : 0.1576776073263214\n",
      "Loss after epoch 810 : 0.15739231389769542\n",
      "Loss after epoch 820 : 0.15710814021842187\n",
      "Loss after epoch 830 : 0.15682508072046716\n",
      "Loss after epoch 840 : 0.15654312987934943\n",
      "Loss after epoch 850 : 0.15626228221272961\n",
      "Loss after epoch 860 : 0.15598253227911915\n",
      "Loss after epoch 870 : 0.15570387467669636\n",
      "Loss after epoch 880 : 0.15542630404222058\n",
      "Loss after epoch 890 : 0.15514981505003708\n",
      "Loss after epoch 900 : 0.15487440241116374\n",
      "Loss after epoch 910 : 0.15460006087245198\n",
      "Loss after epoch 920 : 0.15432678521581664\n",
      "Loss after epoch 930 : 0.1540545702575255\n",
      "Loss after epoch 940 : 0.15378341084754563\n",
      "Loss after epoch 950 : 0.15351330186893872\n",
      "Loss after epoch 960 : 0.15324423823730202\n",
      "Loss after epoch 970 : 0.15297621490024776\n",
      "Loss after epoch 980 : 0.15270922683692012\n",
      "Loss after epoch 990 : 0.15244326905754274\n",
      "Loss after epoch 1001 : 0.1522047838495397\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "1b202527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6505\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec3d52",
   "metadata": {},
   "source": [
    "# More epochs again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "34650b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.15191442454440818\n",
      "Loss after epoch 20 : 0.15165152798279682\n",
      "Loss after epoch 30 : 0.15138964204869057\n",
      "Loss after epoch 40 : 0.1511287619017964\n",
      "Loss after epoch 50 : 0.1508688827306692\n",
      "Loss after epoch 60 : 0.15060999975239645\n",
      "Loss after epoch 70 : 0.15035210821229322\n",
      "Loss after epoch 80 : 0.1500952033836065\n",
      "Loss after epoch 90 : 0.14983928056722853\n",
      "Loss after epoch 100 : 0.14958433509141605\n",
      "Loss after epoch 110 : 0.14933036231151722\n",
      "Loss after epoch 120 : 0.14907735760970434\n",
      "Loss after epoch 130 : 0.14882531639471183\n",
      "Loss after epoch 140 : 0.1485742341015798\n",
      "Loss after epoch 150 : 0.14832410619140332\n",
      "Loss after epoch 160 : 0.14807492815108647\n",
      "Loss after epoch 170 : 0.14782669549310223\n",
      "Loss after epoch 180 : 0.14757940375525846\n",
      "Loss after epoch 190 : 0.14733304850046886\n",
      "Loss after epoch 200 : 0.1470876253165313\n",
      "Loss after epoch 210 : 0.14684312981591177\n",
      "Loss after epoch 220 : 0.14659955763553573\n",
      "Loss after epoch 230 : 0.14635690443658492\n",
      "Loss after epoch 240 : 0.146115165904303\n",
      "Loss after epoch 250 : 0.14587433774780564\n",
      "Loss after epoch 260 : 0.1456344156998984\n",
      "Loss after epoch 270 : 0.1453953955168999\n",
      "Loss after epoch 280 : 0.14515727297847017\n",
      "Loss after epoch 290 : 0.14492004388744295\n",
      "Loss after epoch 300 : 0.1446837040696614\n",
      "Loss after epoch 310 : 0.144448249373815\n",
      "Loss after epoch 320 : 0.1442136756712761\n",
      "Loss after epoch 330 : 0.14397997885593486\n",
      "Loss after epoch 340 : 0.14374715484402967\n",
      "Loss after epoch 350 : 0.14351519957397132\n",
      "Loss after epoch 360 : 0.1432841090061584\n",
      "Loss after epoch 370 : 0.1430538791227812\n",
      "Loss after epoch 380 : 0.14282450592761275\n",
      "Loss after epoch 390 : 0.1425959854457836\n",
      "Loss after epoch 400 : 0.14236831372354025\n",
      "Loss after epoch 410 : 0.1421414868279829\n",
      "Loss after epoch 420 : 0.1419155008467852\n",
      "Loss after epoch 430 : 0.14169035188789156\n",
      "Loss after epoch 440 : 0.14146603607919578\n",
      "Loss after epoch 450 : 0.14124254956819876\n",
      "Loss after epoch 460 : 0.14101988852165015\n",
      "Loss after epoch 470 : 0.14079804912517294\n",
      "Loss after epoch 480 : 0.14057702758287732\n",
      "Loss after epoch 490 : 0.14035682011696535\n",
      "Loss after epoch 500 : 0.14013742296733192\n",
      "Loss after epoch 510 : 0.13991883239116717\n",
      "Loss after epoch 520 : 0.1397010446625651\n",
      "Loss after epoch 530 : 0.13948405607214248\n",
      "Loss after epoch 540 : 0.1392678629266762\n",
      "Loss after epoch 550 : 0.13905246154876125\n",
      "Loss after epoch 560 : 0.13883784827649506\n",
      "Loss after epoch 570 : 0.1386240194631925\n",
      "Loss after epoch 580 : 0.13841097147713394\n",
      "Loss after epoch 590 : 0.1381987007013488\n",
      "Loss after epoch 600 : 0.13798720353343788\n",
      "Loss after epoch 610 : 0.13777647638543242\n",
      "Loss after epoch 620 : 0.13756651568369205\n",
      "Loss after epoch 630 : 0.13735731786883976\n",
      "Loss after epoch 640 : 0.13714887939573192\n",
      "Loss after epoch 650 : 0.1369411967334619\n",
      "Loss after epoch 660 : 0.13673426636539351\n",
      "Loss after epoch 670 : 0.13652808478922226\n",
      "Loss after epoch 680 : 0.13632264851706072\n",
      "Loss after epoch 690 : 0.13611795407554217\n",
      "Loss after epoch 700 : 0.13591399800594406\n",
      "Loss after epoch 710 : 0.13571077686432223\n",
      "Loss after epoch 720 : 0.13550828722165606\n",
      "Loss after epoch 730 : 0.13530652566400087\n",
      "Loss after epoch 740 : 0.13510548879264314\n",
      "Loss after epoch 750 : 0.13490517322425805\n",
      "Loss after epoch 760 : 0.13470557559106613\n",
      "Loss after epoch 770 : 0.13450669254098635\n",
      "Loss after epoch 780 : 0.13430852073778593\n",
      "Loss after epoch 790 : 0.13411105686122288\n",
      "Loss after epoch 800 : 0.13391429760718315\n",
      "Loss after epoch 810 : 0.13371823968780905\n",
      "Loss after epoch 820 : 0.13352287983161934\n",
      "Loss after epoch 830 : 0.13332821478362156\n",
      "Loss after epoch 840 : 0.13313424130541393\n",
      "Loss after epoch 850 : 0.13294095617527887\n",
      "Loss after epoch 860 : 0.1327483561882678\n",
      "Loss after epoch 870 : 0.13255643815627574\n",
      "Loss after epoch 880 : 0.13236519890810794\n",
      "Loss after epoch 890 : 0.1321746352895378\n",
      "Loss after epoch 900 : 0.1319847441633561\n",
      "Loss after epoch 910 : 0.13179552240941222\n",
      "Loss after epoch 920 : 0.13160696692464785\n",
      "Loss after epoch 930 : 0.13141907462312316\n",
      "Loss after epoch 940 : 0.13123184243603547\n",
      "Loss after epoch 950 : 0.13104526731173194\n",
      "Loss after epoch 960 : 0.13085934621571527\n",
      "Loss after epoch 970 : 0.13067407613064302\n",
      "Loss after epoch 980 : 0.1304894540563218\n",
      "Loss after epoch 990 : 0.13030547700969522\n",
      "Loss after epoch 1001 : 0.13014044671471006\n"
     ]
    }
   ],
   "source": [
    "c = nn_sigmoid.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "71419ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7001\n"
     ]
    }
   ],
   "source": [
    "acc = nn_sigmoid.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e67115",
   "metadata": {},
   "source": [
    "## Conclusion : Sigmoid is slow to learn lets try tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "9bd1df52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.19901271791427455\n",
      "Loss after epoch 20 : 0.1839460437641077\n",
      "Loss after epoch 30 : 0.1760564874681165\n",
      "Loss after epoch 40 : 0.17084134159373232\n",
      "Loss after epoch 50 : 0.16702597105090627\n",
      "Loss after epoch 60 : 0.1640225265694221\n",
      "Loss after epoch 70 : 0.16153178938422919\n",
      "Loss after epoch 80 : 0.1593877113964945\n",
      "Loss after epoch 90 : 0.1574894483359804\n",
      "Loss after epoch 100 : 0.15577189774246541\n",
      "Loss after epoch 110 : 0.15419132016498646\n",
      "Loss after epoch 120 : 0.15271742154501597\n",
      "Loss after epoch 130 : 0.1513287025573449\n",
      "Loss after epoch 140 : 0.15000958318016647\n",
      "Loss after epoch 150 : 0.14874855503497456\n",
      "Loss after epoch 160 : 0.1475369643038639\n",
      "Loss after epoch 170 : 0.14636819285711322\n",
      "Loss after epoch 180 : 0.14523709684316952\n",
      "Loss after epoch 190 : 0.1441396165799856\n",
      "Loss after epoch 200 : 0.1430725024526592\n",
      "Loss after epoch 210 : 0.1420331191153498\n",
      "Loss after epoch 220 : 0.14101930220177158\n",
      "Loss after epoch 230 : 0.14002925120224552\n",
      "Loss after epoch 240 : 0.13906144899884254\n",
      "Loss after epoch 250 : 0.13811460190333721\n",
      "Loss after epoch 260 : 0.13718759497602115\n",
      "Loss after epoch 270 : 0.13627945807231895\n",
      "Loss after epoch 280 : 0.13538933926303398\n",
      "Loss after epoch 290 : 0.13451648352834858\n",
      "Loss after epoch 300 : 0.13366021550933285\n",
      "Loss after epoch 310 : 0.13281992557566638\n",
      "Loss after epoch 320 : 0.1319950586608364\n",
      "Loss after epoch 330 : 0.13118510537276504\n",
      "Loss after epoch 340 : 0.13038959492067734\n",
      "Loss after epoch 350 : 0.12960808945769625\n",
      "Loss after epoch 360 : 0.1288401795190163\n",
      "Loss after epoch 370 : 0.12808548031431238\n",
      "Loss after epoch 380 : 0.12734362869478438\n",
      "Loss after epoch 390 : 0.12661428065800878\n",
      "Loss after epoch 400 : 0.12589710928317568\n",
      "Loss after epoch 410 : 0.12519180301180968\n",
      "Loss after epoch 420 : 0.1244980642085437\n",
      "Loss after epoch 430 : 0.12381560795335711\n",
      "Loss after epoch 440 : 0.12314416102921195\n",
      "Loss after epoch 450 : 0.12248346107576023\n",
      "Loss after epoch 460 : 0.12183325588134578\n",
      "Loss after epoch 470 : 0.1211933027845933\n",
      "Loss after epoch 480 : 0.12056336815664195\n",
      "Loss after epoch 490 : 0.11994322693750929\n",
      "Loss after epoch 500 : 0.11933266220522033\n",
      "Loss after epoch 510 : 0.11873146476295646\n",
      "Loss after epoch 520 : 0.11813943273604033\n",
      "Loss after epoch 530 : 0.11755637117597859\n",
      "Loss after epoch 540 : 0.11698209167260012\n",
      "Loss after epoch 550 : 0.1164164119775913\n",
      "Loss after epoch 560 : 0.11585915564370361\n",
      "Loss after epoch 570 : 0.11531015168390535\n",
      "Loss after epoch 580 : 0.11476923425402688\n",
      "Loss after epoch 590 : 0.1142362423612483\n",
      "Loss after epoch 600 : 0.11371101959930832\n",
      "Loss after epoch 610 : 0.11319341390979751\n",
      "Loss after epoch 620 : 0.11268327736753264\n",
      "Loss after epoch 630 : 0.11218046598698429\n",
      "Loss after epoch 640 : 0.11168483954614632\n",
      "Loss after epoch 650 : 0.11119626142415438\n",
      "Loss after epoch 660 : 0.11071459844934113\n",
      "Loss after epoch 670 : 0.11023972075517974\n",
      "Loss after epoch 680 : 0.10977150164257858\n",
      "Loss after epoch 690 : 0.10930981744808868\n",
      "Loss after epoch 700 : 0.10885454741860477\n",
      "Loss after epoch 710 : 0.1084055735938848\n",
      "Loss after epoch 720 : 0.10796278069852984\n",
      "Loss after epoch 730 : 0.10752605604480434\n",
      "Loss after epoch 740 : 0.10709528944676204\n",
      "Loss after epoch 750 : 0.10667037314459427\n",
      "Loss after epoch 760 : 0.10625120173610335\n",
      "Loss after epoch 770 : 0.10583767211015492\n",
      "Loss after epoch 780 : 0.10542968337561727\n",
      "Loss after epoch 790 : 0.10502713677958037\n",
      "Loss after epoch 800 : 0.10462993561126337\n",
      "Loss after epoch 810 : 0.10423798509273807\n",
      "Loss after epoch 820 : 0.10385119226285923\n",
      "Loss after epoch 830 : 0.10346946586420637\n",
      "Loss after epoch 840 : 0.10309271624260861\n",
      "Loss after epoch 850 : 0.10272085526509564\n",
      "Loss after epoch 860 : 0.10235379625692859\n",
      "Loss after epoch 870 : 0.10199145395421008\n",
      "Loss after epoch 880 : 0.10163374446683003\n",
      "Loss after epoch 890 : 0.1012805852470244\n",
      "Loss after epoch 900 : 0.1009318950605125\n",
      "Loss after epoch 910 : 0.10058759395895411\n",
      "Loss after epoch 920 : 0.10024760325369379\n",
      "Loss after epoch 930 : 0.09991184549131861\n",
      "Loss after epoch 940 : 0.09958024443161306\n",
      "Loss after epoch 950 : 0.09925272502829627\n",
      "Loss after epoch 960 : 0.09892921341267177\n",
      "Loss after epoch 970 : 0.0986096368801239\n",
      "Loss after epoch 980 : 0.09829392387928726\n",
      "Loss after epoch 990 : 0.0979820040036948\n",
      "Loss after epoch 1001 : 0.09770446196459562\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "fb3d65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7345\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231cc3",
   "metadata": {},
   "source": [
    "## tanh faster to learn lets try more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "15e4a857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.09736926769271226\n",
      "Loss after epoch 20 : 0.09706831612501637\n",
      "Loss after epoch 30 : 0.09677088741593402\n",
      "Loss after epoch 40 : 0.09647691683306854\n",
      "Loss after epoch 50 : 0.0961863407807747\n",
      "Loss after epoch 60 : 0.09589909680308277\n",
      "Loss after epoch 70 : 0.09561512358641353\n",
      "Loss after epoch 80 : 0.09533436096136601\n",
      "Loss after epoch 90 : 0.09505674990296167\n",
      "Loss after epoch 100 : 0.0947822325289809\n",
      "Loss after epoch 110 : 0.09451075209640591\n",
      "Loss after epoch 120 : 0.09424225299642072\n",
      "Loss after epoch 130 : 0.09397668074879285\n",
      "Loss after epoch 140 : 0.09371398199667001\n",
      "Loss after epoch 150 : 0.0934541045027858\n",
      "Loss after epoch 160 : 0.0931969971477858\n",
      "Loss after epoch 170 : 0.09294260993093617\n",
      "Loss after epoch 180 : 0.09269089397298262\n",
      "Loss after epoch 190 : 0.09244180152051566\n",
      "Loss after epoch 200 : 0.09219528595094599\n",
      "Loss after epoch 210 : 0.09195130177713623\n",
      "Loss after epoch 220 : 0.09170980465083806\n",
      "Loss after epoch 230 : 0.09147075136430176\n",
      "Loss after epoch 240 : 0.09123409984969526\n",
      "Loss after epoch 250 : 0.09099980917624488\n",
      "Loss after epoch 260 : 0.0907678395452582\n",
      "Loss after epoch 270 : 0.0905381522834008\n",
      "Loss after epoch 280 : 0.09031070983476454\n",
      "Loss after epoch 290 : 0.0900854757523992\n",
      "Loss after epoch 300 : 0.08986241469008072\n",
      "Loss after epoch 310 : 0.08964149239516844\n",
      "Loss after epoch 320 : 0.08942267570345218\n",
      "Loss after epoch 330 : 0.08920593253689439\n",
      "Loss after epoch 340 : 0.08899123190509263\n",
      "Loss after epoch 350 : 0.08877854391105816\n",
      "Loss after epoch 360 : 0.0885678397614346\n",
      "Loss after epoch 370 : 0.08835909178044499\n",
      "Loss after epoch 380 : 0.08815227342556252\n",
      "Loss after epoch 390 : 0.0879473593011377\n",
      "Loss after epoch 400 : 0.08774432516420193\n",
      "Loss after epoch 410 : 0.0875431479149531\n",
      "Loss after epoch 420 : 0.08734380556394156\n",
      "Loss after epoch 430 : 0.08714627716974316\n",
      "Loss after epoch 440 : 0.08695054274550859\n",
      "Loss after epoch 450 : 0.08675658313955904\n",
      "Loss after epoch 460 : 0.08656437990204534\n",
      "Loss after epoch 470 : 0.08637391515369741\n",
      "Loss after epoch 480 : 0.08618517147185174\n",
      "Loss after epoch 490 : 0.08599813180348567\n",
      "Loss after epoch 500 : 0.08581277940735768\n",
      "Loss after epoch 510 : 0.08562909782069208\n",
      "Loss after epoch 520 : 0.08544707084221455\n",
      "Loss after epoch 530 : 0.08526668252298802\n",
      "Loss after epoch 540 : 0.08508791715836174\n",
      "Loss after epoch 550 : 0.08491075927700245\n",
      "Loss after epoch 560 : 0.08473519362538796\n",
      "Loss after epoch 570 : 0.08456120514783586\n",
      "Loss after epoch 580 : 0.08438877896306993\n",
      "Loss after epoch 590 : 0.08421790033868302\n",
      "Loss after epoch 600 : 0.08404855466484418\n",
      "Loss after epoch 610 : 0.08388072742841426\n",
      "Loss after epoch 620 : 0.08371440418838545\n",
      "Loss after epoch 630 : 0.08354957055331472\n",
      "Loss after epoch 640 : 0.08338621216120794\n",
      "Loss after epoch 650 : 0.08322431466213442\n",
      "Loss after epoch 660 : 0.08306386370371671\n",
      "Loss after epoch 670 : 0.08290484491952846\n",
      "Loss after epoch 680 : 0.08274724392035453\n",
      "Loss after epoch 690 : 0.0825910462882024\n",
      "Loss after epoch 700 : 0.08243623757290602\n",
      "Loss after epoch 710 : 0.08228280329112923\n",
      "Loss after epoch 720 : 0.08213072892754765\n",
      "Loss after epoch 730 : 0.0819799999379744\n",
      "Loss after epoch 740 : 0.08183060175418216\n",
      "Loss after epoch 750 : 0.0816825197901732\n",
      "Loss after epoch 760 : 0.08153573944964948\n",
      "Loss after epoch 770 : 0.08139024613444407\n",
      "Loss after epoch 780 : 0.08124602525368335\n",
      "Loss after epoch 790 : 0.08110306223346747\n",
      "Loss after epoch 800 : 0.0809613425268691\n",
      "Loss after epoch 810 : 0.08082085162407393\n",
      "Loss after epoch 820 : 0.08068157506250224\n",
      "Loss after epoch 830 : 0.08054349843677461\n",
      "Loss after epoch 840 : 0.08040660740840387\n",
      "Loss after epoch 850 : 0.0802708877151166\n",
      "Loss after epoch 860 : 0.08013632517972694\n",
      "Loss after epoch 870 : 0.08000290571850426\n",
      "Loss after epoch 880 : 0.0798706153489934\n",
      "Loss after epoch 890 : 0.07973944019726295\n",
      "Loss after epoch 900 : 0.07960936650456993\n",
      "Loss after epoch 910 : 0.0794803806334436\n",
      "Loss after epoch 920 : 0.07935246907320018\n",
      "Loss after epoch 930 : 0.07922561844491167\n",
      "Loss after epoch 940 : 0.07909981550585782\n",
      "Loss after epoch 950 : 0.07897504715349585\n",
      "Loss after epoch 960 : 0.07885130042898834\n",
      "Loss after epoch 970 : 0.07872856252032946\n",
      "Loss after epoch 980 : 0.07860682076511134\n",
      "Loss after epoch 990 : 0.07848606265297105\n",
      "Loss after epoch 1001 : 0.07837821115129574\n"
     ]
    }
   ],
   "source": [
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "ff918631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7616\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039da005",
   "metadata": {},
   "source": [
    "## Accuracy increases, lets try more hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "4d332421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.18243071300010974\n",
      "Loss after epoch 20 : 0.16443330947833454\n",
      "Loss after epoch 30 : 0.15405631998614444\n",
      "Loss after epoch 40 : 0.14675462757163799\n",
      "Loss after epoch 50 : 0.14134486145826794\n",
      "Loss after epoch 60 : 0.13708421482458347\n",
      "Loss after epoch 70 : 0.13355617250438886\n",
      "Loss after epoch 80 : 0.13052072819131544\n",
      "Loss after epoch 90 : 0.12783552961703754\n",
      "Loss after epoch 100 : 0.12541230826896504\n",
      "Loss after epoch 110 : 0.12319353806869811\n",
      "Loss after epoch 120 : 0.12113998922543047\n",
      "Loss after epoch 130 : 0.11922385426614797\n",
      "Loss after epoch 140 : 0.11742474232179387\n",
      "Loss after epoch 150 : 0.11572721248276718\n",
      "Loss after epoch 160 : 0.1141192226790189\n",
      "Loss after epoch 170 : 0.11259115084606686\n",
      "Loss after epoch 180 : 0.11113515371283451\n",
      "Loss after epoch 190 : 0.10974472511765353\n",
      "Loss after epoch 200 : 0.10841438165369169\n",
      "Loss after epoch 210 : 0.10713943371617227\n",
      "Loss after epoch 220 : 0.10591581517898954\n",
      "Loss after epoch 230 : 0.10473995524159851\n",
      "Loss after epoch 240 : 0.10360868274578552\n",
      "Loss after epoch 250 : 0.10251915581817454\n",
      "Loss after epoch 260 : 0.10146880987948456\n",
      "Loss after epoch 270 : 0.1004553178284084\n",
      "Loss after epoch 280 : 0.09947655821088398\n",
      "Loss after epoch 290 : 0.09853058916697592\n",
      "Loss after epoch 300 : 0.09761562702851867\n",
      "Loss after epoch 310 : 0.09673002873837748\n",
      "Loss after epoch 320 : 0.09587227724697223\n",
      "Loss after epoch 330 : 0.09504096904747437\n",
      "Loss after epoch 340 : 0.09423480316576148\n",
      "Loss after epoch 350 : 0.09345257121324714\n",
      "Loss after epoch 360 : 0.0926931484241993\n",
      "Loss after epoch 370 : 0.09195548577031135\n",
      "Loss after epoch 380 : 0.09123860319958146\n",
      "Loss after epoch 390 : 0.09054158388209181\n",
      "Loss after epoch 400 : 0.08986356922435441\n",
      "Loss after epoch 410 : 0.0892037544015228\n",
      "Loss after epoch 420 : 0.08856138421148663\n",
      "Loss after epoch 430 : 0.08793574912586402\n",
      "Loss after epoch 440 : 0.08732618148661202\n",
      "Loss after epoch 450 : 0.08673205187009163\n",
      "Loss after epoch 460 : 0.08615276569403126\n",
      "Loss after epoch 470 : 0.08558776015444854\n",
      "Loss after epoch 480 : 0.08503650154795368\n",
      "Loss after epoch 490 : 0.08449848298288262\n",
      "Loss after epoch 500 : 0.08397322243751476\n",
      "Loss after epoch 510 : 0.08346026109871575\n",
      "Loss after epoch 520 : 0.08295916190906946\n",
      "Loss after epoch 530 : 0.08246950825887998\n",
      "Loss after epoch 540 : 0.08199090277553896\n",
      "Loss after epoch 550 : 0.08152296618171387\n",
      "Loss after epoch 560 : 0.08106533621111202\n",
      "Loss after epoch 570 : 0.08061766658282632\n",
      "Loss after epoch 580 : 0.08017962604105242\n",
      "Loss after epoch 590 : 0.07975089746707718\n",
      "Loss after epoch 600 : 0.07933117706729095\n",
      "Loss after epoch 610 : 0.07892017363770326\n",
      "Loss after epoch 620 : 0.0785176079049622\n",
      "Loss after epoch 630 : 0.07812321194825357\n",
      "Loss after epoch 640 : 0.07773672871621853\n",
      "Loss after epoch 650 : 0.07735791166561128\n",
      "Loss after epoch 660 : 0.07698652455431988\n",
      "Loss after epoch 670 : 0.076622341399117\n",
      "Loss after epoch 680 : 0.0762651465282589\n",
      "Loss after epoch 690 : 0.07591473451837219\n",
      "Loss after epoch 700 : 0.07557090969945086\n",
      "Loss after epoch 710 : 0.07523348503251255\n",
      "Loss after epoch 720 : 0.07490228056586658\n",
      "Loss after epoch 730 : 0.07457712203269914\n",
      "Loss after epoch 740 : 0.0742578400658888\n",
      "Loss after epoch 750 : 0.07394427007420604\n",
      "Loss after epoch 760 : 0.0736362524984771\n",
      "Loss after epoch 770 : 0.073333633144231\n",
      "Loss after epoch 780 : 0.07303626341974251\n",
      "Loss after epoch 790 : 0.07274400041835886\n",
      "Loss after epoch 800 : 0.07245670682782633\n",
      "Loss after epoch 810 : 0.07217425066611263\n",
      "Loss after epoch 820 : 0.07189650487311183\n",
      "Loss after epoch 830 : 0.0716233468312592\n",
      "Loss after epoch 840 : 0.0713546579124356\n",
      "Loss after epoch 850 : 0.07109032312734942\n",
      "Loss after epoch 860 : 0.07083023089948491\n",
      "Loss after epoch 870 : 0.07057427293628568\n",
      "Loss after epoch 880 : 0.07032234415054814\n",
      "Loss after epoch 890 : 0.07007434259286474\n",
      "Loss after epoch 900 : 0.06983016937603914\n",
      "Loss after epoch 910 : 0.06958972859312886\n",
      "Loss after epoch 920 : 0.06935292724935438\n",
      "Loss after epoch 930 : 0.06911967524666263\n",
      "Loss after epoch 940 : 0.06888988547922771\n",
      "Loss after epoch 950 : 0.06866347411184075\n",
      "Loss after epoch 960 : 0.06844036109979748\n",
      "Loss after epoch 970 : 0.06822047093281342\n",
      "Loss after epoch 980 : 0.06800373341960982\n",
      "Loss after epoch 990 : 0.06779008411321616\n",
      "Loss after epoch 1001 : 0.06760039118329622\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 2*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "61a27347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7867\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381503b",
   "metadata": {},
   "source": [
    "### More hidden nodes => Faster learning\n",
    "2X hidden nodes = 2X learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "6a1ed71b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1768686421578603\n",
      "Loss after epoch 20 : 0.1496912437462031\n",
      "Loss after epoch 30 : 0.13583368568823373\n",
      "Loss after epoch 40 : 0.12685158291716592\n",
      "Loss after epoch 50 : 0.12034212894110974\n",
      "Loss after epoch 60 : 0.11529615154308333\n",
      "Loss after epoch 70 : 0.11119942488119336\n",
      "Loss after epoch 80 : 0.10776107435545887\n",
      "Loss after epoch 90 : 0.10480530305644188\n",
      "Loss after epoch 100 : 0.10221926425090824\n",
      "Loss after epoch 110 : 0.09992576509578766\n",
      "Loss after epoch 120 : 0.09786903565317893\n",
      "Loss after epoch 130 : 0.09600721632390159\n",
      "Loss after epoch 140 : 0.0943081059081259\n",
      "Loss after epoch 150 : 0.09274651087502041\n",
      "Loss after epoch 160 : 0.09130245744484108\n",
      "Loss after epoch 170 : 0.08995992549071942\n",
      "Loss after epoch 180 : 0.0887059252038117\n",
      "Loss after epoch 190 : 0.08752981111536305\n",
      "Loss after epoch 200 : 0.08642276695041502\n",
      "Loss after epoch 210 : 0.0853774163385686\n",
      "Loss after epoch 220 : 0.08438752708130536\n",
      "Loss after epoch 230 : 0.08344778510653884\n",
      "Loss after epoch 240 : 0.08255362041599741\n",
      "Loss after epoch 250 : 0.08170107199693039\n",
      "Loss after epoch 260 : 0.08088668220027127\n",
      "Loss after epoch 270 : 0.08010741374302419\n",
      "Loss after epoch 280 : 0.0793605843871309\n",
      "Loss after epoch 290 : 0.07864381534983494\n",
      "Loss after epoch 300 : 0.07795498947450029\n",
      "Loss after epoch 310 : 0.07729221504440527\n",
      "Loss after epoch 320 : 0.07665379299857376\n",
      "Loss after epoch 330 : 0.07603818871341356\n",
      "Loss after epoch 340 : 0.07544400979805235\n",
      "Loss after epoch 350 : 0.07486998807751853\n",
      "Loss after epoch 360 : 0.07431496289768724\n",
      "Loss after epoch 370 : 0.07377786581063026\n",
      "Loss after epoch 380 : 0.07325770878491944\n",
      "Loss after epoch 390 : 0.0727535764786948\n",
      "Loss after epoch 400 : 0.07226462087531235\n",
      "Loss after epoch 410 : 0.07179005650762076\n",
      "Loss after epoch 420 : 0.07132915560127354\n",
      "Loss after epoch 430 : 0.07088124320724278\n",
      "Loss after epoch 440 : 0.07044569255343733\n",
      "Loss after epoch 450 : 0.07002192076392247\n",
      "Loss after epoch 460 : 0.06960938499367465\n",
      "Loss after epoch 470 : 0.06920757896510785\n",
      "Loss after epoch 480 : 0.0688160298645122\n",
      "Loss after epoch 490 : 0.06843429554786572\n",
      "Loss after epoch 500 : 0.06806196200698532\n",
      "Loss after epoch 510 : 0.06769864105423638\n",
      "Loss after epoch 520 : 0.06734396819471258\n",
      "Loss after epoch 530 : 0.0669976006668523\n",
      "Loss after epoch 540 : 0.06665921564343667\n",
      "Loss after epoch 550 : 0.06632850859250598\n",
      "Loss after epoch 560 : 0.06600519180041493\n",
      "Loss after epoch 570 : 0.06568899305672103\n",
      "Loss after epoch 580 : 0.0653796544937743\n",
      "Loss after epoch 590 : 0.0650769315648214\n",
      "Loss after epoch 600 : 0.0647805921361383\n",
      "Loss after epoch 610 : 0.06449041566449845\n",
      "Loss after epoch 620 : 0.06420619243367187\n",
      "Loss after epoch 630 : 0.06392772283285465\n",
      "Loss after epoch 640 : 0.06365481667295705\n",
      "Loss after epoch 650 : 0.06338729254812124\n",
      "Loss after epoch 660 : 0.06312497725467398\n",
      "Loss after epoch 670 : 0.06286770527643322\n",
      "Loss after epoch 680 : 0.06261531833673059\n",
      "Loss after epoch 690 : 0.06236766500870075\n",
      "Loss after epoch 700 : 0.06212460037018027\n",
      "Loss after epoch 710 : 0.06188598568893604\n",
      "Loss after epoch 720 : 0.06165168812657223\n",
      "Loss after epoch 730 : 0.06142158045331721\n",
      "Loss after epoch 740 : 0.06119554076948147\n",
      "Loss after epoch 750 : 0.06097345223209669\n",
      "Loss after epoch 760 : 0.06075520278711186\n",
      "Loss after epoch 770 : 0.060540684908753015\n",
      "Loss after epoch 780 : 0.0603297953484085\n",
      "Loss after epoch 790 : 0.06012243489570101\n",
      "Loss after epoch 800 : 0.05991850815420956\n",
      "Loss after epoch 810 : 0.05971792333361913\n",
      "Loss after epoch 820 : 0.05952059205904803\n",
      "Loss after epoch 830 : 0.05932642919717323\n",
      "Loss after epoch 840 : 0.05913535269781354\n",
      "Loss after epoch 850 : 0.058947283449013045\n",
      "Loss after epoch 860 : 0.058762145143454035\n",
      "Loss after epoch 870 : 0.05857986415414876\n",
      "Loss after epoch 880 : 0.05840036941769183\n",
      "Loss after epoch 890 : 0.05822359232376828\n",
      "Loss after epoch 900 : 0.058049466610005\n",
      "Loss after epoch 910 : 0.05787792826157432\n",
      "Loss after epoch 920 : 0.05770891541518556\n",
      "Loss after epoch 930 : 0.05754236826724766\n",
      "Loss after epoch 940 : 0.05737822898606184\n",
      "Loss after epoch 950 : 0.05721644162794217\n",
      "Loss after epoch 960 : 0.057056952057164295\n",
      "Loss after epoch 970 : 0.05689970786964134\n",
      "Loss after epoch 980 : 0.05674465832020976\n",
      "Loss after epoch 990 : 0.056591754253400385\n",
      "Loss after epoch 1001 : 0.05645593558288547\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "7be2f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8087\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f615843",
   "metadata": {},
   "source": [
    "### Slightly better results, learning not that fast let's try relu before adding optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "05601545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.22090266196295236\n",
      "Loss after epoch 20 : 0.19523942278124612\n",
      "Loss after epoch 30 : 0.1841315866303666\n",
      "Loss after epoch 40 : 0.17746053111685306\n",
      "Loss after epoch 50 : 0.17263425961832415\n",
      "Loss after epoch 60 : 0.1688167680664316\n",
      "Loss after epoch 70 : 0.16562200664513124\n",
      "Loss after epoch 80 : 0.1628434081903318\n",
      "Loss after epoch 90 : 0.1603743470318354\n",
      "Loss after epoch 100 : 0.15813575456560658\n",
      "Loss after epoch 110 : 0.15606843318793037\n",
      "Loss after epoch 120 : 0.15413539092196019\n",
      "Loss after epoch 130 : 0.15230972958807631\n",
      "Loss after epoch 140 : 0.15056102913785943\n",
      "Loss after epoch 150 : 0.1488740647590752\n",
      "Loss after epoch 160 : 0.14724008855485587\n",
      "Loss after epoch 170 : 0.14565090588175914\n",
      "Loss after epoch 180 : 0.1441013395402311\n",
      "Loss after epoch 190 : 0.14259182558012873\n",
      "Loss after epoch 200 : 0.14112181406831384\n",
      "Loss after epoch 210 : 0.13969009529060336\n",
      "Loss after epoch 220 : 0.1382972423931227\n",
      "Loss after epoch 230 : 0.13693682308501795\n",
      "Loss after epoch 240 : 0.13560593262598894\n",
      "Loss after epoch 250 : 0.1343074739056616\n",
      "Loss after epoch 260 : 0.13304257964005772\n",
      "Loss after epoch 270 : 0.1318083590433974\n",
      "Loss after epoch 280 : 0.13060566788899935\n",
      "Loss after epoch 290 : 0.12943215881898\n",
      "Loss after epoch 300 : 0.12828724693310586\n",
      "Loss after epoch 310 : 0.1271668502243643\n",
      "Loss after epoch 320 : 0.12607113438212403\n",
      "Loss after epoch 330 : 0.12499906808479451\n",
      "Loss after epoch 340 : 0.12394900512585555\n",
      "Loss after epoch 350 : 0.1229218048363989\n",
      "Loss after epoch 360 : 0.1219158257531586\n",
      "Loss after epoch 370 : 0.12093082771135237\n",
      "Loss after epoch 380 : 0.1199654904309692\n",
      "Loss after epoch 390 : 0.11902062814518584\n",
      "Loss after epoch 400 : 0.11809673499453953\n",
      "Loss after epoch 410 : 0.11719255187128488\n",
      "Loss after epoch 420 : 0.11630847574105911\n",
      "Loss after epoch 430 : 0.11544481986030876\n",
      "Loss after epoch 440 : 0.11459952334427274\n",
      "Loss after epoch 450 : 0.11377308618055734\n",
      "Loss after epoch 460 : 0.11296476863206907\n",
      "Loss after epoch 470 : 0.1121742627492154\n",
      "Loss after epoch 480 : 0.11140136539681254\n",
      "Loss after epoch 490 : 0.11064599184118143\n",
      "Loss after epoch 500 : 0.10990767610601543\n",
      "Loss after epoch 510 : 0.10918603065782718\n",
      "Loss after epoch 520 : 0.1084809332560679\n",
      "Loss after epoch 530 : 0.10778959867113146\n",
      "Loss after epoch 540 : 0.10711239662358271\n",
      "Loss after epoch 550 : 0.10644816243444187\n",
      "Loss after epoch 560 : 0.10579684849336374\n",
      "Loss after epoch 570 : 0.10515782477055073\n",
      "Loss after epoch 580 : 0.10453184738864911\n",
      "Loss after epoch 590 : 0.10391695336583587\n",
      "Loss after epoch 600 : 0.10331365523236627\n",
      "Loss after epoch 610 : 0.10272241627533037\n",
      "Loss after epoch 620 : 0.10214387429825714\n",
      "Loss after epoch 630 : 0.10157635352361732\n",
      "Loss after epoch 640 : 0.10101907372341187\n",
      "Loss after epoch 650 : 0.10046980633621398\n",
      "Loss after epoch 660 : 0.09992936025631811\n",
      "Loss after epoch 670 : 0.09940003914041569\n",
      "Loss after epoch 680 : 0.09888061085298097\n",
      "Loss after epoch 690 : 0.09837032182634885\n",
      "Loss after epoch 700 : 0.0978691249453477\n",
      "Loss after epoch 710 : 0.09737629569475924\n",
      "Loss after epoch 720 : 0.09689087075800547\n",
      "Loss after epoch 730 : 0.09641199661227567\n",
      "Loss after epoch 740 : 0.09593995500034988\n",
      "Loss after epoch 750 : 0.09547572443319192\n",
      "Loss after epoch 760 : 0.09501847929733992\n",
      "Loss after epoch 770 : 0.0945676446587438\n",
      "Loss after epoch 780 : 0.09412284238906087\n",
      "Loss after epoch 790 : 0.09368431935983894\n",
      "Loss after epoch 800 : 0.09325086382752236\n",
      "Loss after epoch 810 : 0.0928229163103153\n",
      "Loss after epoch 820 : 0.09239998651350811\n",
      "Loss after epoch 830 : 0.0919819137256425\n",
      "Loss after epoch 840 : 0.09156826798012073\n",
      "Loss after epoch 850 : 0.09115813132094475\n",
      "Loss after epoch 860 : 0.09075122373045519\n",
      "Loss after epoch 870 : 0.09034767535736261\n",
      "Loss after epoch 880 : 0.08994688061205991\n",
      "Loss after epoch 890 : 0.08954777340816472\n",
      "Loss after epoch 900 : 0.08915115623244153\n",
      "Loss after epoch 910 : 0.08875655250427175\n",
      "Loss after epoch 920 : 0.08836392367614222\n",
      "Loss after epoch 930 : 0.08797342287458562\n",
      "Loss after epoch 940 : 0.08758453665585181\n",
      "Loss after epoch 950 : 0.08719752387125895\n",
      "Loss after epoch 960 : 0.08681289993906087\n",
      "Loss after epoch 970 : 0.0864305695535124\n",
      "Loss after epoch 980 : 0.08604973156698663\n",
      "Loss after epoch 990 : 0.08567111706052381\n",
      "Loss after epoch 1001 : 0.08533235345436108\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "f5a86c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7503\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c085d4e",
   "metadata": {},
   "source": [
    "# Similar results to tanh, let's add hidden nodes and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "5ffb3c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.20999322813225482\n",
      "Loss after epoch 20 : 0.16621525188621342\n",
      "Loss after epoch 30 : 0.14329902067824313\n",
      "Loss after epoch 40 : 0.12870198180130324\n",
      "Loss after epoch 50 : 0.11832672042598538\n",
      "Loss after epoch 60 : 0.11044672544622539\n",
      "Loss after epoch 70 : 0.10420482189985736\n",
      "Loss after epoch 80 : 0.09912683063309624\n",
      "Loss after epoch 90 : 0.0949092299038056\n",
      "Loss after epoch 100 : 0.09133997121026016\n",
      "Loss after epoch 110 : 0.0882733975150196\n",
      "Loss after epoch 120 : 0.08560483915792877\n",
      "Loss after epoch 130 : 0.0832561327578845\n",
      "Loss after epoch 140 : 0.08116321210382554\n",
      "Loss after epoch 150 : 0.07928060696679286\n",
      "Loss after epoch 160 : 0.07757213931440257\n",
      "Loss after epoch 170 : 0.07601432521725095\n",
      "Loss after epoch 180 : 0.07458399309461819\n",
      "Loss after epoch 190 : 0.07326676932193549\n",
      "Loss after epoch 200 : 0.07205356699734393\n",
      "Loss after epoch 210 : 0.07092939149681263\n",
      "Loss after epoch 220 : 0.06988344304179005\n",
      "Loss after epoch 230 : 0.06891068379429008\n",
      "Loss after epoch 240 : 0.06800243629821691\n",
      "Loss after epoch 250 : 0.06715108665229196\n",
      "Loss after epoch 260 : 0.06634963788226204\n",
      "Loss after epoch 270 : 0.0655948169535663\n",
      "Loss after epoch 280 : 0.06488370225047552\n",
      "Loss after epoch 290 : 0.06421095629140897\n",
      "Loss after epoch 300 : 0.06357443154442967\n",
      "Loss after epoch 310 : 0.06297106078136225\n",
      "Loss after epoch 320 : 0.06239747716589233\n",
      "Loss after epoch 330 : 0.061850512296591414\n",
      "Loss after epoch 340 : 0.06132917719220682\n",
      "Loss after epoch 350 : 0.060832247089748985\n",
      "Loss after epoch 360 : 0.06035797951688438\n",
      "Loss after epoch 370 : 0.059904377975945354\n",
      "Loss after epoch 380 : 0.05946997005591431\n",
      "Loss after epoch 390 : 0.059053386717776046\n",
      "Loss after epoch 400 : 0.0586535185565746\n",
      "Loss after epoch 410 : 0.05826918040014603\n",
      "Loss after epoch 420 : 0.05789976512312447\n",
      "Loss after epoch 430 : 0.057544078729395924\n",
      "Loss after epoch 440 : 0.05720073388144488\n",
      "Loss after epoch 450 : 0.05686991656099373\n",
      "Loss after epoch 460 : 0.05655051525850738\n",
      "Loss after epoch 470 : 0.056241685470699646\n",
      "Loss after epoch 480 : 0.05594236735908761\n",
      "Loss after epoch 490 : 0.055651924587826265\n",
      "Loss after epoch 500 : 0.05537055716453745\n",
      "Loss after epoch 510 : 0.05509810976072097\n",
      "Loss after epoch 520 : 0.05483350125873447\n",
      "Loss after epoch 530 : 0.05457637127205065\n",
      "Loss after epoch 540 : 0.054326293887637316\n",
      "Loss after epoch 550 : 0.05408312987713889\n",
      "Loss after epoch 560 : 0.053846773139575595\n",
      "Loss after epoch 570 : 0.053616841849937404\n",
      "Loss after epoch 580 : 0.05339360018966934\n",
      "Loss after epoch 590 : 0.053177062608005275\n",
      "Loss after epoch 600 : 0.052966981938444194\n",
      "Loss after epoch 610 : 0.05276287330456872\n",
      "Loss after epoch 620 : 0.05256484461168967\n",
      "Loss after epoch 630 : 0.0523723726609366\n",
      "Loss after epoch 640 : 0.05218537004655258\n",
      "Loss after epoch 650 : 0.05200338506065297\n",
      "Loss after epoch 660 : 0.0518256781174227\n",
      "Loss after epoch 670 : 0.051652594929976405\n",
      "Loss after epoch 680 : 0.05148405488554234\n",
      "Loss after epoch 690 : 0.05132035052602539\n",
      "Loss after epoch 700 : 0.05116064765409251\n",
      "Loss after epoch 710 : 0.05100466649885098\n",
      "Loss after epoch 720 : 0.05085217187112786\n",
      "Loss after epoch 730 : 0.05070339986586884\n",
      "Loss after epoch 740 : 0.050558312120043684\n",
      "Loss after epoch 750 : 0.05041691814812104\n",
      "Loss after epoch 760 : 0.050278655192809586\n",
      "Loss after epoch 770 : 0.05014336014099392\n",
      "Loss after epoch 780 : 0.050010927996006684\n",
      "Loss after epoch 790 : 0.049881326215012525\n",
      "Loss after epoch 800 : 0.04975439706104884\n",
      "Loss after epoch 810 : 0.049629631598041714\n",
      "Loss after epoch 820 : 0.04950695631566017\n",
      "Loss after epoch 830 : 0.049386383560165184\n",
      "Loss after epoch 840 : 0.04926780838196524\n",
      "Loss after epoch 850 : 0.049151337647006424\n",
      "Loss after epoch 860 : 0.04903683155284055\n",
      "Loss after epoch 870 : 0.048924233965957735\n",
      "Loss after epoch 880 : 0.04881334123915737\n",
      "Loss after epoch 890 : 0.0487044144000415\n",
      "Loss after epoch 900 : 0.04859755936177264\n",
      "Loss after epoch 910 : 0.04849251747187844\n",
      "Loss after epoch 920 : 0.048389381313359424\n",
      "Loss after epoch 930 : 0.048288074870867746\n",
      "Loss after epoch 940 : 0.04818852291781872\n",
      "Loss after epoch 950 : 0.04809051338672313\n",
      "Loss after epoch 960 : 0.04799421212090101\n",
      "Loss after epoch 970 : 0.04789954688590069\n",
      "Loss after epoch 980 : 0.047806509477423605\n",
      "Loss after epoch 990 : 0.047714849873844216\n",
      "Loss after epoch 1001 : 0.04763351629522698\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "b87bf633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8209\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6bd219",
   "metadata": {},
   "source": [
    "## Better accuracy than tanh, lets try optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f25d8f",
   "metadata": {},
   "source": [
    "# Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058f85d",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "Dropping sigmoid as tanh is almost always better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "4ebd4cd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.05185703938220312\n",
      "Loss after epoch 20 : 0.04471078186022689\n",
      "Loss after epoch 30 : 0.041428303200184785\n",
      "Loss after epoch 40 : 0.03939626268286578\n",
      "Loss after epoch 50 : 0.037954493371327516\n",
      "Loss after epoch 60 : 0.0368417009344809\n",
      "Loss after epoch 70 : 0.03592997290066909\n",
      "Loss after epoch 80 : 0.03515323775305513\n",
      "Loss after epoch 90 : 0.03447133335275145\n",
      "Loss after epoch 100 : 0.03385885670744845\n",
      "Loss after epoch 110 : 0.033305375640224975\n",
      "Loss after epoch 120 : 0.03279715846604191\n",
      "Loss after epoch 130 : 0.0323252801148695\n",
      "Loss after epoch 140 : 0.03188310428444356\n",
      "Loss after epoch 150 : 0.03147076768105533\n",
      "Loss after epoch 160 : 0.031083000505919573\n",
      "Loss after epoch 170 : 0.030716833912471483\n",
      "Loss after epoch 180 : 0.030369887815975154\n",
      "Loss after epoch 190 : 0.03004014176796332\n",
      "Loss after epoch 200 : 0.029725845380103078\n",
      "Loss after epoch 210 : 0.029425524437904842\n",
      "Loss after epoch 220 : 0.02913772080378199\n",
      "Loss after epoch 230 : 0.02886090643554675\n",
      "Loss after epoch 240 : 0.028595676854613946\n",
      "Loss after epoch 250 : 0.028340918159051348\n",
      "Loss after epoch 260 : 0.028095682817168705\n",
      "Loss after epoch 270 : 0.027859205329726182\n",
      "Loss after epoch 280 : 0.027630789903802844\n",
      "Loss after epoch 290 : 0.027409784455094958\n",
      "Loss after epoch 300 : 0.027195288842105696\n",
      "Loss after epoch 310 : 0.026987026173189873\n",
      "Loss after epoch 320 : 0.026785816930689647\n",
      "Loss after epoch 330 : 0.026590455838755172\n",
      "Loss after epoch 340 : 0.026400444708405777\n",
      "Loss after epoch 350 : 0.026215664857775173\n",
      "Loss after epoch 360 : 0.026036046867623727\n",
      "Loss after epoch 370 : 0.02586227644589221\n",
      "Loss after epoch 380 : 0.025693431175843946\n",
      "Loss after epoch 390 : 0.02552912382153214\n",
      "Loss after epoch 400 : 0.02536918984764279\n",
      "Loss after epoch 410 : 0.025213421996434733\n",
      "Loss after epoch 420 : 0.02506123661288793\n",
      "Loss after epoch 430 : 0.024913184791523987\n",
      "Loss after epoch 440 : 0.02476888837793714\n",
      "Loss after epoch 450 : 0.02462801060952916\n",
      "Loss after epoch 460 : 0.0244903547874982\n",
      "Loss after epoch 470 : 0.02435581418757769\n",
      "Loss after epoch 480 : 0.024224289995290128\n",
      "Loss after epoch 490 : 0.024095670676648053\n",
      "Loss after epoch 500 : 0.023969818914531666\n",
      "Loss after epoch 510 : 0.02384659123683394\n",
      "Loss after epoch 520 : 0.023725851536277175\n",
      "Loss after epoch 530 : 0.023607469078975363\n",
      "Loss after epoch 540 : 0.02349131531737613\n",
      "Loss after epoch 550 : 0.02337726218048874\n",
      "Loss after epoch 560 : 0.02326518421541372\n",
      "Loss after epoch 570 : 0.02315498119681816\n",
      "Loss after epoch 580 : 0.02304661198483635\n",
      "Loss after epoch 590 : 0.02294004610562487\n",
      "Loss after epoch 600 : 0.022835221123441923\n",
      "Loss after epoch 610 : 0.022732064948268824\n",
      "Loss after epoch 620 : 0.022630508028907856\n",
      "Loss after epoch 630 : 0.022530486879416514\n",
      "Loss after epoch 640 : 0.022431944107153563\n",
      "Loss after epoch 650 : 0.022334818899268107\n",
      "Loss after epoch 660 : 0.022239024652626362\n",
      "Loss after epoch 670 : 0.022144473856762915\n",
      "Loss after epoch 680 : 0.022051139631608877\n",
      "Loss after epoch 690 : 0.02195874152065588\n",
      "Loss after epoch 700 : 0.021867349375400934\n",
      "Loss after epoch 710 : 0.02177766099627336\n",
      "Loss after epoch 720 : 0.02168951529855521\n",
      "Loss after epoch 730 : 0.02160273509532613\n",
      "Loss after epoch 740 : 0.021517222695643046\n",
      "Loss after epoch 750 : 0.021432904806031237\n",
      "Loss after epoch 760 : 0.02134971965514246\n",
      "Loss after epoch 770 : 0.021267619294870244\n",
      "Loss after epoch 780 : 0.021186572653434762\n",
      "Loss after epoch 790 : 0.021106560036128454\n",
      "Loss after epoch 800 : 0.02102756733853354\n",
      "Loss after epoch 810 : 0.020949581634172328\n",
      "Loss after epoch 820 : 0.020872583253774488\n",
      "Loss after epoch 830 : 0.020796544334284975\n",
      "Loss after epoch 840 : 0.02072143358150254\n",
      "Loss after epoch 850 : 0.020647220010783444\n",
      "Loss after epoch 860 : 0.02057387475768823\n",
      "Loss after epoch 870 : 0.02050137224791996\n",
      "Loss after epoch 880 : 0.020429690997907972\n",
      "Loss after epoch 890 : 0.0203588137524467\n",
      "Loss after epoch 900 : 0.020288726670880948\n",
      "Loss after epoch 910 : 0.020219417648234657\n",
      "Loss after epoch 920 : 0.020150874596947504\n",
      "Loss after epoch 930 : 0.020083084942646726\n",
      "Loss after epoch 940 : 0.020016036374617394\n",
      "Loss after epoch 950 : 0.01994971618880282\n",
      "Loss after epoch 960 : 0.019884111525953638\n",
      "Loss after epoch 970 : 0.019819219277674858\n",
      "Loss after epoch 980 : 0.01975503421987976\n",
      "Loss after epoch 990 : 0.0196915436902402\n",
      "Loss after epoch 1001 : 0.01963498524387584\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "dab3d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8606\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf8eb7",
   "metadata": {},
   "source": [
    "## Better accuracy than normal tanh ANN but slow to train, let's do the same with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "82bf1878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.04551065167444922\n",
      "Loss after epoch 20 : 0.0407957885722934\n",
      "Loss after epoch 30 : 0.03838111070194218\n",
      "Loss after epoch 40 : 0.036775252491078436\n",
      "Loss after epoch 50 : 0.03557719780359323\n",
      "Loss after epoch 60 : 0.034622886847097856\n",
      "Loss after epoch 70 : 0.03382450247829172\n",
      "Loss after epoch 80 : 0.033132708439367664\n",
      "Loss after epoch 90 : 0.032526286488784556\n",
      "Loss after epoch 100 : 0.0319863336788978\n",
      "Loss after epoch 110 : 0.03149484703424328\n",
      "Loss after epoch 120 : 0.03105192804294003\n",
      "Loss after epoch 130 : 0.030643495854840812\n",
      "Loss after epoch 140 : 0.03026251351529908\n",
      "Loss after epoch 150 : 0.029904785164562703\n",
      "Loss after epoch 160 : 0.029571393926833313\n",
      "Loss after epoch 170 : 0.029257107141883266\n",
      "Loss after epoch 180 : 0.02896193196497473\n",
      "Loss after epoch 190 : 0.02868209253011767\n",
      "Loss after epoch 200 : 0.028415440003546524\n",
      "Loss after epoch 210 : 0.028158692600169257\n",
      "Loss after epoch 220 : 0.027913776948015622\n",
      "Loss after epoch 230 : 0.02767899804118269\n",
      "Loss after epoch 240 : 0.027454146087506556\n",
      "Loss after epoch 250 : 0.027236032889716783\n",
      "Loss after epoch 260 : 0.027026077748524075\n",
      "Loss after epoch 270 : 0.02682130128315319\n",
      "Loss after epoch 280 : 0.026624301034773618\n",
      "Loss after epoch 290 : 0.026434763688538344\n",
      "Loss after epoch 300 : 0.026251224883640543\n",
      "Loss after epoch 310 : 0.026073894724575625\n",
      "Loss after epoch 320 : 0.025899799241725384\n",
      "Loss after epoch 330 : 0.025729862647582806\n",
      "Loss after epoch 340 : 0.02556590635652637\n",
      "Loss after epoch 350 : 0.025405535989997657\n",
      "Loss after epoch 360 : 0.02525086667422398\n",
      "Loss after epoch 370 : 0.025098162061172137\n",
      "Loss after epoch 380 : 0.02495148190019649\n",
      "Loss after epoch 390 : 0.02480879222202942\n",
      "Loss after epoch 400 : 0.024668939228146607\n",
      "Loss after epoch 410 : 0.024532396748846278\n",
      "Loss after epoch 420 : 0.024400271523632857\n",
      "Loss after epoch 430 : 0.0242725057368106\n",
      "Loss after epoch 440 : 0.024147512869711115\n",
      "Loss after epoch 450 : 0.024024285352522604\n",
      "Loss after epoch 460 : 0.023902984126855792\n",
      "Loss after epoch 470 : 0.02378486854249039\n",
      "Loss after epoch 480 : 0.023669207406098885\n",
      "Loss after epoch 490 : 0.02355509165822318\n",
      "Loss after epoch 500 : 0.023441401615503916\n",
      "Loss after epoch 510 : 0.02333152564413214\n",
      "Loss after epoch 520 : 0.023223221357183483\n",
      "Loss after epoch 530 : 0.02311755524858084\n",
      "Loss after epoch 540 : 0.02301459583313574\n",
      "Loss after epoch 550 : 0.022911083996843638\n",
      "Loss after epoch 560 : 0.022809858774939292\n",
      "Loss after epoch 570 : 0.022710329340468932\n",
      "Loss after epoch 580 : 0.02261094676564882\n",
      "Loss after epoch 590 : 0.022512717435238982\n",
      "Loss after epoch 600 : 0.02241675982552449\n",
      "Loss after epoch 610 : 0.0223220742504408\n",
      "Loss after epoch 620 : 0.022230131313682677\n",
      "Loss after epoch 630 : 0.022138249033676796\n",
      "Loss after epoch 640 : 0.022048393659476893\n",
      "Loss after epoch 650 : 0.021960213072873474\n",
      "Loss after epoch 660 : 0.02187303261438038\n",
      "Loss after epoch 670 : 0.021787804208156965\n",
      "Loss after epoch 680 : 0.02170213822951107\n",
      "Loss after epoch 690 : 0.02161792418390765\n",
      "Loss after epoch 700 : 0.02153638244320312\n",
      "Loss after epoch 710 : 0.02145449350409452\n",
      "Loss after epoch 720 : 0.02137465334888805\n",
      "Loss after epoch 730 : 0.02129599878195374\n",
      "Loss after epoch 740 : 0.021218077749096367\n",
      "Loss after epoch 750 : 0.021141587941134722\n",
      "Loss after epoch 760 : 0.021064943944654455\n",
      "Loss after epoch 770 : 0.020987981828182543\n",
      "Loss after epoch 780 : 0.020913132214916436\n",
      "Loss after epoch 790 : 0.020840350952735818\n",
      "Loss after epoch 800 : 0.02076649394403128\n",
      "Loss after epoch 810 : 0.02069556337245528\n",
      "Loss after epoch 820 : 0.020621711880931287\n",
      "Loss after epoch 830 : 0.020550683603560223\n",
      "Loss after epoch 840 : 0.020480765539506698\n",
      "Loss after epoch 850 : 0.020410822395975668\n",
      "Loss after epoch 860 : 0.020342327818281266\n",
      "Loss after epoch 870 : 0.020275347520219745\n",
      "Loss after epoch 880 : 0.020208366708695857\n",
      "Loss after epoch 890 : 0.020142946787226446\n",
      "Loss after epoch 900 : 0.020077075910310223\n",
      "Loss after epoch 910 : 0.020014008288114733\n",
      "Loss after epoch 920 : 0.019950128858518587\n",
      "Loss after epoch 930 : 0.019887101935436354\n",
      "Loss after epoch 940 : 0.01982468708537441\n",
      "Loss after epoch 950 : 0.019763603913067158\n",
      "Loss after epoch 960 : 0.019703965211797486\n",
      "Loss after epoch 970 : 0.019644761000242938\n",
      "Loss after epoch 980 : 0.01958537095089791\n",
      "Loss after epoch 990 : 0.019527885636146207\n",
      "Loss after epoch 1001 : 0.01947754674898345\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='minibatch',\n",
    "                delta_stop = 1e-5,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "6f31cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8633\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae474efb",
   "metadata": {},
   "source": [
    "## Similar to tanh with mini batch, now the famous Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cceb5f",
   "metadata": {},
   "source": [
    "# ADAM :\n",
    "tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "79913131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.07540961550600114\n",
      "Loss after epoch 20 : 0.06379622132602146\n",
      "Loss after epoch 30 : 0.05743046067665448\n",
      "Loss after epoch 40 : 0.05362365589356266\n",
      "Loss after epoch 50 : 0.051076798614497244\n",
      "Loss after epoch 60 : 0.04932787259640259\n",
      "Loss after epoch 70 : 0.04797701415674386\n",
      "Loss after epoch 80 : 0.04695245753975253\n",
      "Loss after epoch 90 : 0.046065043356619\n",
      "Loss after epoch 100 : 0.04532642482701896\n",
      "Loss after epoch 110 : 0.04465664309412634\n",
      "Loss after epoch 120 : 0.04404162733662966\n",
      "Loss after epoch 130 : 0.0434748116273695\n",
      "Loss after epoch 140 : 0.04295821380851785\n",
      "Loss after epoch 150 : 0.04251021472014239\n",
      "Loss after epoch 160 : 0.04208423235509456\n",
      "Loss after epoch 170 : 0.04169605805435618\n",
      "Loss after epoch 180 : 0.041331207668652475\n",
      "Loss after epoch 190 : 0.04098103546277455\n",
      "Loss after epoch 200 : 0.04065304416061686\n",
      "Loss after epoch 210 : 0.04034359088412875\n",
      "Loss after epoch 220 : 0.04005377867437489\n",
      "Loss after epoch 230 : 0.03977123106613451\n",
      "Loss after epoch 240 : 0.039499904568372116\n",
      "Loss after epoch 250 : 0.03924996385952121\n",
      "Loss after epoch 260 : 0.03902828066600092\n",
      "Loss after epoch 270 : 0.03880960303050625\n",
      "Loss after epoch 280 : 0.03860933114302857\n",
      "Loss after epoch 290 : 0.03840454206810971\n",
      "Loss after epoch 301 : 0.038223659247277025\n"
     ]
    }
   ],
   "source": [
    "nn_tanh = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = tanh,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_tanh.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "853d376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8294\n"
     ]
    }
   ],
   "source": [
    "acc = nn_tanh.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94a872",
   "metadata": {},
   "source": [
    "# Faster to train, lets see relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "f01b6101",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10 : 0.1833568634839161\n",
      "Loss after epoch 20 : 0.10212137958706662\n",
      "Loss after epoch 30 : 0.06857828516646716\n",
      "Loss after epoch 40 : 0.05480903830273192\n",
      "Loss after epoch 50 : 0.04886009451627954\n",
      "Loss after epoch 60 : 0.04534392041862924\n",
      "Loss after epoch 70 : 0.042973871754454086\n",
      "Loss after epoch 80 : 0.041385505327800685\n",
      "Loss after epoch 90 : 0.04019432291581741\n",
      "Loss after epoch 100 : 0.039235811596445576\n",
      "Loss after epoch 110 : 0.03843753377871331\n",
      "Loss after epoch 120 : 0.03775150697198504\n",
      "Loss after epoch 130 : 0.037152268159316826\n",
      "Loss after epoch 140 : 0.03661478933500159\n",
      "Loss after epoch 150 : 0.03612215125704253\n",
      "Loss after epoch 160 : 0.03566968200889436\n",
      "Loss after epoch 170 : 0.03524993006066657\n",
      "Loss after epoch 180 : 0.03485311856834381\n",
      "Loss after epoch 190 : 0.03447427974880276\n",
      "Loss after epoch 200 : 0.034115324281767216\n",
      "Loss after epoch 210 : 0.03377870478865493\n",
      "Loss after epoch 220 : 0.03346214536007906\n",
      "Loss after epoch 230 : 0.0331675649158025\n",
      "Loss after epoch 240 : 0.03289133896151734\n",
      "Loss after epoch 250 : 0.032633904725833364\n",
      "Loss after epoch 260 : 0.03239306258032296\n",
      "Loss after epoch 270 : 0.032164263399240806\n",
      "Loss after epoch 280 : 0.03194279605134301\n",
      "Loss after epoch 290 : 0.031729895309850036\n",
      "Loss after epoch 301 : 0.03154625930612474\n"
     ]
    }
   ],
   "source": [
    "nn_relu = HiddenOne(input_nodes = D, \n",
    "               output_nodes = K, \n",
    "               hidden_nodes = 4*M,\n",
    "               learning_rate = 0.01,\n",
    "               activation_hidden = relu,\n",
    "                optimizer ='adam',\n",
    "                delta_stop = 1e-4,\n",
    "                patience = 5,)\n",
    "c = nn_relu.run(X_train, y_train_cat, epochs=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "5791b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8574\n"
     ]
    }
   ],
   "source": [
    "acc = nn_relu.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26d2ba",
   "metadata": {},
   "source": [
    "## Better accuracy with fewer epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f15eaf",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part7'></a>\n",
    "\n",
    "### Part 7 -  Fashion MNIST with 2 hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae499b6",
   "metadata": {},
   "source": [
    "# 2 Hidden Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2269f4",
   "metadata": {},
   "source": [
    "# Basically do the same as before and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2be10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
