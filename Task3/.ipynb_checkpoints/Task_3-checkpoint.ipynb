{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4484a339",
   "metadata": {},
   "source": [
    "Used resources:\n",
    "- Lab 6 and 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065be285",
   "metadata": {},
   "source": [
    "## Content:\n",
    "- [Part 1](#part1)- Activation functions, their derivatives and loss function\n",
    "- [Part 2](#part2)- Class Neural Network\n",
    "- [Part 3](#part3)- Fashion Mnist Dataset \n",
    "- [Part 4](#part4)- Initialisation of Neural Network with one hidden layer\n",
    "- [Part 4](#part5)-  Initialisation of Neural Network class with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97227f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70acec4e",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "###  Activation functions, their derivatives and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4432939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid activation function with forward pass\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#Sigmoid activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "#ReLU activation function with forward pass\n",
    "@np.vectorize\n",
    "def relu (x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "#ReLU activation function with backward pass\n",
    "@np.vectorize\n",
    "def d_relu (x):\n",
    "    return (np.sign(x) + 1) / 2\n",
    "   \n",
    "#loss method cross entropy\n",
    "def cross_entropy(output, target):\n",
    "    errors = -np.sum(target*np.log(output), axis=1)\n",
    "    return errors/ len(errors)\n",
    "\n",
    "# output function - softmax\n",
    "def softmax(x):\n",
    "    a = x - np.max(x, axis=0, keepdims=True)\n",
    "    new_a = np.exp(a)\n",
    "    return new_a / new_a.sum(axis=0, keepdims=True)\n",
    "   \n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a25ead",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part2'></a>\n",
    "### Class Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e400dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_nodes,\n",
    "                 learning_rate,\n",
    "                 epochs):\n",
    "        self.no_nodes = no_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    # bring as an output weigths list with weight considering input nodes and output nodes    \n",
    "    def create_weight_matrices(self): \n",
    "        weights = []\n",
    "        for i in range(len(self.no_nodes)-1):\n",
    "            rad = 0.5 \n",
    "            X = truncated_normal(mean=1, sd=1, low=-rad, upp=rad)\n",
    "            weight = X.rvs((self.no_nodes[i], self.no_nodes[i+1]))\n",
    "            weights.append(weight)\n",
    "        return weights  \n",
    "    \n",
    "    #bias\n",
    "    def f_bias (self):\n",
    "        biases = []\n",
    "        for i in range(1, len(self.no_nodes)):\n",
    "            rad = 0.5 \n",
    "            tn = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            bias = tn.rvs(self.no_nodes[i]).reshape(-1,1) \n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, X_train_trans):\n",
    "        biases = self.f_bias()\n",
    "        weights = self.create_weight_matrices()\n",
    "        output_list = []\n",
    "        for i in range(len(weights)):\n",
    "            #input vector\n",
    "            if i == 0:\n",
    "                output_vector = np.dot(weights[i].T, X_train_trans) + biases[i]\n",
    "                output_vector_in = activation_function(output_vector)\n",
    "                output_list.append(output_vector_in)\n",
    "            #output \n",
    "            elif i == (len(weights)-1):\n",
    "                output_vector = np.dot (weights[i].T, output_list[i-1]) + biases[i]\n",
    "                output_vector_out = softmax(output_vector) # softmax for the output layer\n",
    "                output_list.append(output_vector_out)\n",
    "            #hidden layers\n",
    "            else:\n",
    "                output_vector = np.dot (weights[i].T, output_list[i-1]) + biases[i]\n",
    "                output_vector_out = activation_function(output_vector)\n",
    "                output_list.append(output_vector_out)\n",
    "        return output_vector_out, output_list\n",
    "    \n",
    "\n",
    "    #training with forward pass and backpropagation\n",
    "    def train(self, X_train, y_train):\n",
    "        weights = self.create_weight_matrices()\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        X_train_trans = np.array(X_train, ndmin=2).T\n",
    "        y_train_trans = np.array(y_train, ndmin=2).T\n",
    "    \n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            # forward pass\n",
    "            forward = self.forward(X_train_trans)\n",
    "            output = forward[0]            \n",
    "            output_list = forward[1]\n",
    "            # cost\n",
    "            cost = cross_entropy(output, y_train_trans) # cross entropy\n",
    "            #backprop   \n",
    "            for i in reversed(range(len(weights))):\n",
    "               \n",
    "                if i == (len(weights)-1):\n",
    "                    # derivative of the loss for the output\n",
    "                    errors = (y_train_trans - output)\n",
    "                    # derivative of the activation function\n",
    "                    derivative = activation_derivative (output)  \n",
    "                    tmp = errors * derivative\n",
    "                    # multiply with the previous activation (output_vector_hidden)\n",
    "                    who_update = self.learning_rate * (np.dot(tmp, output_list[i-1].T))\n",
    "                    weights[i] += who_update.T \n",
    "\n",
    "                elif i == 0:\n",
    "                    #from hidden to input layer\n",
    "                    errors = np.dot(weights[i+1], errors * derivative)\n",
    "                    derivative = activation_derivative(output_list[i])  \n",
    "                    tmp = errors * derivative\n",
    "                    wih_update = self.learning_rate * np.dot(tmp, X_train_trans.T)\n",
    "                    weights[i] += wih_update.T\n",
    "\n",
    "                elif i > 0 and i < (len(weights)-1):\n",
    "                   # hidden layers\n",
    "                    errors = np.dot(weights[i+1], errors * derivative)\n",
    "                    derivative = activation_derivative(output_list[i])  \n",
    "                    tmp = errors * derivative\n",
    "                    whh_update = self.learning_rate * np.dot(tmp, output_list[i-1].T)\n",
    "                    weights[i] += whh_update.T\n",
    "                \n",
    "        return weights, cost\n",
    "\n",
    "\n",
    "    def run(self, X_test, weights):\n",
    "        biases = self.f_bias()\n",
    "        for i in range(len(weights)):\n",
    "            #input layer\n",
    "            if i == 0:\n",
    "                input_vector = np.array(X_test, ndmin=2).T\n",
    "                output_vector = np.dot(weights[0].T, input_vector) + biases[i]\n",
    "                output_vector = activation_function(output_vector)\n",
    "            #output layer    \n",
    "            elif i == (len(weights)-1):\n",
    "                output_vector = np.dot(weights[i].T, output_vector) + biases[i]\n",
    "                output_vector = softmax(output_vector)    # softmax activation function for the output layer \n",
    "           #hidden layer\n",
    "            else:\n",
    "                output_vector = np.dot(weights[i].T, output_vector) + biases[i]\n",
    "                output_vector = activation_function(output_vector)                \n",
    "            y_hat = output_vector.T\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d7cb9",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part3'></a>\n",
    "### Fashion Mnist Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ec7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#introducing dataset and preparing for manipulation\n",
    "fashion = fashion_mnist.load_data()\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27344f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93f5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean (X_train, axis = (0,1,2))\n",
    "std = np.std (X_train, axis = (0,1,2))\n",
    "\n",
    "X_train = (X_train- mean)/(std+1e-7)\n",
    "X_test = (X_test- mean)/(std+1e-7)\n",
    "\n",
    "#reshaping datset input\n",
    "X_train = X_train.reshape((X_train.shape[0], 28*28))  #28*28 = 784 nodes\n",
    "X_test = X_test.reshape((X_test.shape[0], 28*28))\n",
    "\n",
    "#onehot encoding the output\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3368d81",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part4'></a>\n",
    "### Initialisation of Neural Network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291ded7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#introducing activation functions\n",
    "activation_function = sigmoid\n",
    "activation_derivative = d_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6090b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising NN class\n",
    "model = NeuralNetwork([784, 32, 10], 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5daa5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "new_weights, cost = model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbb68fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_10404/1687737000.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "#output\n",
    "y_hat = model.run(X_test, new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326351eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "#accuracy \n",
    "y_hat[y_hat>0.5]=1\n",
    "y_hat[y_hat<0.5]=0\n",
    "accuracy = np.mean(sum(y_hat==y_test)/len(y_hat))\n",
    "print(f'Accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab6656",
   "metadata": {},
   "source": [
    "[Back to top](#Content:)\n",
    "\n",
    "\n",
    "<a id='part5'></a>\n",
    "### Initialisation of Neural Network class with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e874a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = NeuralNetwork([784, 32, 16, 10], 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59955a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "new_weights_2, cost_2 = model_2.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c047bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_10404/1687737000.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "#output\n",
    "y_hat_2 = model_2.run(X_test, new_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0e2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "#accuracy \n",
    "y_hat_2[y_hat_2>0.5]=1\n",
    "y_hat_2[y_hat_2<0.5]=0\n",
    "accuracy_2 = np.mean(sum(y_hat_2==y_test)/len(y_hat_2))\n",
    "print(f'Accuracy: %.2f' % accuracy_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
